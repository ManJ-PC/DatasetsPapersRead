
@misc{raghavan_create_2017,
	title = {Create a model to predict house prices using {Python}},
	url = {https://towardsdatascience.com/create-a-model-to-predict-house-prices-using-python-d34fe8fad88f},
	abstract = {Hey there ,},
	language = {en},
	urldate = {2023-08-01},
	journal = {Medium},
	author = {Raghavan, Shreyas},
	month = jun,
	year = {2017},
	keywords = {ImoestatisticaC},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\4T9NUQGR\\create-a-model-to-predict-house-prices-using-python-d34fe8fad88f.html:text/html},
}

@misc{servico_de_infoliteracia_guias_nodate,
	title = {Guias {Temáticos} de {Apoio}: {ZOTERO}: {Ferramenta} para {Citar} e {Referenciar}: {Adicionar} referências},
	copyright = {Copyright FEUP 2023},
	shorttitle = {Guias {Temáticos} de {Apoio}},
	url = {https://feup.libguides.com/Zotero/adicionar_referencias},
	abstract = {Apresentar as diferentes formas de adicionar referências bibliográficas},
	language = {pt},
	urldate = {2023-08-01},
	author = {Serviço de Infoliteracia, Biblioteca da FEUP},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\QLFMITTR\\adicionar_referencias.html:text/html},
}

@misc{servico_de_infoliteracia_guias_nodate-1,
	title = {Guias {Temáticos} de {Apoio}: {ZOTERO}: {Ferramenta} para {Citar} e {Referenciar}: {Adicionar} referências},
	copyright = {Copyright FEUP 2023},
	shorttitle = {Guias {Temáticos} de {Apoio}},
	url = {https://feup.libguides.com/Zotero/adicionar_referencias},
	abstract = {Apresentar as diferentes formas de adicionar referências bibliográficas},
	language = {pt},
	urldate = {2023-08-01},
	author = {Serviço de Infoliteracia, Biblioteca da FEUP},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\T7XD7QRU\\adicionar_referencias.html:text/html},
}

@article{cruz_dynamic_2018,
	title = {Dynamic classifier selection: {Recent} advances and perspectives},
	issn = {15662535},
	doi = {10.1016/j.inffus.2017.09.010},
	abstract = {Multiple Classifier Systems (MCS) have been widely studied as an alternative for increasing accuracy in pattern recognition. One of the most promising MCS approaches is Dynamic Selection (DS), in which the base classifiers are selected on the fly, according to each new sample to be classified. This paper provides a review of the DS techniques proposed in the literature from a theoretical and empirical point of view. We propose an updated taxonomy based on the main characteristics found in a dynamic selection system: (1) The methodology used to define a local region for the estimation of the local competence of the base classifiers; (2) The source of information used to estimate the level of competence of the base classifiers, such as local accuracy, oracle, ranking and probabilistic models, and (3) The selection approach, which determines whether a single or an ensemble of classifiers is selected. We categorize the main dynamic selection techniques in the DS literature based on the proposed taxonomy. We also conduct an extensive experimental analysis, considering a total of 18 state-of-the-art dynamic selection techniques, as well as static ensemble combination and single classification models. To date, this is the first analysis comparing all the key DS techniques under the same experimental protocol. Furthermore, we also present several perspectives and open research questions that can be used as a guide for future works in this domain.},
	journal = {Information Fusion},
	author = {Cruz, Rafael M.O. and Sabourin, Robert and Cavalcanti, George D.C.},
	year = {2018},
	keywords = {Classifier competence, Dynamic classifier selection, Dynamic ensemble selection, Ensemble of classifiers, Multiple classifier systems, Survey},
}

@article{beavers_alan_nodate,
	title = {Alan {Turing}: {Mathematical} {Mechanist} 1},
	urldate = {2021-09-15},
	author = {Beavers, Anthony F},
}

@article{nouriani_vision-based_2022,
	title = {Vision-based housing price estimation using interior, exterior \& satellite images},
	volume = {14},
	url = {https://doi.org/10.1016/j.iswa.2022.20},
	doi = {10.1016/j.iswa.2022.20},
	abstract = {Real estate price estimation has been an interesting subject in the literature from the appearance of online real estate services like Zillow and Redfin. These websites and many other works in the literature have proposed their methods for evaluation and pricing of the real estate. However, these methods fail to consider important information about the appearance and the neighborhood of the house which leads to occasional incorrect estimations. The novel proposed method in this paper tries to estimate housing price by considering attributes of the home as well as interior, exterior, and satellite visual features of the house. Deep convolutional neural networks on a large dataset of images of interior, exterior and satellite images of houses are trained to extract visual features of the houses. These features along with house attributes are fed to another system to automatically estimate the value of the house. Finally, the performance of the system is compared to Zestimate and some vision-based methods in the literature on a new dataset.},
	urldate = {2023-07-21},
	journal = {Intelligent Systems with Applications},
	author = {Nouriani, Ali and Lemke, Lance},
	year = {2022},
	keywords = {Automatic price estimation, Computer, Housing price, Luxury level, Satellite view CNN, Vision},
	pages = {81},
}

@article{stevens_predicting_2014,
	title = {Predicting {Real} {Estate} {Price} {Using} {Text} {Mining} {Automated} {Real} {Estate} {Description} {Analysis}},
	abstract = {This study describes two methods for real estate price prediction using text mining: classification and regression. Both methods use stemmed n-grams (unigrams and bigrams) derived from real estate description to train different machine learning techniques. Real estate price predictions involve the following pricing indicators: selling price, asking price and price fluctuation. In our classification experiment we succeeded to predict all pricing indicators significantly above baseline with a maximum increase of 38\% (F 1 = .652). Our regression experiment shows that the SGD classifier performs best for all pricing indicators with highest performance achieved for predicting selling price (R 2 = .303). Both can be seen as respectable results given the complex nature of the task. Our results indicate that performance increases when we enlarge the total amount of unigrams and bigrams used in our price prediction model. For the prediction of selling price we see a significant logarithmic increase, which means that the positive effect of adding more n-grams to the model reduces over time. A predictive system for real estate price using text mining reveals to be highly challenging and becomes increasingly complex for larger datasets. Extracting predictive n-grams from real estate description can be a unique and meaningful addition to the standard hedonic pricing models widely used for the prediction of real estate price. PREDICTING REAL ESTATE PRICE USING TEXT MINING 3},
	urldate = {2023-07-21},
	author = {Stevens, Dick and Van Zaanen, M M},
	year = {2014},
	keywords = {asking price, automated description analysis, bigrams, classification, data mining, house prices, price fluctuation, regression, selling price, stemming, Tags: text mining, unigrams},
}

@article{ahmed_house_nodate,
	title = {House price estimation from visual and textual features},
	abstract = {Most existing automatic house price estimation systems rely only on some textual data like its neighborhood area and the number of rooms. The final price is estimated by a human agent who visits the house and assesses it visually. In this paper, we propose extracting visual features from house photographs and combining them with the house's textual information. The combined features are fed to a fully connected multilayer Neural Network (NN) that estimates the house price as its single output. To train and evaluate our network, we have collected the first houses dataset (to our knowledge) that combines both images and textual attributes. The dataset is composed of 535 sample houses from the state of California, USA. Our experiments showed that adding the visual features increased the R-value by a factor of 3 and decreased the Mean Square Error (MSE) by one order of magnitude compared with textual-only features. Additionally, when trained on the textual-only features housing dataset (Lichman, 2013), our proposed NN still outperformed the existing model published results (Khamis and Kamarudin, 2014).},
	urldate = {2023-07-21},
	author = {Ahmed, Eman H and Moustafa, Mohamed N},
	note = {arXiv: 1609.08399v1},
	keywords = {House price estimation, Houses Dataset, Neural Networks, Support Vector Regression},
}

@techreport{stevens_predicting_2014-1,
	title = {Predicting {Real} {Estate} {Price} {Using} {Text} {Mining} {Automated} {Real} {Estate} {Description} {Analysis}},
	abstract = {This study describes two methods for real estate price prediction using text mining: classification and regression. Both methods use stemmed n-grams (unigrams and bigrams) derived from real estate description to train different machine learning techniques. Real estate price predictions involve the following pricing indicators: selling price, asking price and price fluctuation. In our classification experiment we succeeded to predict all pricing indicators significantly above baseline with a maximum increase of 38\% (F 1 = .652). Our regression experiment shows that the SGD classifier performs best for all pricing indicators with highest performance achieved for predicting selling price (R 2 = .303). Both can be seen as respectable results given the complex nature of the task. Our results indicate that performance increases when we enlarge the total amount of unigrams and bigrams used in our price prediction model. For the prediction of selling price we see a significant logarithmic increase, which means that the positive effect of adding more n-grams to the model reduces over time. A predictive system for real estate price using text mining reveals to be highly challenging and becomes increasingly complex for larger datasets. Extracting predictive n-grams from real estate description can be a unique and meaningful addition to the standard hedonic pricing models widely used for the prediction of real estate price. PREDICTING REAL ESTATE PRICE USING TEXT MINING 3},
	author = {Stevens, Dick and Van Zaanen, M M},
	year = {2014},
	keywords = {asking price, automated description analysis, bigrams, classification, data mining, house prices, price fluctuation, regression, selling price, stemming, Tags: text mining, unigrams},
	file = {PDF:C\:\\Users\\manue\\Zotero\\storage\\RK6494XT\\Predicting Real Estate Price Using Text Miningfile134740 (1).pdf:application/pdf},
}

@article{kasinathan_insect_2020,
	title = {Insect classification and detection in field crops using modern machine learning techniques},
	volume = {8},
	doi = {10.1016/j.inpa.2020.09.006},
	abstract = {The agriculture sector has an immense potential to improve the requirement of food and supplies healthy and nutritious food. Crop insect detection is a challenging task for farmers as a significant portion of the crops are damaged, and the quality is degraded due to the pest attack. Traditional insect identification has the drawback of requiring well-trained taxonomists to identify insects based on morphological features accurately. Experiments were conducted for classification on nine and 24 insect classes of Wang and Xie dataset using the shape features and applying machine learning techniques such as artificial neural networks (ANN), support vector machine (SVM), k-nearest neighbors (KNN), naive bayes (NB) and convolutional neural network (CNN) model. This paper presents the insect pest detection algorithm that consists of foreground extraction and contour identification to detect the insects for Wang, Xie, Deng, and IP102 datasets in a highly complex background. The 9-fold cross-validation was applied to improve the performance of the classification models. The highest classification rate of 91.5\% and 90\% was achieved for nine and 24 class insects using the CNN model. The detection performance was accomplished with less computation time for Wang, Xie, Deng, and IP102 datasets using insect pest detection algorithm. The comparison results with the state-of-the-art classification algorithms exhibited considerable improvement in classification accuracy, computation time performance while apply more efficiently in field crops to recognize the insects. The results of classification accuracy are used to recognize the crop insects in the early stages and reduce the time to enhance the crop yield and crop quality in agriculture.},
	journal = {Information Processing in Agriculture},
	author = {Kasinathan, Thenmozhi and Singaraju, Dakshayani and Reddy, U. Srinivasulu},
	month = oct,
	year = {2020},
}

@article{nguyen_vgg-19_2022,
	title = {A {VGG}-19 {Model} with {Transfer} {Learning} and {Image} {Segmentation} for {Classification} of {Tomato} {Leaf} {Disease}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2624-7402},
	url = {https://www.mdpi.com/2624-7402/4/4/56},
	doi = {10.3390/agriengineering4040056},
	abstract = {Tomato leaves can have different diseases which can affect harvest performance. Therefore, accurate classification for the early detection of disease for treatment is very important. This article proposes one classification model, in which 16,010 tomato leaf images obtained from the Plant Village database are segmented before being used to train a deep convolutional neural network (DCNN). This means that this classification model will reduce training time compared with that of the model without segmenting the images. In particular, we applied a VGG-19 model with transfer learning for re-training in later layers. In addition, the parameters such as epoch and learning rate were chosen to be suitable for increasing classification performance. One highlight point is that the leaf images were segmented for extracting the original regions and removing the backgrounds to be black using a hue, saturation, and value (HSV) color space. The segmentation of the leaf images is to synchronize the black background of all leaf images. It is obvious that this segmentation saves time for training the DCNN and also increases the classification performance. This approach improves the model accuracy to 99.72\% and decreases the training time of the 16,010 tomato leaf images. The results illustrate that the model is effective and can be developed for more complex image datasets.},
	language = {en},
	number = {4},
	urldate = {2023-09-16},
	journal = {AgriEngineering},
	author = {Nguyen, Thanh-Hai and Nguyen, Thanh-Nghia and Ngo, Ba-Viet},
	month = dec,
	year = {2022},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {HSV color space, image segmentation, tomato leaf images, transfer learning, VGG-19 model},
	pages = {871--887},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\FBZMDHFI\\Nguyen et al. - 2022 - A VGG-19 Model with Transfer Learning and Image Se.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\PAQR9VQ7\\1706.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\B8HZJJX9\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@article{landgraf_dimensionality_2020,
	title = {Dimensionality reduction for binary data through the projection of natural parameters},
	volume = {180},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X20302499},
	doi = {10.1016/j.jmva.2020.104668},
	abstract = {Principal component analysis (PCA) for binary data, known as logistic PCA, has become a popular alternative to dimensionality reduction of binary data. It is motivated as an extension of ordinary PCA by means of a matrix factorization, akin to the singular value decomposition, that maximizes the Bernoulli log-likelihood. We propose a new formulation of logistic PCA which extends Pearson’s formulation of a low dimensional data representation with minimum error to binary data. Our formulation does not require a matrix factorization, as previous methods do, but instead looks for projections of the natural parameters from the saturated model. Due to this diﬀerence, the number of parameters does not grow with the number of observations and the principal component scores on new data can be computed with simple matrix multiplication. We derive explicit solutions for data matrices of special structure and provide computationally eﬃcient algorithms for solving for the principal component loadings. Through simulation experiments and an analysis of medical diagnoses data, we compare our formulation of logistic PCA to the previous formulation as well as ordinary PCA to demonstrate its beneﬁts.},
	language = {en},
	urldate = {2023-09-20},
	journal = {Journal of Multivariate Analysis},
	author = {Landgraf, Andrew J. and Lee, Yoonkyung},
	month = nov,
	year = {2020},
	pages = {104668},
}

@misc{rodrigues_advancing_2023,
	title = {Advancing {Neural} {Encoding} of {Portuguese} with {Transformer} {Albertina} {PT}-*},
	url = {http://arxiv.org/abs/2305.06721},
	abstract = {To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR). To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over data sets we gathered for PT-PT and PT-BR, and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese. Both Albertina PT-PT and PT-BR versions are distributed free of charge and under the most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.},
	urldate = {2023-09-22},
	publisher = {arXiv},
	author = {Rodrigues, João and Gomes, Luís and Silva, João and Branco, António and Santos, Rodrigo and Cardoso, Henrique Lopes and Osório, Tomás},
	month = jun,
	year = {2023},
	note = {arXiv:2305.06721 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\ZBKN22HB\\2305.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\66JLU63S\\Rodrigues et al. - 2023 - Advancing Neural Encoding of Portuguese with Trans.pdf:application/pdf},
}

@misc{donahue_decaf_2013,
	title = {{DeCAF}: {A} {Deep} {Convolutional} {Activation} {Feature} for {Generic} {Visual} {Recognition}},
	shorttitle = {{DeCAF}},
	url = {http://arxiv.org/abs/1310.1531},
	doi = {10.48550/arXiv.1310.1531},
	abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
	month = oct,
	year = {2013},
	note = {arXiv:1310.1531 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, AI breakthoughts},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\FWMNF27W\\Donahue et al. - 2013 - DeCAF A Deep Convolutional Activation Feature for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\3BYJ5UR2\\1310.html:text/html},
}

@article{noauthor_using_2021,
	title = {Using {Machine} {Learning} {Algorithms} for {Housing} {Price} {Prediction}: {The} {Case} of {Islamabad} {Housing} {Data}},
	volume = {1},
	shorttitle = {Using {Machine} {Learning} {Algorithms} for {Housing} {Price} {Prediction}},
	doi = {10.22995/scmi.2021.1.1.03},
	abstract = {House price prediction is a significant financial decision for individuals working in the housing market as well as for potential buyers. From investment to buying a house for residence, a person investing in the housing market is interested in the potential gain. This paper presents machine learning algorithms to develop intelligent regressions models for House price prediction. The proposed research methodology consists of four stages, namely Data Collection, Pre Processing the data collected and transforming it to the best format, developing intelligent models using machine learning algorithms, training, testing, and validating the model on house prices of the housing market in the Capital, Islamabad. The data used for model validation and testing is the asking price from online property stores, which provide a reasonable estimate of the city housing market. The prediction model can significantly assist in the prediction of future housing prices in Pakistan. The regression results are encouraging and give promising directions for future prediction work on the collected dataset.},
	journal = {Fundamenta Informaticae},
	author = {, Imran and Zaman, Umar and Waqar, Muhammad and Zaman, Atif},
	month = jul,
	year = {2021},
	pages = {11--23},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\MZVAFYK9\\ et al. - 2021 - Using Machine Learning Algorithms for Housing Pric.pdf:application/pdf},
}

@article{zhang_describe_2023,
	title = {Describe the house and {I} will tell you the price: {House} price prediction with textual description data},
	shorttitle = {Describe the house and {I} will tell you the price},
	doi = {10.1017/S1351324923000360},
	abstract = {House price prediction is an important problem that could benefit home buyers and sellers. Traditional models for house price prediction use numerical attributes such as the number of rooms but disregard the house description text. The recent developments in text processing suggest these can be valuable attributes, which motivated us to use house descriptions. This paper focuses on the house asking/advertising price and studies the impact of using house description texts to predict the final house price. To achieve this, we collected a large and diverse set of attributes on house postings, including the house advertising price. Then, we compare the performance of three scenarios: using only the house description, only numeric attributes, or both. We processed the description text through three word embedding techniques: TF-IDF, Word2Vec, and BERT. Four regression algorithms are trained using only textual data, non-textual data, or both. Our results show that by using exclusively the description data with Word2Vec and a Deep Learning model, we can achieve good performance. However, the best overall performance is obtained when using both textual and non-textual features. An \$R{\textasciicircum}2\$ of 0.7904 is achieved by the deep learning model using only description data on the testing data. This clearly indicates that using the house description text alone is a strong predictor for the house price. However, when observing the RMSE on the test data, the best model was gradient boosting using both numeric and description data. Overall, we observe that combining the textual and non-textual features improves the learned model and provides performance benefits when compared against using only one of the feature types. We also provide a freely available application for house price prediction, which is solely based on a house text description and uses our final developed model with Word2Vec and Deep Learning to predict the house price.},
	journal = {Natural Language Engineering},
	author = {Zhang, Hanxiang and Li, Yansong and Branco, Paula},
	month = jul,
	year = {2023},
	pages = {1--35},
	file = {Full Text:C\:\\Users\\manue\\Zotero\\storage\\AJ76WD6W\\Zhang et al. - 2023 - Describe the house and I will tell you the price .pdf:application/pdf},
}

@article{poursaeed_vision-based_2018,
	title = {Vision-based {Real} {Estate} {Price} {Estimation}},
	volume = {29},
	issn = {0932-8092, 1432-1769},
	url = {http://arxiv.org/abs/1707.05489},
	doi = {10.1007/s00138-018-0922-2},
	abstract = {Since the advent of online real estate database companies like Zillow, Trulia and Redfin, the problem of automatic estimation of market values for houses has received considerable attention. Several real estate websites provide such estimates using a proprietary formula. Although these estimates are often close to the actual sale prices, in some cases they are highly inaccurate. One of the key factors that affects the value of a house is its interior and exterior appearance, which is not considered in calculating automatic value estimates. In this paper, we evaluate the impact of visual characteristics of a house on its market value. Using deep convolutional neural networks on a large dataset of photos of home interiors and exteriors, we develop a method for estimating the luxury level of real estate photos. We also develop a novel framework for automated value assessment using the above photos in addition to home characteristics including size, offered price and number of bedrooms. Finally, by applying our proposed method for price estimation to a new dataset of real estate photos and metadata, we show that it outperforms Zillow's estimates.},
	number = {4},
	urldate = {2023-09-29},
	journal = {Machine Vision and Applications},
	author = {Poursaeed, Omid and Matera, Tomas and Belongie, Serge},
	month = may,
	year = {2018},
	note = {arXiv:1707.05489 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	pages = {667--676},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\44N8V9FP\\1707.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\Q5BN58WQ\\Poursaeed et al. - 2018 - Vision-based Real Estate Price Estimation.pdf:application/pdf},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01108 [cs]
version: 4},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\4KZF8BXJ\\1910.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\BJ94LJ24\\Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf},
}

@misc{sanh_distilbert_2019,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {https://arxiv.org/abs/1910.01108v4},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	language = {en},
	urldate = {2023-10-08},
	journal = {arXiv.org},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = oct,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\ZRA3X7VG\\Sanh et al. - 2019 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2023-10-09},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:C\:\\Users\\manue\\Zotero\\storage\\5RAVLLBB\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@article{li_imbalanced_2023,
	title = {Imbalanced {Multimodal} {Attention}-{Based} {System} for {Multiclass} {House} {Price} {Prediction}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/1/113},
	doi = {10.3390/math11010113},
	abstract = {House price prediction is an important problem for individuals, companies, organizations, and governments. With a vast amount of diversified and multimodal data available about houses, the predictive models built should seek to make the best use of these data. This leads to the complex problem of how to effectively use multimodal data for house price prediction. Moreover, this is also a context suffering from class imbalance, an issue that cannot be disregarded. In this paper, we propose a new algorithm for addressing these problems: the imbalanced multimodal attention-based system (IMAS). The IMAS makes use of an oversampling strategy that operates on multimodal data, namely using text, numeric, categorical, and boolean data types. A self-attention mechanism is embedded to leverage the usage of neighboring information that can benefit the model’s performance. Moreover, the self-attention mechanism allows for the determination of the features that are the most relevant and adapts the weights used according to that information when performing inference. Our experimental results show the clear advantage of the IMAS, which outperforms all the competitors tested. The analysis of the weights obtained through the self-attention mechanism provides insights into the features’ relevance and also supports the importance of using this mechanism in the predictive model.},
	language = {en},
	number = {1},
	urldate = {2023-10-10},
	journal = {Mathematics},
	author = {Li, Yansong and Branco, Paula and Zhang, Hanxiang},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {attention, house price prediction, imbalance, multimodal},
	pages = {113},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\6SS8P2PZ\\Li et al. - 2023 - Imbalanced Multimodal Attention-Based System for M.pdf:application/pdf},
}

@article{pimentel_unsupervised_2017,
	title = {{UNSUPERVISED} {AND} {SCALABLE} {ALGORITHM} {FOR} {LEARNING} {NODE} {REPRESENTATIONS}},
	abstract = {Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning ﬁelds, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efﬁcient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes’ neighborhoods as contexts to generate representations. NBNE achieves results comparable or better than state-of-the-art feature learning algorithms in three different datasets and, differently from our main baseline (Node2Vec), which needs to have its parameters tuned in a validation set, is completely unsupervised.},
	language = {en},
	author = {Pimentel, Tiago and Veloso, Adriano and Ziviani, Nivio},
	year = {2017},
	file = {Pimentel et al. - 2017 - UNSUPERVISED AND SCALABLE ALGORITHM FOR LEARNING N.pdf:C\:\\Users\\manue\\Zotero\\storage\\K3GE9CIQ\\Pimentel et al. - 2017 - UNSUPERVISED AND SCALABLE ALGORITHM FOR LEARNING N.pdf:application/pdf},
}

@article{halilaj_machine_2018,
	title = {Machine learning in human movement biomechanics: {Best} practices, common pitfalls, and new opportunities},
	volume = {81},
	issn = {00219290},
	shorttitle = {Machine learning in human movement biomechanics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021929018307309},
	doi = {10.1016/j.jbiomech.2018.09.009},
	abstract = {Traditional laboratory experiments, rehabilitation clinics, and wearable sensors offer biomechanists a wealth of data on healthy and pathological movement. To harness the power of these data and make research more efﬁcient, modern machine learning techniques are starting to complement traditional statistical tools. This survey summarizes the current usage of machine learning methods in human movement biomechanics and highlights best practices that will enable critical evaluation of the literature. We carried out a PubMed/Medline database search for original research articles that used machine learning to study movement biomechanics in patients with musculoskeletal and neuromuscular diseases. Most studies that met our inclusion criteria focused on classifying pathological movement, predicting risk of developing a disease, estimating the effect of an intervention, or automatically recognizing activities to facilitate out-of-clinic patient monitoring. We found that research studies build and evaluate models inconsistently, which motivated our discussion of best practices. We provide recommendations for training and evaluating machine learning models and discuss the potential of several underutilized approaches, such as deep learning, to generate new knowledge about human movement. We believe that cross-training biomechanists in data science and a cultural shift toward sharing of data and tools are essential to maximize the impact of biomechanics research.},
	language = {en},
	urldate = {2023-11-03},
	journal = {Journal of Biomechanics},
	author = {Halilaj, Eni and Rajagopal, Apoorva and Fiterau, Madalina and Hicks, Jennifer L. and Hastie, Trevor J. and Delp, Scott L.},
	month = nov,
	year = {2018},
	pages = {1--11},
	file = {Halilaj et al. - 2018 - Machine learning in human movement biomechanics B.pdf:C\:\\Users\\manue\\Zotero\\storage\\GSTJZSW9\\Halilaj et al. - 2018 - Machine learning in human movement biomechanics B.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\FJ98WAY6\\1301.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\M2UKD73E\\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf},
}

@article{schmidl_anomaly_2022,
	title = {Anomaly detection in time series: a comprehensive evaluation},
	volume = {15},
	issn = {2150-8097},
	shorttitle = {Anomaly detection in time series},
	url = {https://dl.acm.org/doi/10.14778/3538598.3538602},
	doi = {10.14778/3538598.3538602},
	abstract = {Detecting anomalous subsequences in time series data is an important task in areas ranging from manufacturing processes over finance applications to health care monitoring. An anomaly can indicate important events, such as production faults, delivery bottlenecks, system defects, or heart flicker, and is therefore of central interest. Because time series are often large and exhibit complex patterns, data scientists have developed various specialized algorithms for the automatic detection of such anomalous patterns. The number and variety of anomaly detection algorithms has grown significantly in the past and, because many of these solutions have been developed independently and by different research communities, there is no comprehensive study that systematically evaluates and compares the different approaches. For this reason, choosing the best detection technique for a given anomaly detection task is a difficult challenge.},
	language = {en},
	number = {9},
	urldate = {2023-12-19},
	journal = {Proceedings of the VLDB Endowment},
	author = {Schmidl, Sebastian and Wenig, Phillip and Papenbrock, Thorsten},
	month = may,
	year = {2022},
	pages = {1779--1797},
	file = {Schmidl et al. - 2022 - Anomaly detection in time series a comprehensive .pdf:C\:\\Users\\manue\\Zotero\\storage\\4VK27XD8\\Schmidl et al. - 2022 - Anomaly detection in time series a comprehensive .pdf:application/pdf},
}

@article{coelho_predictive_2022,
	title = {Predictive maintenance on sensorized stamping presses by time series segmentation, anomaly detection, and classification algorithms},
	volume = {200},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050922003271},
	doi = {10.1016/j.procs.2022.01.318},
	abstract = {Abstract Sheet metal forming tools, like stamping presses, play an ubiquitous role in the manufacture of several products. With increasing rSehqeueitremmeetanltsfoorfmqiunaglittoyoalsn,dliekﬃe sctiaemncpyi,negnpsurersinsegs,mpalxaiymaunmubuipqtuimitoeuosfrothleesien ttohoelsmiasnfuufnadcatumreenotfalsetovemraalrpkreotpdluacctes.cWomitpheitnitcivreeanseisnsg. Uresqiunigreamnoemntasloyfdqetueacltiitoynaannddepﬃrecdieicntcivye, emnasiunrtiennganmcaextiemchunmiquupetsi,miteisopfotshseisbeletotooldseivselfoupndloawmeernrtiaslktoanmd amrkoeretpilnatceellicgoemntpaeptiptirvoeancehsess. tUosminaginatneonmanaclye dscehteecdtuiolinnagn, dhopwreedviecrt,ivinedmuasitnritaelniamnpceletmecehnntaiqtiuoenss, iotfisthpeossesmibelethtooddsevreemloapinloswcaerrcreisdkuaentdomthoerediiﬃnteclullitgieens toafpopbrtoaainchinegs atoccmepaitnabtelnearnecseulstcshiendurelianlg-w, hoorlwdesvceern, ainridouss,trmiaalkiimngpleamppelnictaattiioonnss ooff tshuecshe mteecthhnoidqsueresminaisntasmcapricnegdpureotcoetshseesdisﬃelcduolmtiefsooufnodb. tIanintihnigs wacocrekp,tawbeleprroepsuolstes aincoremabl-iwnaotrilodnsocfentwaroiodsi,stminacktianpgparopapclihceast:io(nas) toimf seuscehgmteecnhtnaitqiounestoingestthaemr pwinitgh fperaotcuersesedsimseelndsoiomn froeudnudct.ioInn tahnids awnoormk,awlyedpertoepcotisoena; acnodm(bbin) amtiaocnhoinfetwleoardnisintigncctlaaspspiﬁrocaactihoens:a(lag)otriimthemsse,gfmorenetﬀaeticotnivteogdeotwhnertiwmiethpfreeadtiucrtieodni.mTehnesiaopnprreodauchcti(oan)+a(nbd) aalnloomwsalfyordeatnecimtiopnr;ovaenmd e(nbt) rmataechuipnetole2a2r.n9i7n1g\%cloafsstihﬁecamtiaocnroalFg1o-rsicthomres,,wfohreneﬀceocmtipvaereddowtontsiomlee apprpedroicatcihon(.bT).hAe aRpOpCroaAcUhC(ai)n+d(ebx) oalflo9w6\%s foisr aanttaiimnepdrobvyemuesnintgraRteanudpotmo i2z2ed.97D1e\%cisoifonthTermeeasc,robeFin1g-scthoereb, ewsthecnlascsoimﬁepraroefdtwtoeslvoeleteasptperdo.aAchn (ubs)e. AcaRseOwCitAhUaCdiencdeenxtorfal9iz6e\%d pisreadtitcatiinveedmbayinutseinnagncReanardcohmitiezcetdureDefocristihoen dTorweenst,imbeeinfogrethceasbtiensgt oclfaasssiﬁtaemr poifngtwperlevses,tewshteicdh. AisnaucsreiticcaaslemwaicthhinaediencethnemtraalnizuefdacptureridnigctfivaecilmitaieinstoefnaBnocsecharTchhietremctouTreecfhonrotlhoegdy,oiwsndtiismcuesfsoerde.casting of a stamping press, which is a critical machine in the manufacturing facilities of Bosch ThermoTechnology, is discussed.},
	language = {en},
	urldate = {2023-12-20},
	journal = {Procedia Computer Science},
	author = {Coelho, Daniel and Costa, Diogo and Rocha, Eugénio M. and Almeida, Duarte and Santos, José P.},
	year = {2022},
	pages = {1184--1193},
	file = {Coelho et al. - 2022 - Predictive maintenance on sensorized stamping pres.pdf:C\:\\Users\\manue\\Zotero\\storage\\7GUYACL4\\Coelho et al. - 2022 - Predictive maintenance on sensorized stamping pres.pdf:application/pdf},
}

@article{coelho_predictive_2022-1,
	series = {3rd {International} {Conference} on {Industry} 4.0 and {Smart} {Manufacturing}},
	title = {Predictive maintenance on sensorized stamping presses by time series segmentation, anomaly detection, and classification algorithms},
	volume = {200},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050922003271},
	doi = {10.1016/j.procs.2022.01.318},
	abstract = {Sheet metal forming tools, like stamping presses, play an ubiquitous role in the manufacture of several products. With increasing requirements of quality and efficiency, ensuring maximum uptime of these tools is fundamental to marketplace competitiveness. Using anomaly detection and predictive maintenance techniques, it is possible to develop lower risk and more intelligent approaches to maintenance scheduling, however, industrial implementations of these methods remain scarce due to the difficulties of obtaining acceptable results in real-world scenarios, making applications of such techniques in stamping processes seldom found. In this work, we propose a combination of two distinct approaches: (a) time segmentation together with feature dimension reduction and anomaly detection; and (b) machine learning classification algorithms, for effective downtime prediction. The approach (a)+(b) allows for an improvement rate up to 22.971\% of the macro F1-score, when compared to sole approach (b). A ROC AUC index of 96\% is attained by using Randomized Decision Trees, being the best classifier of twelve tested. An use case with a decentralized predictive maintenance architecture for the downtime forecasting of a stamping press, which is a critical machine in the manufacturing facilities of Bosch Thermo Technology, is discussed.},
	urldate = {2023-12-20},
	journal = {Procedia Computer Science},
	author = {Coelho, Daniel and Costa, Diogo and Rocha, Eugénio M. and Almeida, Duarte and Santos, José P.},
	month = jan,
	year = {2022},
	keywords = {Anomaly Detection, Machine Learning, Predictive Maintenance, Time Segmentation},
	pages = {1184--1193},
}

@article{nizam_real-time_2022,
	title = {Real-{Time} {Deep} {Anomaly} {Detection} {Framework} for {Multivariate} {Time}-{Series} {Data} in {Industrial} {IoT}},
	volume = {22},
	issn = {1558-1748},
	url = {https://ieeexplore.ieee.org/document/9915308},
	doi = {10.1109/JSEN.2022.3211874},
	abstract = {The data produced by millions of connected devices and smart sensors in the Industrial Internet of Things (IIoT) is highly dynamic, large-scale, heterogeneous, and time-stamped. These time-stamped data are the core of IIoT automation and have the potential to affect industrial processes intensely. It poses significant challenges to effectively detect anomalies from time-series data and deliver actionable insights in real time to drive improvements to industrial processes. In most practical applications, where data are used to make automated decisions, real-time anomaly detection is critical. With this focus, in this article, we advise a hybrid end-to-end deep anomaly detection (DAD) framework to accurately detect anomalies and extremely rare events on sensitive, Internet of Things (IoT) streaming data in real time or near real time. The proposed framework is based on a convolutional neural network (CNN) and a two-stage long short-term memory (LSTM)-based Autoencoder (AE). We exploit a two-stage LSTM AE in parallel to detect anomalies and extremely rare events hidden in massive sensor data by identifying short- and long-term variations in actual sensor values from the predicted values. We design and train a hybrid model using the Keras/TensorFlow framework as the backend. The experimental results on one simulation and two real datasets demonstrate that the proposed framework achieved better performance and outperforms other state-of-the-art competitive models. Moreover, to prove that the proposed model can be designed for the network edge, we train, optimize, and quantize the model to run-on resource-constrained (i.e., edge) devices. Further evaluation indicates that the training and inference time for each sample is short enough to carry out anomaly detection on edge.},
	number = {23},
	urldate = {2023-12-29},
	journal = {IEEE Sensors Journal},
	author = {Nizam, Hussain and Zafar, Samra and Lv, Zefeng and Wang, Fan and Hu, Xiaopeng},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Sensors Journal},
	pages = {22836--22849},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\manue\\Zotero\\storage\\Q37RUMWW\\9915308.html:text/html},
}

@article{lourenco_adaptive_2023,
	title = {Adaptive time series representation for out-of-round railway wheels fault diagnosis in wayside monitoring},
	volume = {152},
	issn = {1350-6307},
	url = {https://www.sciencedirect.com/science/article/pii/S1350630723003874},
	doi = {10.1016/j.engfailanal.2023.107433},
	abstract = {Through the integration of advanced sensor technologies and machine learning algorithms, artificial intelligence has revolutionized wayside monitoring in the railway sector. Although several algorithms have been proposed for detecting out-of-roundness, i.e., flats, wear treads and polygonization, they generally fall short in isolating the root cause of the wheel's issue in a train passage. In this context, the paper presents a novel approach for wheel out-of-roundness diagnosis with (1) detection of aberrant train behavior; (2) isolation of specific defective wheels; (3) identification of the severity. For this, the methodology automatically segments a strain gauge signal, capturing the complex nature and temporal dependence of vibration patterns. This segmentation allows the extraction of localized accelerometer features in both the time and frequency domain, as well as implicit axle count and labelling of each wheel passage. Moreover, a single-value damage indicator based on anomaly detection algorithms was proposed. To validate the effectiveness of the proposed methodology, experiments on a set 3D numerical train-track dynamic interaction simulations are performed for different wheel profiles, track irregularities, train speeds, sensor placement and noise, associated to other environmental and operational variations. This demonstrates the potential of artificial intelligence for real-time assessment of wheels without interfering with normal service conditions, suggesting the possibility of automated fault diagnosis. © 2023 Elsevier Inc. All rights reserved.},
	urldate = {2024-01-03},
	journal = {Engineering Failure Analysis},
	author = {Lourenço, Afonso and Ferraz, Carolina and Ribeiro, Diogo and Mosleh, Araliya and Montenegro, Pedro and Vale, Cecília and Meixedo, Andreia and Marreiros, Goreti},
	month = oct,
	year = {2023},
	keywords = {Machine Learning, Artificial Intelligence, Fault diagnosis, Wayside monitoring, Wheel Out-of-roundess},
	pages = {107433},
}

@article{van_onsem_hierarchical_2022,
	title = {Hierarchical pattern matching for anomaly detection in time series},
	volume = {193},
	issn = {0140-3664},
	url = {https://www.sciencedirect.com/science/article/pii/S0140366422002298},
	doi = {10.1016/j.comcom.2022.06.027},
	abstract = {As companies rely on an ever increasing number of connected devices for their day to day operations, a need arises for automated anomaly detectors to constantly observe crucial device metrics in real time to prevent downtime and data loss. As production environments tend to monitor a huge amount of these metrics, it prevents current state-of-the-art techniques to be deployed as the required computational resources is too high. This paper proposes a lightweight anomaly detection method that can be deployed in these environments without a reduction in accuracy. The approach works fully online, and does not require an extensive history set to be kept in memory. The method is benchmarked on the publicly available Numenta dataset, as well as a network monitoring dataset from different environments provided by a network management solution vendor. These benchmarks show the proposed technique to be very competitive with the current state-of-the-art and exceeding it in production applicability.},
	urldate = {2024-01-03},
	journal = {Computer Communications},
	author = {Van Onsem, M. and De Paepe, D. and Vanden Hautte, S. and Bonte, P. and Ledoux, V. and Lejon, A. and Ongenae, F. and Dreesen, D. and Van Hoecke, S.},
	month = sep,
	year = {2022},
	keywords = {Anomaly detection, Network monitoring, Time series},
	pages = {75--81},
	file = {Full Text:C\:\\Users\\manue\\Zotero\\storage\\H6A4J5QU\\Van Onsem et al. - 2022 - Hierarchical pattern matching for anomaly detectio.pdf:application/pdf},
}

@inproceedings{sun_time_2019,
	title = {Time {Series} {Anomaly} {Detection} {Based} on {GAN}},
	url = {https://ieeexplore.ieee.org/abstract/document/8931714},
	doi = {10.1109/SNAMS.2019.8931714},
	abstract = {Downtime reduction is one of the top priorities for commercial vehicles providers. The major reasons for long downtime include vehicle failures in the middle of the road or trips, prolonged service time due to lack of availability of parts and technician. Furthermore vehicle failures in the mid of the road trips pose danger to the nearby passing vehicles and pedestrians. Huge expenses are observed in the delayed repair due to the fact that failed parts can deteriorate other components. In order to prevent the risks of component failures and huge costs, a deep learning based system was implemented to provide predictive warning before the actual failure. A novel method has been proposed to mimic domain expert's abnormality detection process using GAN (Generative Adversarial Network): Generator in the GAN was used to generate expected normal behavior; discriminator was used to distinguish normal and abnormal behaviors. The prediction score of Machine Learning (ML)/Deep Learning (DL) of generated expected normal behavior, was used as a threshold. Real world Isuzu vehicle data was used to validate the complete pipeline, advanced warning capability was implemented, and validation was shown. A complete pipeline of infrastructure and software development were introduced in this paper.},
	urldate = {2024-01-03},
	booktitle = {2019 {Sixth} {International} {Conference} on {Social} {Networks} {Analysis}, {Management} and {Security} ({SNAMS})},
	author = {Sun, Yong and Yu, Wenbo and Chen, Yuting and Kadam, Aishwarya},
	month = oct,
	year = {2019},
	pages = {375--382},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\manue\\Zotero\\storage\\U9FXS4UJ\\8931714.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\EGGQ8ANX\\Sun et al. - 2019 - Time Series Anomaly Detection Based on GAN.pdf:application/pdf},
}

@misc{shi_simple_2019,
	title = {Simple {BERT} {Models} for {Relation} {Extraction} and {Semantic} {Role} {Labeling}},
	url = {http://arxiv.org/abs/1904.05255},
	doi = {10.48550/arXiv.1904.05255},
	abstract = {We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.},
	urldate = {2024-01-04},
	publisher = {arXiv},
	author = {Shi, Peng and Lin, Jimmy},
	month = apr,
	year = {2019},
	note = {arXiv:1904.05255 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: work in progress},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\ILV3TYYH\\Shi and Lin - 2019 - Simple BERT Models for Relation Extraction and Sem.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\T8DZVM5Y\\1904.html:text/html},
}

@article{harrison_hedonic_1978,
	title = {Hedonic housing prices and the demand for clean air},
	volume = {5},
	doi = {10.1016/0095-0696(78)90006-2},
	abstract = {In this paper we estimate the price premium associated with organic baby food by applying a hedonic model to price and characteristic data for baby food products collected in two cities: Raleigh/Durham, North Carolina and San Jose, California. We use price per jar of baby food as the dependent variable and control for a number of baby food characteristics (e.g., brand, type, and stage) as well as store characteristics (e.g. type of retail establishment). We find the price premium associated with the organic characteristic to be approximately 12 cents per jar. To the extent this premium reflects parents’ preferences regarding the reduction of their baby’s exposure to pesticide residues, our results could be paired up with risk data to estimate the value of the health benefits associated with reduced exposure.},
	journal = {Journal of Environmental Economics and Management},
	author = {Harrison, David and Rubinfeld, Daniel},
	month = mar,
	year = {1978},
	pages = {81--102},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\BJY7S9TJ\\Harrison and Rubinfeld - 1978 - Hedonic housing prices and the demand for clean ai.pdf:application/pdf},
}

@article{fontes_pattern_2016,
	title = {Pattern recognition in multivariate time series – {A} case study applied to fault detection in a gas turbine},
	volume = {49},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197615002602},
	doi = {10.1016/j.engappai.2015.11.005},
	abstract = {Advances in information technology, together with the evolution of systems in control, automation and instrumentation have enabled the recovery, storage and manipulation of a large amount of data from industrial plants. This development has motivated the advancement of research in fault detection, especially based on process history data. Although a large amount of work has been conducted in recent years on the diagnostics of gas turbines, few of them present the use of clustering approaches applied to multivariate time series, adopting PCA similarity factor (SPCA) in order to detect and/or prevent failures. This paper presents a comprehensive method for pattern recognition associated to fault prediction in gas turbines using time series mining techniques. Algorithms comprising appropriate similarity metrics, subsequence matching and fuzzy clustering were applied on data extracted from a Plant Information Management System (PIMS) represented by multivariate time series. A real case study comprising the fault detection in a gas turbine was investigated. The results suggest the existence of a safe way to start the turbine that can be useful to support the development of a dynamic system for monitoring and predicting the probability of failure and for decision-making at operational level.},
	urldate = {2024-01-09},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Fontes, Cristiano Hora and Pereira, Otacílio},
	month = mar,
	year = {2016},
	keywords = {Clustering, Data mining, Fault detection, Gas turbines, Multivariate time series},
	pages = {10--18},
	file = {ScienceDirect Snapshot:C\:\\Users\\manue\\Zotero\\storage\\AJPJZ3LR\\S0952197615002602.html:text/html},
}

@inproceedings{pokharel_time_2019,
	title = {Time {Series} {Based} {Pattern} {Recognition} for {Anomaly} {Detection} from {System} {Audit} {Logs}},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/8947448},
	doi = {10.1109/AITB48515.2019.8947448},
	abstract = {Pattern recognition is very important for the identification of anomalous patterns in log messages. This paper presents pattern recognition in time series log data for anomaly detection. The proposed method uses Seasonal Auto Regression Integrated Moving Average (Seasonal ARIMA) to identify deviations between actual and predicted values. The deviations beyond a defined threshold are identified as anomalous data points. Anomalous data points for the positively correlated data points are used to calculate the composite anomalous score. Finally, the approach is compared with Seasonal Extreme Studentized Deviate (ESD). The method calculates the anomalous score in the range of 0 to 100. This can be used to understand the security risk posture and thus prioritize the incidents associated with a given user.},
	urldate = {2024-01-09},
	booktitle = {2019 {Artificial} {Intelligence} for {Transforming} {Business} and {Society} ({AITB})},
	author = {Pokharel, Prabhat and Sigdel, Sandeep and Pokhrel, Roshan and Joshi, Basanta},
	month = nov,
	year = {2019},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\manue\\Zotero\\storage\\WDSAHB8W\\8947448.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\NFMXBNCV\\Pokharel et al. - 2019 - Time Series Based Pattern Recognition for Anomaly .pdf:application/pdf},
}

@article{zhou_anomaly_2021,
	title = {An anomaly detection framework for time series data: {An} interval-based approach},
	volume = {228},
	issn = {0950-7051},
	shorttitle = {An anomaly detection framework for time series data},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121004160},
	doi = {10.1016/j.knosys.2021.107153},
	abstract = {Due to the high data volume and non-stationarity of time series data, it is very difficult to directly use the original data for anomaly detection. In this study, a novel framework of anomaly detection is proposed, whose intent is to capture more detailed data of time series’ shape and morphology characteristics by data representation to carry out anomaly detection. First, high-order differences and intervals are employed to realize data representation, and then such rectangles and cubes are constructed with the results of data representation for similarity measurement and anomaly detection. Compared with existing state-of-the-art methods, based on the experimental studies completed on large amount of datasets, the methods proposed in this framework are effective in detecting anomalies caused by changes in shape and amplitude. Meanwhile, it can detect anomalies with higher accuracy and better performance of data anomaly resolution.},
	urldate = {2024-01-09},
	journal = {Knowledge-Based Systems},
	author = {Zhou, Yanjun and Ren, Huorong and Li, Zhiwu and Pedrycz, Witold},
	month = sep,
	year = {2021},
	keywords = {Anomaly detection, Data representation, Intervals, Similarity measurement, Time series data},
	pages = {107153},
	file = {ScienceDirect Snapshot:C\:\\Users\\manue\\Zotero\\storage\\D4XTMQTW\\S0950705121004160.html:text/html},
}

@misc{noauthor_explainableml-context-impersonation_2023,
	title = {{ExplainableML}/in-context-impersonation},
	copyright = {MIT},
	url = {https://github.com/ExplainableML/in-context-impersonation},
	abstract = {[NeurIPS 2023 Spotlight] In-Context Impersonation Reveals Large Language Models' Strengths and Biases},
	urldate = {2024-01-16},
	publisher = {EML Tübingen},
	month = dec,
	year = {2023},
	note = {original-date: 2023-10-18T15:23:11Z},
	keywords = {artificial-intelligence, bandit, chatbot, clip, in-context-impersonation, llama, llama2, mmlu, neurips-2023, reasoning, text-generation},
}

@misc{wang_automated_2021,
	title = {Automated {Concatenation} of {Embeddings} for {Structured} {Prediction}},
	url = {http://arxiv.org/abs/2010.05006},
	abstract = {Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Wang, Xinyu and Jiang, Yong and Bach, Nguyen and Wang, Tao and Huang, Zhongqiang and Huang, Fei and Tu, Kewei},
	month = jun,
	year = {2021},
	note = {arXiv:2010.05006 [cs]
version: 4},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to Proceedings of ACL-IJCNLP 2021. 17 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\GMUYZI4X\\2010.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\E2TB97HW\\Wang et al. - 2021 - Automated Concatenation of Embeddings for Structur.pdf:application/pdf},
}

@misc{frantar_gptq_2023,
	title = {{GPTQ}: {Accurate} {Post}-{Training} {Quantization} for {Generative} {Pre}-trained {Transformers}},
	shorttitle = {{GPTQ}},
	url = {http://arxiv.org/abs/2210.17323},
	doi = {10.48550/arXiv.2210.17323},
	abstract = {Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.17323 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR 2023},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\4H82DMFM\\Frantar et al. - 2023 - GPTQ Accurate Post-Training Quantization for Gene.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\F8EK9U9K\\2210.html:text/html},
}

@misc{bi4all_market_2021,
	title = {Market {Basket} {Analysis}},
	url = {https://www.bi4all.pt/noticias/blog/market-basket-analysis/},
	abstract = {Market Basket Analysis é uma técnica de Associate Rule Mining em grande uso por diversas empresas interessadas em encontrar relações entre os diversos produtos que têm em loja.},
	language = {pt-pt},
	urldate = {2024-01-22},
	journal = {BI4ALL - Turning Data Into Insights},
	author = {BI4ALL},
	month = apr,
	year = {2021},
	note = {Section: Blog},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\KPIC7JK9\\market-basket-analysis.html:text/html},
}

@incollection{kulkarni_introduction_2023,
	address = {Berkeley, CA},
	title = {Introduction to {Recommendation} {Systems}},
	isbn = {978-1-4842-8954-9},
	url = {https://doi.org/10.1007/978-1-4842-8954-9_1},
	abstract = {In today’s world, every customer is faced with multiple choices for every decision. Let’s assume that a person is looking for a book to read without any specific idea of what he wants. There’s a wide range of possibilities for how his search might pan out. He might waste a lot of time browsing the Internet and trawling through various sites hoping to strike gold. He might look for recommendations from other people.},
	language = {en},
	urldate = {2024-01-22},
	booktitle = {Applied {Recommender} {Systems} with {Python}: {Build} {Recommender} {Systems} with {Deep} {Learning}, {NLP} and {Graph}-{Based} {Techniques}},
	publisher = {Apress},
	author = {Kulkarni, Akshay and Shivananda, Adarsha and Kulkarni, Anoosh and Krishnan, V. Adithya},
	editor = {Kulkarni, Akshay and Shivananda, Adarsha and Kulkarni, Anoosh and Krishnan, V. Adithya},
	year = {2023},
	doi = {10.1007/978-1-4842-8954-9_1},
	pages = {1--19},
}

@inproceedings{abdollahpouri_user-centered_2021,
	address = {New York, NY, USA},
	series = {{UMAP} '21},
	title = {User-centered {Evaluation} of {Popularity} {Bias} in {Recommender} {Systems}},
	isbn = {978-1-4503-8366-0},
	url = {https://dl.acm.org/doi/10.1145/3450613.3456821},
	doi = {10.1145/3450613.3456821},
	abstract = {Recommendation and ranking systems are known to suffer from popularity bias; the tendency of the algorithm to favor a few popular items while under-representing the majority of other items. Prior research has examined various approaches for mitigating popularity bias and enhancing the recommendation of long-tail, less popular, items. The effectiveness of these approaches is often assessed using different metrics to evaluate the extent to which over-concentration on popular items is reduced. However, not much attention has been given to the user-centered evaluation of this bias; how different users with different levels of interest towards popular items are affected by such algorithms. In this paper, we show the limitations of the existing metrics to evaluate popularity bias mitigation when we want to assess these algorithms from the users’ perspective and we propose a new metric that can address these limitations. In addition, we present an effective approach that mitigates popularity bias from the user-centered point of view. Finally, we investigate several state-of-the-art approaches proposed in recent years to mitigate popularity bias and evaluate their performances using the existing metrics and also from the users’ perspective. Our experimental results using two publicly-available datasets show that existing popularity bias mitigation techniques ignore the users’ tolerance towards popular items. Our proposed user-centered method can tackle popularity bias effectively for different users while also improving the existing metrics.},
	urldate = {2024-01-23},
	booktitle = {Proceedings of the 29th {ACM} {Conference} on {User} {Modeling}, {Adaptation} and {Personalization}},
	publisher = {Association for Computing Machinery},
	author = {Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad and Malthouse, Edward},
	month = jun,
	year = {2021},
	keywords = {calibration, fairness, long-tail recommendation, popularity bias, recommender systems},
	pages = {119--129},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\DYC4VY8P\\Abdollahpouri et al. - 2021 - User-centered Evaluation of Popularity Bias in Rec.pdf:application/pdf},
}

@article{ghalyan_optimal_2022,
	title = {Optimal {Window}-{Symbolic} {Time} {Series} {Analysis} for {Pattern} {Classification} and {Anomaly} {Detection}},
	volume = {18},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/abstract/document/9454389},
	doi = {10.1109/TII.2021.3089199},
	abstract = {This article proposes the optimal window-symbolic time series analysis (OW-STSA) methodology to optimize parameters of feature extraction and pattern classification in industrial processes. The underlying theory is built upon minimization of an empirical risk function to discriminate between nominal and anomalous operations of the physical process under consideration. In particular, the proposed methodology produces: optimized windows of the time series used for pattern classification and anomaly detection, and optimized identification of feature extractors and classifier parameters. The algorithm is realized by segmenting a given time series into windows of equal size. Then, the stationary state probability vector is computed for each window in the sense of OW-STSA for anomaly prediction with locally optimal accuracy of detection performance. The proposed methodology has been experimentally validated in laboratory environment with different classifiers for two distinct industrial processes. The first experiment addresses detection of fatigue failure in polycrystalline alloy structures using time series of ultrasonic signals. The second experiment investigates detection of thermoacoustic instability in an emulated combustion system using time series of pressure-wave signals. In both experiments, the proposed OW-STSA methodology yielded excellent detection performance of anomalous behavior with multiple classification techniques.},
	number = {4},
	urldate = {2024-01-26},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Ghalyan, Ibrahim F. and Ghalyan, Najah F. and Ray, Asok},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Anomaly detection, Dynamical systems, empirical risk function (ERF), Fatigue, Feature extraction, Informatics, Partitioning algorithms, symbolic dynamics, Time series analysis, time series signals},
	pages = {2614--2621},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\manue\\Zotero\\storage\\EAIL3CBH\\9454389.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\VL45NPTL\\Ghalyan et al. - 2022 - Optimal Window-Symbolic Time Series Analysis for P.pdf:application/pdf},
}

@inproceedings{pokharel_time_2019-1,
	title = {Time {Series} {Based} {Pattern} {Recognition} for {Anomaly} {Detection} from {System} {Audit} {Logs}},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/8947448},
	doi = {10.1109/AITB48515.2019.8947448},
	abstract = {Pattern recognition is very important for the identification of anomalous patterns in log messages. This paper presents pattern recognition in time series log data for anomaly detection. The proposed method uses Seasonal Auto Regression Integrated Moving Average (Seasonal ARIMA) to identify deviations between actual and predicted values. The deviations beyond a defined threshold are identified as anomalous data points. Anomalous data points for the positively correlated data points are used to calculate the composite anomalous score. Finally, the approach is compared with Seasonal Extreme Studentized Deviate (ESD). The method calculates the anomalous score in the range of 0 to 100. This can be used to understand the security risk posture and thus prioritize the incidents associated with a given user.},
	urldate = {2024-01-26},
	booktitle = {2019 {Artificial} {Intelligence} for {Transforming} {Business} and {Society} ({AITB})},
	author = {Pokharel, Prabhat and Sigdel, Sandeep and Pokhrel, Roshan and Joshi, Basanta},
	month = nov,
	year = {2019},
	keywords = {Anomaly Detection, Anomaly detection, Time series analysis, Autocorrelation, Autoregression, Autoregressive processes, Correlation, Data models, Electrostatic discharges, ESD, Log Data, Moving Average, Seasonal ARIMA, Security},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\manue\\Zotero\\storage\\RMXZ54HJ\\8947448.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\UJ95BTL2\\Pokharel et al. - 2019 - Time Series Based Pattern Recognition for Anomaly .pdf:application/pdf},
}

@article{fontes_pattern_2016-1,
	title = {Pattern recognition in multivariate time series – {A} case study applied to fault detection in a gas turbine},
	volume = {49},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197615002602},
	doi = {10.1016/j.engappai.2015.11.005},
	abstract = {Advances in information technology, together with the evolution of systems in control, automation and instrumentation have enabled the recovery, storage and manipulation of a large amount of data from industrial plants. This development has motivated the advancement of research in fault detection, especially based on process history data. Although a large amount of work has been conducted in recent years on the diagnostics of gas turbines, few of them present the use of clustering approaches applied to multivariate time series, adopting PCA similarity factor (SPCA) in order to detect and/or prevent failures. This paper presents a comprehensive method for pattern recognition associated to fault prediction in gas turbines using time series mining techniques. Algorithms comprising appropriate similarity metrics, subsequence matching and fuzzy clustering were applied on data extracted from a Plant Information Management System (PIMS) represented by multivariate time series. A real case study comprising the fault detection in a gas turbine was investigated. The results suggest the existence of a safe way to start the turbine that can be useful to support the development of a dynamic system for monitoring and predicting the probability of failure and for decision-making at operational level.},
	urldate = {2024-01-26},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Fontes, Cristiano Hora and Pereira, Otacílio},
	month = mar,
	year = {2016},
	keywords = {Clustering, Data mining, Fault detection, Gas turbines, Multivariate time series},
	pages = {10--18},
}

@article{dadouchi_recommender_2021,
	title = {Recommender systems as an agility enabler in supply chain management},
	volume = {32},
	issn = {1572-8145},
	url = {https://doi.org/10.1007/s10845-020-01619-5},
	doi = {10.1007/s10845-020-01619-5},
	abstract = {In recent years, recommender systems have become necessary in overcoming the challenges related to the incredible growth of information. They are used in a wide range of contexts and applications, mainly as prediction tools for customer interest, designed to help customers decide, compare, discover and explore products (Meyer in Recommender systems in industrial contexts, Sciences et Technologies de l’Information, Grenoble, 2012). Therefore, research in the field has focused on improving the efficiency of data processing for instant and accurate recommendations. Recommendation of products, accordingly, does not take into consideration supply chain constraints for deliveries. This can lead to recommendations for products that can be costly or too long to ship to the customer, resulting in an avoidable increase in the stress on the supply chain. This paper addresses the problem of considering delivery constraints in product recommendations. The objective is to shift demand toward products that can be delivered using the current network state without additional resources in a given time window, perimeter and with a minimum acceptable profit, in the context of e-commerce. To achieve this goal, we propose a methodology to adjust product recommendations in order to shift customers’ interests towards particular products with consideration for remaining unit loads of scheduled deliveries. For this, quasireal-time information about the supply chain is taken into consideration to improve the number of shippable products in the recommendation list, resulting in a possible improvement in truck-load utilization, lower operation costs and reduced lead-times for delivery. This method works in two stages: the first stage is the computation of the recommendation with traditional recommendation systems, and the second stage is recommendation adjustments in four phases that consider the evaluation of active trucks, evaluation of physical constraints for transportation, evaluation of the profits associated with adding a pickup/delivery to a scheduled tour for each recommended item and adjustment of recommendation scores. A sensitivity analysis of the impact of the recommendation adjustment on the recommendation list has been conducted for each of the parameters considered in the proposed method: time window, perimeter radius and minimum acceptable profit. Various experimental results prove that the method permits increasing the number of recommended products that can be shipped using the available resources within a given perimeter radius, time window and minimum profit.},
	language = {en},
	number = {5},
	urldate = {2024-01-29},
	journal = {Journal of Intelligent Manufacturing},
	author = {Dadouchi, Camélia and Agard, Bruno},
	month = jun,
	year = {2021},
	keywords = {Agility, Big data, E-commerce, Recommender systems, Supply chain},
	pages = {1229--1248},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\JVSHAXWJ\\Dadouchi and Agard - 2021 - Recommender systems as an agility enabler in suppl.pdf:application/pdf},
}

@article{cui_ai_2022,
	title = {{AI} and {Procurement}},
	volume = {24},
	issn = {1523-4614},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/msom.2021.0989},
	doi = {10.1287/msom.2021.0989},
	abstract = {Problem definition: In this research, we study how buyers’ use of artificial intelligence (AI) affects suppliers’ price quoting strategies. Specifically, we study the impact of automation—that is, the buyer uses a chatbot to automatically inquire about prices instead of asking in person—and the impact of smartness—that is, the buyer signals the use of a smart AI algorithm in selecting the supplier. Academic/practical relevance: In a world advancing toward AI, we explore how AI creates and delivers value in procurement. AI has two unique abilities: automation and smartness, which are associated with physical machines or software that enable us to operate more efficiently and effectively. Methodology: We collaborate with a trading company to run a field experiment on an online platform in which we compare suppliers’ wholesale price quotes across female, male, and chatbot buyer types under AI and no recommendation conditions. Results: We find that, when not equipped with a smart control, there is price discrimination against chatbot buyers who receive a higher wholesale price quote than human buyers. In fact, without smartness, automation alone receives the highest quoted wholesale price. However, signaling the use of a smart recommendation system can effectively reduce suppliers’ price quote for chatbot buyers. We also show that AI delivers the most value when buyers adopt automation and smartness simultaneously in procurement. Managerial implications: Our results imply that automation is not very valuable when implemented without smartness, which in turn suggests that building smartness is necessary before considering high levels of autonomy. Our study unlocks the optimal steps that buyers could adopt to develop AI in procurement processes.},
	number = {2},
	urldate = {2024-01-30},
	journal = {Manufacturing \& Service Operations Management},
	author = {Cui, Ruomeng and Li, Meng and Zhang, Shichen},
	month = mar,
	year = {2022},
	note = {Publisher: INFORMS},
	keywords = {artificial intelligence, automation, procurement, smartness, wholesale pricing},
	pages = {691--706},
}

@inproceedings{alm_emotions_2005,
	address = {Vancouver, British Columbia, Canada},
	title = {Emotions from {Text}: {Machine} {Learning} for {Text}-based {Emotion} {Prediction}},
	shorttitle = {Emotions from {Text}},
	url = {https://aclanthology.org/H05-1073},
	urldate = {2024-02-06},
	booktitle = {Proceedings of {Human} {Language} {Technology} {Conference} and {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alm, Cecilia Ovesdotter and Roth, Dan and Sproat, Richard},
	editor = {Mooney, Raymond and Brew, Chris and Chien, Lee-Feng and Kirchhoff, Katrin},
	month = oct,
	year = {2005},
	pages = {579--586},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\NCA4F69I\\Alm et al. - 2005 - Emotions from Text Machine Learning for Text-base.pdf:application/pdf},
}

@inproceedings{dewi_analysis_2019,
	title = {Analysis of {LFCC} {Feature} {Extraction} in {Baby} {Crying} {Classification} using {KNN}},
	doi = {10.1109/IoTaIS47347.2019.8980389},
	abstract = {Cry is a form of communication for children to express their feeling. Baby's cry can be characterized according to its natural periodic tone and the change of voice. It has a base frequency (pitch) in range 250Hz to 600Hz. Through their baby's cries detection, parents can monitor their baby remotely only in important condition. This study of sound recognition has two main processes, the first process is feature extraction and the second process is classification or determining the sound pattern. In the Linear Frequency Cepstral Coefficient (LFCC) method, the analysis of changes in pre-emphasis, numbers of filter bank and numbers of cepstral are conducted. The selection of the filter bank value which applied must be greater than the cepstral value which applied. Cepstral values is adjusted to get the better accuracy. The highest percentage of accuracy is 90\% when this system uses 8 as the cepstral value and 3 as the nearest neighbor value, and all rules are considered the best value based on the test results. The use of LFCC as feature extraction method and K-Nearest Neighbor (K-NN) classification can be implemented to detect the baby is crying or not so that it can be applied as a solution for parents to monitor their children remotely only in certain condition.},
	author = {Dewi, Sita and Prasasti, Anggunmeka and Irawan, Budhi},
	month = nov,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\5I4ZW8AZ\\Dewi et al. - 2019 - Analysis of LFCC Feature Extraction in Baby Crying.pdf:application/pdf},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	doi = {10.48550/arXiv.1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10084 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published at EMNLP 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\44WWYRZD\\Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\CFRYT325\\1908.html:text/html},
}

@inproceedings{ayoola_refined_2022,
	address = {Hybrid: Seattle, Washington + Online},
	title = {{ReFinED}: {An} {Efficient} {Zero}-shot-capable {Approach} to {End}-to-{End} {Entity} {Linking}},
	shorttitle = {{ReFinED}},
	url = {https://aclanthology.org/2022.naacl-industry.24},
	doi = {10.18653/v1/2022.naacl-industry.24},
	abstract = {We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-ofthe-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zeroshot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed. Our code and pre-trained models are available at https://github.com/alexa/ReFinED.},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Ayoola, Tom and Tyagi, Shubhi and Fisher, Joseph and Christodoulopoulos, Christos and Pierleoni, Andrea},
	year = {2022},
	pages = {209--220},
	file = {Ayoola et al. - 2022 - ReFinED An Efficient Zero-shot-capable Approach t.pdf:C\:\\Users\\manue\\Zotero\\storage\\JZJ984P9\\Ayoola et al. - 2022 - ReFinED An Efficient Zero-shot-capable Approach t.pdf:application/pdf},
}

@inproceedings{ayoola_improving_2022,
	address = {Seattle, United States},
	title = {Improving {Entity} {Disambiguation} by {Reasoning} over a {Knowledge} {Base}},
	url = {https://aclanthology.org/2022.naacl-main.210},
	doi = {10.18653/v1/2022.naacl-main.210},
	abstract = {Recent work in entity disambiguation (ED) has typically neglected structured knowledge base (KB) facts, and instead relied on a limited subset of KB information, such as entity descriptions or types. This limits the range of contexts in which entities can be disambiguated. To allow the use of all KB facts, as well as descriptions and types, we introduce an ED model which links entities by reasoning over a symbolic knowledge base in a fully differentiable fashion. Our model surpasses state-of-the-art baselines on six well-established ED datasets by 1.3 F1 on average. By allowing access to all KB information, our model is less reliant on popularity-based entity priors, and improves performance on the challenging ShadowLink dataset (which emphasises infrequent and ambiguous entities) by 12.7 F1.},
	language = {en},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Ayoola, Tom and Fisher, Joseph and Pierleoni, Andrea},
	year = {2022},
	pages = {2899--2912},
	file = {Ayoola et al. - 2022 - Improving Entity Disambiguation by Reasoning over .pdf:C\:\\Users\\manue\\Zotero\\storage\\FHQTGSPW\\Ayoola et al. - 2022 - Improving Entity Disambiguation by Reasoning over .pdf:application/pdf},
}

@article{chandola_anomaly_2009,
	title = {Anomaly detection: {A} survey},
	volume = {41},
	issn = {0360-0300},
	shorttitle = {Anomaly detection},
	url = {https://doi.org/10.1145/1541880.1541882},
	doi = {10.1145/1541880.1541882},
	abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
	number = {3},
	urldate = {2024-02-24},
	journal = {ACM Computing Surveys},
	author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	month = jul,
	year = {2009},
	keywords = {Anomaly detection, outlier detection},
	pages = {15:1--15:58},
}

@incollection{srivastava_pattern_2012,
	title = {Pattern {Recognition} in {Time} {Series}},
	volume = {20124949},
	isbn = {978-1-4398-4173-0 978-1-4398-4174-7},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/b11822-36},
	abstract = {Massive amount of time series data are generated daily, in areas as diverse as astronomy, industry, sciences, and aerospace, to name just a few. One obvious problem of handling time series databases concerns with its typically massive size—gigabytes or even terabytes are common, with more and more databases reaching the petabyte scale. Most classic data mining algorithms do not perform or scale well on time series data. The intrinsic structural characteristics of time series data such as the high dimensionality and feature correlation, combined with the measurement-induced noises that beset real-world time series data, pose challenges that render classic data mining algorithms ineffective and inefficient for time series. As a result, time series data mining has attracted enormous amount of attention in the past two decades.},
	language = {en},
	urldate = {2024-02-26},
	booktitle = {Advances in {Machine} {Learning} and {Data} {Mining} for {Astronomy}},
	publisher = {Chapman and Hall/CRC},
	author = {Lin, Jessica and Williamson, Sheri and Borne, Kirk and Debarr, And},
	editor = {Srivastava, Ashok},
	month = mar,
	year = {2012},
	doi = {10.1201/b11822-36},
	note = {Series Title: Chapman \& Hall/CRC Data Mining and Knowledge Discovery Series},
	file = {Lin et al. - 2012 - Pattern Recognition in Time Series.pdf:C\:\\Users\\manue\\Zotero\\storage\\EQ3NA8R9\\Lin et al. - 2012 - Pattern Recognition in Time Series.pdf:application/pdf},
}

@misc{correa_teenytinyllama_2024,
	title = {{TeenyTinyLlama}: open-source tiny language models trained in {Brazilian} {Portuguese}},
	shorttitle = {{TeenyTinyLlama}},
	url = {http://arxiv.org/abs/2401.16640},
	doi = {10.48550/arXiv.2401.16640},
	abstract = {Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama},
	urldate = {2024-02-29},
	publisher = {arXiv},
	author = {Corrêa, Nicholas Kluge and Falk, Sophia and Fatimah, Shiza and Sen, Aniket and de Oliveira, Nythamar},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16640 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 21 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\DJ2RL9IG\\Corrêa et al. - 2024 - TeenyTinyLlama open-source tiny language models t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\7UZ275TM\\2401.html:text/html},
}

@misc{bojanowski_enriching_2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {http://arxiv.org/abs/1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	language = {en},
	urldate = {2024-03-01},
	publisher = {arXiv},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jun,
	year = {2017},
	note = {arXiv:1607.04606 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to TACL. The two first authors contributed equally},
	file = {Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:C\:\\Users\\manue\\Zotero\\storage\\DJY2SBI9\\Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:C\:\\Users\\manue\\Zotero\\storage\\9YNJCI3R\\Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{vaswani_attention_2023-1,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\P64FSSXY\\1706.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\PHBXXQWX\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{clark_electra_2020,
	title = {{ELECTRA}: {Pre}-training {Text} {Encoders} as {Discriminators} {Rather} {Than} {Generators}},
	shorttitle = {{ELECTRA}},
	url = {http://arxiv.org/abs/2003.10555},
	doi = {10.48550/arXiv.2003.10555},
	abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
	month = mar,
	year = {2020},
	note = {arXiv:2003.10555 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\QFWSBQU8\\Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\YZ9AKFWP\\2003.html:text/html},
}

@misc{rodrigues_advancing_2023-1,
	title = {Advancing {Neural} {Encoding} of {Portuguese} with {Transformer} {Albertina} {PT}-*},
	url = {http://arxiv.org/abs/2305.06721},
	abstract = {To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR). To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over data sets we gathered for PT-PT and PT-BR, and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese. Both Albertina PT-PT and PT-BR versions are distributed free of charge and under the most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Rodrigues, João and Gomes, Luís and Silva, João and Branco, António and Santos, Rodrigo and Cardoso, Henrique Lopes and Osório, Tomás},
	month = jun,
	year = {2023},
	note = {arXiv:2305.06721 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\6XQGVYE5\\2305.html:text/html;Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\WBWXDK27\\Rodrigues et al. - 2023 - Advancing Neural Encoding of Portuguese with Trans.pdf:application/pdf},
}

@misc{alayrac_flamingo_2022,
	title = {Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}},
	shorttitle = {Flamingo},
	url = {http://arxiv.org/abs/2204.14198},
	doi = {10.48550/arXiv.2204.14198},
	abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	month = nov,
	year = {2022},
	note = {arXiv:2204.14198 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: 54 pages. In Proceedings of Neural Information Processing Systems (NeurIPS) 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\7K3ZU3DI\\Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\CV76H5PT\\2204.html:text/html},
}

@article{stellzig-eisenhauer_interaction_2010,
	title = {Interaction between otorhinolaryngology and orthodontics: correlation between the nasopharyngeal airway and the craniofacial complex},
	volume = {9},
	shorttitle = {Interaction between otorhinolaryngology and orthodontics},
	doi = {10.3205/cto000068},
	abstract = {In terms of pathophysiology, an anatomically narrow airway is a predisposing factor for obstruction of the upper respiratory tract. The correlation between the nasopharyngeal airway and the craniofacial structures is discussed in this context. Thus a mutual interaction between the pharynx and the mandibular position was demonstrated, whereby the transverse dimension of the nasopharynx was significantly larger in patients with prognathism than in patients with retrognathism. The influence of chronic obstruction of the nasal airway on craniofacial development was also discussed. The form-and-function interaction, which ought to explain the causal relationship between nasal obstruction and craniofacial growth, appears to be of a multifactorial rather than a one-dimensional, linear nature. It is not disputed, however, that expanding the maxilla improves not only nasal volume and nasal flow, but also the subjective sensation of patients, although it is not possible to make a prognostic statement about the extent of this improvement because of the differing reactions of individuals. Orthodontic appliances for advancing the mandible can also be successfully used in the treatment of mild obstructive sleep apnea syndrome. This treatment method should be considered particularly for patients who are unwilling to undergo or cannot tolerate CPAP (continuous positive airway pressure) treatment.},
	journal = {GMS current topics in otorhinolaryngology, head and neck surgery},
	author = {Stellzig-Eisenhauer, Angelika and Meyer-Marcotty, Philipp},
	month = jan,
	year = {2010},
	pages = {Doc04},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\WUCPTHSV\\Stellzig-Eisenhauer and Meyer-Marcotty - 2010 - Interaction between otorhinolaryngology and orthod.pdf:application/pdf},
}

@article{alharbi_sustainable_2023,
	title = {A {Sustainable} {Price} {Prediction} {Model} for {Airbnb} {Listings} {Using} {Machine} {Learning} and {Sentiment} {Analysis}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/15/17/13159},
	doi = {10.3390/su151713159},
	abstract = {Since 2008, the company Airbnb has brought significant changes to the hospitality industry worldwide. Experiencing remarkable growth, it currently offers over six million listings in 191 countries across one hundred thousand cities. Airbnb has gained immense popularity among travellers seeking accommodations globally. Consequently, Airbnb generates extensive datasets from its listings that contain rich features that have captured the attention of researchers. These datasets offer potentially valuable information that can be extracted to greatly assist individuals and governments in making more informed decisions. Pricing rental properties on Airbnb still presents a challenge for owners, as it directly impacts customer demand. This research aimed to conquer the challenge by developing a sustainable price prediction model for Airbnb listings by incorporating property specifications, owner information and customer reviews. By utilising this model, owners can estimate the expected value of their Airbnb listings. We trained and fine-tuned several machine learning models using an Airbnb listing dataset from Barcelona. Performance evaluation metrics, such as mean squared error (MSE), mean absolute error (MAE), root mean square error (RMSE) and R2 score were then used to compare the models. To enhance the performance of the predictive models, sentiment analysis was used to extract relevant features from customer reviews. Feature importance analysis was also conducted to determine which attributes were the most influential on listing price predictions. The results show that the Lasso and Ridge models outperformed the others considered in the study, with an average R2 score of 99\%. We found that amenities-related features had a negligible impact on all models’ performance. The most significant features found were polarity (positive/negative sentiment), the number of bedrooms, the accommodation’s maximum capacity, the number of beds and the quantity of reviews received by the listing in the past 12 months, respectively. We found that certain room types (categorized as entire home/apartment, private room or shared room) are associated with lower predicted prices.},
	language = {en},
	number = {17},
	urldate = {2024-04-22},
	journal = {Sustainability},
	author = {Alharbi, Zahyah H.},
	month = jan,
	year = {2023},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {regression, Airbnb, machine learning, sentiment analysis, sharing economy, sustainable price},
	pages = {13159},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\4N68CBYC\\Alharbi - 2023 - A Sustainable Price Prediction Model for Airbnb Li.pdf:application/pdf},
}

@article{sanchez-ferreres_unleashing_2021,
	title = {Unleashing textual descriptions of business processes},
	volume = {20},
	issn = {1619-1366, 1619-1374},
	url = {https://link.springer.com/10.1007/s10270-021-00886-x},
	doi = {10.1007/s10270-021-00886-x},
	abstract = {Textual descriptions of processes are ubiquous in organizations, so that documentation of the important processes can be accessible to anyone involved. Unfortunately, the value of this rich data source is hampered by the challenge of analyzing unstructured information. In this paper we propose a framework to overcome the current limitations on dealing with textual descriptions of processes. This framework considers extraction and analysis, and connects to process mining via simulation. The framework is grounded in the notion of annotated textual descriptions of processes, which represents a middle-ground between formalization and accessibility, and which accounts for diﬀerent modeling styles, ranging from purely imperative to purely declarative. The contributions of this paper are implemented in several tools, and case studies are highlighted.},
	language = {en},
	number = {6},
	urldate = {2024-04-22},
	journal = {Software and Systems Modeling},
	author = {Sànchez-Ferreres, Josep and Burattin, Andrea and Carmona, Josep and Montali, Marco and Padró, Lluís and Quishpi, Luís},
	month = dec,
	year = {2021},
	pages = {2131--2153},
	file = {Sànchez-Ferreres et al. - 2021 - Unleashing textual descriptions of business proces.pdf:C\:\\Users\\manue\\Zotero\\storage\\YGZJUHXR\\Sànchez-Ferreres et al. - 2021 - Unleashing textual descriptions of business proces.pdf:application/pdf},
}

@misc{zhao_pate_2022,
	title = {{PATE}: {Property}, {Amenities}, {Traffic} and {Emotions} {Coming} {Together} for {Real} {Estate} {Price} {Prediction}},
	shorttitle = {{PATE}},
	url = {http://arxiv.org/abs/2209.05471},
	abstract = {Real estate prices have a signiﬁcant impact on individuals, families, businesses, and governments. The general objective of real estate price prediction is to identify and exploit socioeconomic patterns arising from real estate transactions over multiple aspects, ranging from the property itself to other contributing factors. However, price prediction is a challenging multidimensional problem that involves estimating many characteristics beyond the property itself. In this paper, we use multiple sources of data to evaluate the economic contribution of different socioeconomic characteristics such as surrounding amenities, trafﬁc conditions and social emotions. Our experiments were conducted on 28, 550 houses in Beijing, China and we rank each characteristic by its importance. Since the use of multisource information improves the accuracy of predictions, the aforementioned characteristics can be an invaluable resource to assess the economic and social value of real estate. Code and data are available at: https://github.com/IndigoPurple/PATE.},
	language = {en},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Zhao, Yaping and Ravi, Ramgopal and Shi, Shuhui and Wang, Zhongrui and Lam, Edmund Y. and Zhao, Jichang},
	month = oct,
	year = {2022},
	note = {arXiv:2209.05471 [cs]},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: Accepted by IEEE DSAA 2022. 10 pages, 3 figures},
	file = {Zhao et al. - 2022 - PATE Property, Amenities, Traffic and Emotions Co.pdf:C\:\\Users\\manue\\Zotero\\storage\\C6TXAHRK\\Zhao et al. - 2022 - PATE Property, Amenities, Traffic and Emotions Co.pdf:application/pdf},
}

@article{tran_thesis_nodate,
	title = {A thesis submitted in partial satisfaction of the requirements for the degree {Master} of {Science} in {Applied} {Statistics}},
	language = {en},
	author = {Tran, Richard},
	file = {Tran - A thesis submitted in partial satisfaction of the .pdf:C\:\\Users\\manue\\Zotero\\storage\\M5KNGINV\\Tran - A thesis submitted in partial satisfaction of the .pdf:application/pdf},
}

@misc{vaswani_attention_2023-2,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2023 - Attention Is All You Need.pdf:C\:\\Users\\manue\\Zotero\\storage\\DZWEHI8R\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@article{szepesi_multimodal_2024,
	title = {Multimodal assessment of social anxiety among international students},
	volume = {225},
	issn = {0191-8869},
	url = {https://www.sciencedirect.com/science/article/pii/S0191886924000977},
	doi = {10.1016/j.paid.2024.112637},
	abstract = {The present study examined Social Anxiety (SA) through the utilization of multiple measures and experimental paradigms. Participants were categorized using the Liebowitz Social Anxiety Scale (threshold: 30). The Facial Test assessed emotional facial recognition, and eye-tracking technology examined visual attention biases during emotion identification. The Social Cognitions Questionnaire was administered to investigate the relationship between biases and SA. Results revealed no gender-based differences in SA, but Asian participants scored significantly higher on the anxiety scale compared to non-Asians, challenging assumptions about lower Asian social anxiety scores and highlighting cultural influences. Emotion recognition showed no significant differences between SA and control groups. Gaze pattern analysis revealed that females focused more on eyes, aligning with gender-based emotion recognition differences. Contrary to previous findings individuals with higher social anxiety scores did not show hypervigilance towards threatening stimuli but exhibited avoidance towards them, including delayed gaze, fewer and shorter fixations, and reduced gazes directed at the eyes. Both the SA and the control group exhibited maladaptive schemas related to performance and visible signs of anxiety. Men and Asian participants showed higher belief in these schemas, challenging the notion of their lower susceptibility to social anxiety.},
	urldate = {2024-05-10},
	journal = {Personality and Individual Differences},
	author = {Szepesi, Csongor István and Böszörményi-Zelizi, Petra and Szemán-Nagy, Anita and Soós, Mihály and Horváth, Nóra and Rekenyi, Viktor and Zurashvili, Salome and Kolozsvári, László Róbert},
	month = jul,
	year = {2024},
	keywords = {Ethnical differences in SAD, Eye-tracking, Gender differences in SAD, Social anxiety disorder, Social cognition},
	pages = {112637},
}

@inproceedings{trotman_improvements_2014,
	address = {Melbourne VIC Australia},
	title = {Improvements to {BM25} and {Language} {Models} {Examined}},
	isbn = {978-1-4503-3000-8},
	url = {https://dl.acm.org/doi/10.1145/2682862.2682863},
	doi = {10.1145/2682862.2682863},
	abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TFl◦◦pID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best overall.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 2014 {Australasian} {Document} {Computing} {Symposium}},
	publisher = {ACM},
	author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
	month = nov,
	year = {2014},
	pages = {58--65},
	file = {Trotman et al. - 2014 - Improvements to BM25 and Language Models Examined.pdf:C\:\\Users\\manue\\Zotero\\storage\\RL47383L\\Trotman et al. - 2014 - Improvements to BM25 and Language Models Examined.pdf:application/pdf},
}

@inproceedings{trotman_improvements_2014-1,
	address = {Melbourne VIC Australia},
	title = {Improvements to {BM25} and {Language} {Models} {Examined}},
	isbn = {978-1-4503-3000-8},
	url = {https://dl.acm.org/doi/10.1145/2682862.2682863},
	doi = {10.1145/2682862.2682863},
	abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TFl◦◦pID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best overall.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 2014 {Australasian} {Document} {Computing} {Symposium}},
	publisher = {ACM},
	author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
	month = nov,
	year = {2014},
	pages = {58--65},
	file = {Trotman et al. - 2014 - Improvements to BM25 and Language Models Examined.pdf:C\:\\Users\\manue\\Zotero\\storage\\AGHG88GI\\Trotman et al. - 2014 - Improvements to BM25 and Language Models Examined.pdf:application/pdf},
}

@inproceedings{trotman_improvements_2014-2,
	address = {Melbourne VIC Australia},
	title = {Improvements to {BM25} and {Language} {Models} {Examined}},
	isbn = {978-1-4503-3000-8},
	url = {https://dl.acm.org/doi/10.1145/2682862.2682863},
	doi = {10.1145/2682862.2682863},
	abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TFl◦◦pID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best overall.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 2014 {Australasian} {Document} {Computing} {Symposium}},
	publisher = {ACM},
	author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
	month = nov,
	year = {2014},
	pages = {58--65},
	file = {Trotman et al. - 2014 - Improvements to BM25 and Language Models Examined.pdf:C\:\\Users\\manue\\Zotero\\storage\\EQCRLBQC\\Trotman et al. - 2014 - Improvements to BM25 and Language Models Examined.pdf:application/pdf},
}

@inproceedings{rio-torto_captions_2022,
	address = {Cham},
	title = {From {Captions} to {Explanations}: {A} {Multimodal} {Transformer}-based {Architecture} for {Natural} {Language} {Explanation} {Generation}},
	isbn = {978-3-031-04881-4},
	shorttitle = {From {Captions} to {Explanations}},
	doi = {10.1007/978-3-031-04881-4_5},
	abstract = {The growing importance of the Explainable Artificial Intelligence (XAI) field has led to the proposal of several methods for producing visual heatmaps of the classification decisions of deep learning models. However, visual explanations are not sufficient because different end-users have different backgrounds and preferences. Natural language explanations (NLEs) are inherently understandable by humans and, thus, can complement visual explanations. Therefore, we introduce a novel architecture based on multimodal Transformers to enable the generation of NLEs for image classification tasks. Contrary to the current literature, which models NLE generation as a supervised image captioning problem, we propose to learn to generate these textual explanations without their direct supervision, by starting from image captions and evolving to classification-relevant text. Preliminary experiments on a novel dataset where there is a clear demarcation between captions and NLEs show the potential of the approach and shed light on how it can be improved.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Rio-Torto, Isabel and Cardoso, Jaime S. and Teixeira, Luís F.},
	editor = {Pinho, Armando J. and Georgieva, Petia and Teixeira, Luís F. and Sánchez, Joan Andreu},
	year = {2022},
	keywords = {Explainable AI, Image captioning, Natural language explanations, Natural language generation, Transformers},
	pages = {54--65},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\TDBZ5IHD\\Rio-Torto et al. - 2022 - From Captions to Explanations A Multimodal Transf.pdf:application/pdf},
}

@incollection{dzelzkaleja_mobile_2021,
	title = {Mobile {Apps} for {3D} {Face} {Scanning}},
	isbn = {978-3-030-82195-1},
	abstract = {3D scanning is a rapidly growing field, and face scanning is a branch showing a great potential in various applications, but it is still new and little researched. As smartphone availability and functionality grows, the potential for 3D face scanning mobile apps has massively increased as well. Our study is aimed at finding and testing mobile apps that are suitable for 3D face scanning purposes. In our research we scanned two people’s faces using iOS and Android based smartphones. 6 mobile apps were found to be eligible: Bellus 3D, Heges, Scandy Pro, Capture, Trnio, 3D scanner Pro. 3D Scanner Pro were the only Android based app that was found to be suitable for face scanning purposes. Our research shows that based on visual evaluation, the most accurate face 3D models were made by Scandy pro, while Heges was better priced with similar accuracy. In the top three is also Bellus3D, which doesn’t provide the best resemblance to the scanning objects, but it was the easiest to use and process. Our quantitative measurements comparing face proportions show that the best accuracy for male face is obtained by Scandy pro and 3D Scanner Pro, but for female face – Scandy Pro and Capture.},
	author = {Dzelzkaleja, Laura and Knēts, Jēkabs and Rozenovskis, Normens and Sīlītis, Armands},
	month = aug,
	year = {2021},
	doi = {10.1007/978-3-030-82196-8_4},
	pages = {34--50},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\I4PISZNU\\Dzelzkaleja et al. - 2021 - Mobile Apps for 3D Face Scanning.pdf:application/pdf},
}

@inproceedings{gilani_learning_2018,
	title = {Learning from {Millions} of {3D} {Scans} for {Large}-scale {3D} {Face} {Recognition}},
	url = {http://arxiv.org/abs/1711.05942},
	doi = {10.1109/CVPR.2018.00203},
	abstract = {Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10\%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.},
	urldate = {2024-05-29},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gilani, Syed Zulqarnain and Mian, Ajmal},
	month = jun,
	year = {2018},
	note = {arXiv:1711.05942 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1896--1905},
	annote = {Comment: 11 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\UQIWKD87\\Gilani and Mian - 2018 - Learning from Millions of 3D Scans for Large-scale.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\K7N35R99\\1711.html:text/html},
}

@inproceedings{zulqarnain_gilani_learning_2018,
	address = {Salt Lake City, UT, USA},
	title = {Learning from {Millions} of {3D} {Scans} for {Large}-{Scale} {3D} {Face} {Recognition}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578301/},
	doi = {10.1109/CVPR.2018.00203},
	abstract = {Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not beneﬁted from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the ﬁrst deep CNN model designed speciﬁcally for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without ﬁne tuning on this dataset, our network already outperforms state of the art face recognition by over 10\%. We ﬁne tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efﬁcacy of our method for the open world face recognition problem.},
	language = {en},
	urldate = {2024-05-29},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zulqarnain Gilani, Syed and Mian, Ajmal},
	month = jun,
	year = {2018},
	pages = {1896--1905},
	file = {Zulqarnain Gilani and Mian - 2018 - Learning from Millions of 3D Scans for Large-Scale.pdf:C\:\\Users\\manue\\Zotero\\storage\\5HTK6H6F\\Zulqarnain Gilani and Mian - 2018 - Learning from Millions of 3D Scans for Large-Scale.pdf:application/pdf},
}

@misc{noauthor_captions_nodate,
	title = {From {Captions} to {Explanations}: {A} {Multimodal} {Transformer}-based...},
	shorttitle = {From {Captions} to {Explanations}},
	url = {https://openreview.net/forum?id=ecoSUZGNcPV},
	abstract = {The growing importance of the Explainable Artificial Intelligence (XAI) field has led to the proposal of several methods for producing visual heatmaps of the classification decisions of deep...},
	language = {en},
	urldate = {2024-05-29},
	journal = {OpenReview},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\UQMRGDN2\\forum.html:text/html},
}

@misc{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	language = {en},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:C\:\\Users\\manue\\Zotero\\storage\\CL3B4ULN\\Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@misc{rong_word2vec_2016,
	title = {word2vec {Parameter} {Learning} {Explained}},
	url = {http://arxiv.org/abs/1411.2738},
	abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models.},
	language = {en},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Rong, Xin},
	month = jun,
	year = {2016},
	note = {arXiv:1411.2738 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Rong - 2016 - word2vec Parameter Learning Explained.pdf:C\:\\Users\\manue\\Zotero\\storage\\AD69TDC4\\Rong - 2016 - word2vec Parameter Learning Explained.pdf:application/pdf},
}

@article{brown_mathematics_1993,
	title = {The {Mathematics} of {Statistical} {Machine} {Translation}: {Parameter} {Estimation}},
	volume = {19},
	shorttitle = {The {Mathematics} of {Statistical} {Machine} {Translation}},
	url = {https://aclanthology.org/J93-2003},
	number = {2},
	urldate = {2024-06-05},
	journal = {Computational Linguistics},
	author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Mercer, Robert L.},
	editor = {Hirschberg, Julia},
	year = {1993},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {263--311},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\ILJNWA4N\\Brown et al. - 1993 - The Mathematics of Statistical Machine Translation.pdf:application/pdf},
}

@misc{synced_automated_2017,
	title = {Automated {Inference} on {Criminality} using {Face} {Images}},
	url = {https://medium.com/syncedreview/automated-inference-on-criminality-using-face-images-aec51c312cd0},
	abstract = {In this paper, the authors build four classifiers which are the logistic regression, KNN, SVM, CNN by supervised machine learning to…},
	language = {en},
	urldate = {2024-06-07},
	journal = {SyncedReview},
	author = {Synced},
	month = nov,
	year = {2017},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\PEPFVWVY\\automated-inference-on-criminality-using-face-images-aec51c312cd0.html:text/html},
}

@misc{noauthor_new_nodate,
	title = {New message - {Text2StoryProject} - {Slack}},
	url = {https://app.slack.com/client/T01058WV3CP/dms},
	urldate = {2024-06-07},
	file = {New message - Text2StoryProject - Slack:C\:\\Users\\manue\\Zotero\\storage\\ZVS449P9\\dms.html:text/html},
}

@article{puh_predicting_2023,
	title = {Predicting stock market using natural language processing},
	volume = {38},
	issn = {1935-5181},
	url = {https://doi.org/10.1108/AJB-08-2022-0124},
	doi = {10.1108/AJB-08-2022-0124},
	abstract = {Purpose Predicting the stock market's prices has always been an interesting topic since its closely related to making money. Recently, the advances in natural language processing (NLP) have opened new perspectives for solving this task. The purpose of this paper is to show a state-of-the-art natural language approach to using language in predicting the stock market. Design/methodology/approach In this paper, the conventional statistical models for time-series prediction are implemented as a benchmark. Then, for methodological comparison, various state-of-the-art natural language models ranging from the baseline convolutional and recurrent neural network models to the most advanced transformer-based models are developed, implemented and tested. Findings Experimental results show that there is a correlation between the textual information in the news headlines and stock price prediction. The model based on the GRU (gated recurrent unit) cell with one linear layer, which takes pairs of the historical prices and the sentiment score calculated using transformer-based models, achieved the best result. Originality/value This study provides an insight into how to use NLP to improve stock price prediction and shows that there is a correlation between news headlines and stock price prediction.},
	number = {2},
	urldate = {2024-06-10},
	journal = {American Journal of Business},
	author = {Puh, Karlo and Bagić Babac, Marina},
	month = jan,
	year = {2023},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {BERT, GRU, Machine learning, Natural language processing, Recurrent neural network, Stock market prediction, Time-series analysis},
	pages = {41--61},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\GTRK8I42\\Puh and Bagić Babac - 2023 - Predicting stock market using natural language pro.pdf:application/pdf},
}

@misc{blazquez-garcia_review_2020,
	title = {A review on outlier/anomaly detection in time series data},
	url = {http://arxiv.org/abs/2002.04236},
	abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
	language = {en},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Blázquez-García, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
	month = feb,
	year = {2020},
	note = {arXiv:2002.04236 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 32 pages, 21 figures, submitted to ACM Computing Surveys (CSUR)},
	file = {Blázquez-García et al. - 2020 - A review on outlieranomaly detection in time seri.pdf:C\:\\Users\\manue\\Zotero\\storage\\GAJGIN6H\\Blázquez-García et al. - 2020 - A review on outlieranomaly detection in time seri.pdf:application/pdf},
}

@misc{braei_anomaly_2020,
	title = {Anomaly {Detection} in {Univariate} {Time}-series: {A} {Survey} on the {State}-of-the-{Art}},
	shorttitle = {Anomaly {Detection} in {Univariate} {Time}-series},
	url = {http://arxiv.org/abs/2004.00433},
	abstract = {Anomaly detection for time-series data has been an important research ﬁeld for a long time. Seminal work on anomaly detection methods has been focussing on statistical approaches. In recent years an increasing number of machine learning algorithms have been developed to detect anomalies on time-series. Subsequently, researchers tried to improve these techniques using (deep) neural networks. In the light of the increasing number of anomaly detection methods, the body of research lacks a broad comparative evaluation of statistical, machine learning and deep learning methods. This paper studies 20 univariate anomaly detection methods from the all three categories. The evaluation is conducted on publicly available datasets, which serve as benchmarks for time-series anomaly detection. By analyzing the accuracy of each method as well as the computation time of the algorithms, we provide a thorough insight about the performance of these anomaly detection approaches, alongside some general notion of which method is suited for a certain type of data.},
	language = {en},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Braei, Mohammad and Wagner, Sebastian},
	month = apr,
	year = {2020},
	note = {arXiv:2004.00433 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Braei and Wagner - 2020 - Anomaly Detection in Univariate Time-series A Sur.pdf:C\:\\Users\\manue\\Zotero\\storage\\3W4V2BDG\\Braei and Wagner - 2020 - Anomaly Detection in Univariate Time-series A Sur.pdf:application/pdf},
}

@inproceedings{dunning_practical_2014,
	title = {Practical {Machine} {Learning}: {A} {New} {Look} at {Anomaly} {Detection}},
	shorttitle = {Practical {Machine} {Learning}},
	url = {https://www.semanticscholar.org/paper/Practical-Machine-Learning%3A-A-New-Look-at-Anomaly-Dunning-Friedman/fcf31469277285efd2b36416013a0c0e4e2ee3bb},
	abstract = {Anomaly detection is the detective work of machine learning: finding the unusual, catching the fraud, discovering strange activity in large and complex datasets. But, unlike Sherlock Holmes, you may not know what the puzzle is, much less what "suspects" you're looking for. This O'Reilly report uses practical examples to explain how the underlying concepts of anomaly detection work. From banking security to natural sciences, medicine, and marketing, anomaly detection has many useful applications in this age of big data. And the search for anomalies will intensify once the Internet of Things spawns even more new types of data. The concepts described in this report will help you tackle anomaly detection in your own project. Use probabilistic models to predict what's normal and contrast that to what you observe Set an adaptive threshold to determine which data falls outside of the normal range, using the t-digest algorithm Establish normal fluctuations in complex systems and signals (such as an EKG) with a more adaptive probablistic model Use historical data to discover anomalies in sporadic event streams, such as web traffic Learn how to use deviations in expected behavior to trigger fraud alerts},
	urldate = {2024-06-11},
	author = {Dunning, T. and Friedman, Ellen},
	month = jul,
	year = {2014},
	annote = {[TLDR] Practical examples are used to explain how the underlying concepts of anomaly detection work, and how to use deviations in expected behavior to trigger fraud alerts.},
}

@inproceedings{ruff_deep_2018,
	title = {Deep {One}-{Class} {Classification}},
	url = {https://www.semanticscholar.org/paper/Deep-One-Class-Classification-Ruff-G%C3%B6rnitz/6af440915b8a0718c93be1cf61905e41e620484a},
	abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objec-tive. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.},
	urldate = {2024-06-11},
	author = {Ruff, Lukas and Görnitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Vandermeulen, Robert A. and Binder, Alexander and Müller, Emmanuel and Kloft, M.},
	month = jul,
	year = {2018},
	annote = {[TLDR] This paper introduces a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective and shows the effectiveness of the method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.},
}

@inproceedings{ruff_deep_2018-1,
	title = {Deep {One}-{Class} {Classification}},
	url = {https://proceedings.mlr.press/v80/ruff18a.html},
	abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.},
	language = {en},
	urldate = {2024-06-11},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and Müller, Emmanuel and Kloft, Marius},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {4393--4402},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\DEAXUFRW\\Ruff et al. - 2018 - Deep One-Class Classification.pdf:application/pdf;Supplementary PDF:C\:\\Users\\manue\\Zotero\\storage\\HMUTLUI2\\Ruff et al. - 2018 - Deep One-Class Classification.pdf:application/pdf},
}

@inproceedings{ruff_deep_2018-2,
	title = {Deep {One}-{Class} {Classification}},
	url = {https://proceedings.mlr.press/v80/ruff18a.html},
	abstract = {Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.},
	language = {en},
	urldate = {2024-06-11},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ruff, Lukas and Vandermeulen, Robert and Goernitz, Nico and Deecke, Lucas and Siddiqui, Shoaib Ahmed and Binder, Alexander and Müller, Emmanuel and Kloft, Marius},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {4393--4402},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\NCKKK5Z9\\Ruff et al. - 2018 - Deep One-Class Classification.pdf:application/pdf;Supplementary PDF:C\:\\Users\\manue\\Zotero\\storage\\UXBS3DBM\\Ruff et al. - 2018 - Deep One-Class Classification.pdf:application/pdf},
}

@inproceedings{sakurai_mining_2015,
	address = {New York, NY, USA},
	series = {{SIGMOD} '15},
	title = {Mining and {Forecasting} of {Big} {Time}-series {Data}},
	isbn = {978-1-4503-2758-9},
	url = {https://doi.org/10.1145/2723372.2731081},
	doi = {10.1145/2723372.2731081},
	abstract = {Given a large collection of time series, such as web-click logs, electric medical records and motion capture sensors, how can we efficiently and effectively find typical patterns? How can we statistically summarize all the sequences, and achieve a meaningful segmentation? What are the major tools for forecasting and outlier detection? Time-series data analysis is becoming of increasingly high importance, thanks to the decreasing cost of hardware and the increasing on-line processing capability. The objective of this tutorial is to provide a concise and intuitive overview of the most important tools that can help us find patterns in large-scale time-series sequences. We review the state of the art in four related fields: (1) similarity search and pattern discovery, (2) linear modeling and summarization, (3) non-linear modeling and forecasting, and (4) the extension of time-series mining and tensor analysis. The emphasis of the tutorial is to provide the intuition behind these powerful tools, which is usually lost in the technical literature, as well as to introduce case studies that illustrate their practical use.},
	urldate = {2024-06-11},
	booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Sakurai, Yasushi and Matsubara, Yasuko and Faloutsos, Christos},
	year = {2015},
	keywords = {forecasting, pattern discovery, tensors, time-series},
	pages = {919--922},
}

@article{bokde_novel_2018,
	title = {A novel imputation methodology for time series based on pattern sequence forecasting},
	volume = {116},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865518306500},
	doi = {10.1016/j.patrec.2018.09.020},
	abstract = {The Pattern Sequence Forecasting (PSF) algorithm is a previously described algorithm that identifies patterns in time series data and forecasts values using periodic characteristics of the observations. A new method for univariate time series is introduced that modifies the PSF algorithm to simultaneously forecast and backcast missing values for imputation. The imputePSF method extends PSF by characterizing repeating patterns of existing observations to provide a more precise estimate of missing values compared to more conventional methods, such as replacement with means or last observation carried forward. The imputation accuracy of imputePSF was evaluated by simulating varying amounts of missing observations with three univariate datasets. Comparisons of imputePSF with well-established methods using the same simulations demonstrated an overall reduction in error estimates. The imputePSF algorithm can produce more precise imputations on appropriate datasets, particularly those with periodic and repeating patterns.},
	urldate = {2024-06-11},
	journal = {Pattern Recognition Letters},
	author = {Bokde, Neeraj and Beck, Marcus W. and Martínez Álvarez, Francisco and Kulat, Kishore},
	month = dec,
	year = {2018},
	keywords = {Time series, Data mining, Forecasting, Imputation},
	pages = {88--96},
	file = {Accepted Version:C\:\\Users\\manue\\Zotero\\storage\\VMAKASLK\\Bokde et al. - 2018 - A novel imputation methodology for time series bas.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\manue\\Zotero\\storage\\NN5MV5SQ\\S0167865518306500.html:text/html},
}

@inproceedings{guimaraes_perfil_2024,
	address = {Santiago de Compostela, Galicia/Spain},
	title = {Perfil {Público}: {Automatic} {Generation} and {Visualization} of {Author} {Profiles} for {Digital} {News} {Media}},
	shorttitle = {Perfil {Público}},
	url = {https://aclanthology.org/2024.propor-2.27},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Computational} {Processing} of {Portuguese} - {Vol}. 2},
	publisher = {Association for Computational Lingustics},
	author = {Guimarães, Nuno and Campos, Ricardo and Jorge, Alípio},
	editor = {Gamallo, Pablo and Claro, Daniela and Teixeira, António and Real, Livy and Garcia, Marcos and Oliveira, Hugo Gonçalo and Amaro, Raquel},
	month = mar,
	year = {2024},
	pages = {186--189},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\QUDFMCYG\\Guimarães et al. - 2024 - Perfil Público Automatic Generation and Visualiza.pdf:application/pdf},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-06-16},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	pages = {311--318},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\RXTI353A\\Papineni et al. - 2002 - Bleu a Method for Automatic Evaluation of Machine.pdf:application/pdf},
}

@misc{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909},
	abstract = {Neural machine translation (NMT) models typically operate with a ﬁxed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 BLEU, respectively.},
	language = {en},
	urldate = {2024-06-16},
	publisher = {arXiv},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = jun,
	year = {2016},
	note = {arXiv:1508.07909 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: accepted at ACL 2016; new in this version: figure 3},
	file = {Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:C\:\\Users\\manue\\Zotero\\storage\\JEC4QPJX\\Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@article{beco_electrocardiogram_2022,
	title = {Electrocardiogram lead conversion from single-lead blindly-segmented signals},
	volume = {22},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-022-02063-6},
	doi = {10.1186/s12911-022-02063-6},
	abstract = {The standard configuration’s set of twelve electrocardiogram (ECG) leads is optimal for the medical diagnosis of diverse cardiac conditions. However, it requires ten electrodes on the patient’s limbs and chest, which is uncomfortable and cumbersome. Interlead conversion methods can reconstruct missing leads and enable more comfortable acquisitions, including in wearable devices, while still allowing for adequate diagnoses. Currently, methodologies for interlead ECG conversion either require multiple reference (input) leads and/or require input signals to be temporally aligned considering the ECG landmarks.},
	number = {1},
	urldate = {2024-06-17},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Beco, Sofia C. and Pinto, João Ribeiro and Cardoso, Jaime S.},
	month = nov,
	year = {2022},
	keywords = {Autoencoder, Conversion, Deep learning, Electrocardiogram (ECG), Leads, U-Net},
	pages = {314},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\HMNZK8XP\\Beco et al. - 2022 - Electrocardiogram lead conversion from single-lead.pdf:application/pdf},
}

@inproceedings{ferreira_comparison_2021,
	address = {Shenzhen, China},
	title = {A {Comparison} of {AutoML} {Tools} for {Machine} {Learning}, {Deep} {Learning} and {XGBoost}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66543-900-8},
	url = {https://ieeexplore.ieee.org/document/9534091/},
	doi = {10.1109/IJCNN52387.2021.9534091},
	abstract = {This paper presents a benchmark of supervised Automated Machine Learning (AutoML) tools. Firstly, we analyze the characteristics of eight recent open-source AutoML tools (Auto-Keras, Auto-PyTorch, Auto-Sklearn, AutoGluon, H2O AutoML, rminer, TPOT and TransmogrifAI) and describe twelve popular OpenML datasets that were used in the benchmark (divided into regression, binary and multi-class classiﬁcation tasks). Then, we perform a comparison study with hundreds of computational experiments based on three scenarios: General Machine Learning (GML), Deep Learning (DL) and XGBoost (XGB). To select the best tool, we used a lexicographic approach, considering ﬁrst the average prediction score for each task and then the computational effort. The best predictive results were achieved for GML, which were further compared with the best OpenML public results. Overall, the best GML AutoML tools obtained competitive results, outperforming the best OpenML models in ﬁve datasets. These results conﬁrm the potential of the general-purpose AutoML tools to fully automate the Machine Learning (ML) algorithm selection and tuning.},
	language = {en},
	urldate = {2024-06-18},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Ferreira, Luis and Pilastri, Andre and Martins, Carlos Manuel and Pires, Pedro Miguel and Cortez, Paulo},
	month = jul,
	year = {2021},
	pages = {1--8},
	file = {Ferreira et al. - 2021 - A Comparison of AutoML Tools for Machine Learning,.pdf:C\:\\Users\\manue\\Zotero\\storage\\PKVNDBRW\\Ferreira et al. - 2021 - A Comparison of AutoML Tools for Machine Learning,.pdf:application/pdf},
}

@misc{noauthor_xn--end--end20machine20learning20for20predicting20real20estate20prices200120introduction200220web-scraping20real20estate20data200320detecting20outliers20in20real20estate20data200420clustering20real20estate20data200520feature20creation20for20real20estate20price20prediction200620predicting20real20estate20prices200720nlp20for20real20estate20price20prediction2020part201200820nlp20for20real20estate20price20prediction2020part202200920conclusion-he724a7b_nodate,
	title = {xn--end-to-end\%20machine\%20learning\%20for\%20predicting\%20real\%20estate\%20prices\%2001\%20introduction\%2002\%20web-scraping\%20real\%20estate\%20data\%2003\%20detecting\%20outliers\%20in\%20real\%20estate\%20data\%2004\%20clustering\%20real\%20estate\%20data\%2005\%20feature\%20creation\%20for\%20real\%20estate\%20price\%20prediction\%2006\%20predicting\%20real\%20estate\%20prices\%2007\%20nlp\%20for\%20real\%20estate\%20price\%20prediction\%20\%20part\%201\%2008\%20nlp\%20for\%20real\%20estate\%20price\%20prediction\%20\%20part\%202\%2009\%20conclusion-he724a7b - {Pesquisa} {Google}},
	url = {https://www.google.com/search?q=xn--end-to-end%2520machine%2520learning%2520for%2520predicting%2520real%2520estate%2520prices%252001%2520introduction%252002%2520web-scraping%2520real%2520estate%2520data%252003%2520detecting%2520outliers%2520in%2520real%2520estate%2520data%252004%2520clustering%2520real%2520estate%2520data%252005%2520feature%2520creation%2520for%2520real%2520estate%2520price%2520prediction%252006%2520predicting%2520real%2520estate%2520prices%252007%2520nlp%2520for%2520real%2520estate%2520price%2520prediction%2520%2520part%25201%252008%2520nlp%2520for%2520real%2520estate%2520price%2520prediction%2520%2520part%25202%252009%2520conclusion-he724a7b&rlz=1C1ONGR_pt-PTPT1061PT1061&oq=xn--end-to-end%2520machine%2520learning%2520for%2520predicting%2520real%2520estate%2520prices%252001%2520introduction%252002%2520web-scraping%2520real%2520estate%2520data%252003%2520detecting%2520outliers%2520in%2520real%2520estate%2520data%252004%2520clustering%2520real%2520estate%2520data%252005%2520feature%2520creation%2520for%2520real%2520estate%2520price%2520prediction%252006%2520predicting%2520real%2520estate%2520prices%252007%2520nlp%2520for%2520real%2520estate%2520price%2520prediction%2520%2520part%25201%252008%2520nlp%2520for%2520real%2520estate%2520price%2520prediction%2520%2520part%25202%252009%2520conclusion-he724a7b&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCTI3OTM2ajBqNKgCALACAA&sourceid=chrome&ie=UTF-8},
	urldate = {2024-06-18},
	file = {xn--end-to-end%20machine%20learning%20for%20predicting%20real%20estate%20prices%2001%20introduction%2002%20web-scraping%20real%20estate%20data%2003%20detecting%20outliers%20in%20real%20estate%20data%2004%20clustering%20real%20estate%20data%2005%20feature%20creation%20for%20real%20estate%20price%20prediction%2006%20predicting%20real%20estate%20prices%2007%20nlp%20for%20real%20estate%20price%20prediction%20%20part%201%2008%20nlp%20for%20real%20estate%20price%20prediction%20%20part%202%2009%20conclusion-he724a7b - Pesquisa Google:C\:\\Users\\manue\\Zotero\\storage\\PBKWP4KJ\\search.html:text/html},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2024-06-19},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	editor = {Liu, Qun and Schlangen, David},
	month = oct,
	year = {2020},
	pages = {38--45},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\VJS6LP3Y\\Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf:application/pdf},
}

@inproceedings{miquelina_generating_2022,
	address = {Cham},
	title = {Generating a {European} {Portuguese} {BERT} {Based} {Model} {Using} {Content} from {Arquivo}.pt {Archive}},
	isbn = {978-3-031-21753-1},
	doi = {10.1007/978-3-031-21753-1_28},
	abstract = {Building a language model from free available internet information takes several steps and challenges. This new model aims to be a BERT-based language model for European Portuguese, with no specific context. The corpus was built using a web page archive infrastructure provided by Arquivo.pt and restricted to .pt domains. This paper will describe the overall process of building the corpus and training a BERT model.},
	language = {en},
	booktitle = {Intelligent {Data} {Engineering} and {Automated} {Learning} – {IDEAL} 2022},
	publisher = {Springer International Publishing},
	author = {Miquelina, Nuno and Quaresma, Paulo and Nogueira, Vítor Beires},
	editor = {Yin, Hujun and Camacho, David and Tino, Peter},
	year = {2022},
	keywords = {BERT, Arquivo.pt, Portuguese European, Vocabulary},
	pages = {280--288},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\ILBEAWCG\\Miquelina et al. - 2022 - Generating a European Portuguese BERT Based Model .pdf:application/pdf},
}

@article{lee_importance_nodate,
	title = {On the {Importance} of {Text} {Analysis} for {Stock} {Price} {Prediction}},
	abstract = {We investigate the importance of text analysis for stock price prediction. In particular, we introduce a system that forecasts companies’ stock price changes (UP, DOWN, STAY) in response to ﬁnancial events reported in 8-K documents. Our results indicate that using text boosts prediction accuracy over 10\% (relative) over a strong baseline that incorporates many ﬁnancially-rooted features. This impact is most important in the short term (i.e., the next day after the ﬁnancial event) but persists for up to ﬁve days.},
	language = {en},
	author = {Lee, Heeyoung and Surdeanu, Mihai and MacCartney, Bill and Jurafsky, Dan},
	file = {Lee et al. - On the Importance of Text Analysis for Stock Price.pdf:C\:\\Users\\manue\\Zotero\\storage\\VMNX2YX3\\Lee et al. - On the Importance of Text Analysis for Stock Price.pdf:application/pdf},
}

@article{gao_origin_nodate,
	title = {On the {Origin} of {LLMs}},
	abstract = {Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending. However, there is no comprehensive index of LLMs available. We take advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency. Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups. We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation rapidly generates a variety of visualizations, namely dendrograms, graphs, word clouds, and scatter plots. Constellation is available at the following link: https://constellation.sites.stanford.edu/.},
	language = {en},
	author = {Gao, Sarah R and Gao, Andrew K},
	file = {Gao and Gao - On the Origin of LLMs.pdf:C\:\\Users\\manue\\Zotero\\storage\\SGTBKFCZ\\Gao and Gao - On the Origin of LLMs.pdf:application/pdf},
}

@article{gao_origin_nodate-1,
	title = {On the {Origin} of {LLMs}},
	abstract = {Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending. However, there is no comprehensive index of LLMs available. We take advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency. Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups. We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation rapidly generates a variety of visualizations, namely dendrograms, graphs, word clouds, and scatter plots. Constellation is available at the following link: https://constellation.sites.stanford.edu/.},
	language = {en},
	author = {Gao, Sarah R and Gao, Andrew K},
	file = {Gao and Gao - On the Origin of LLMs.pdf:C\:\\Users\\manue\\Zotero\\storage\\FZAW7G2D\\Gao and Gao - On the Origin of LLMs.pdf:application/pdf},
}

@article{loureiro_outlier_nodate,
	title = {Outlier {Detection} {Using} {Clustering} {Methods}: a data cleaning application},
	abstract = {This paper describes a methodology for the application of hierarchical clustering methods to the task of outlier detection. The methodology is tested on the problem of cleaning Oﬃcial Statistics data. The goal is to detect erroneous foreign trade transactions in data collected by the Portuguese Institute of Statistics (INE). These transactions are a minority, but still they have an important impact on the statistics produced by the institute. The task of detecting these rare errors is a manual, time-consuming task. Our methodology is able to save a large amount of time by selecting a small subset of suspicious transactions for manual inspection, which, nevertheless, includes most of the erroneous transactions. In this study we compare several alternative hierarchical clustering methodologies for this task. The results we have obtained conﬁrm the validity of the use of hierarchical clustering techniques for this task. Moreover, our results when compared to previous approaches to the same data, clearly outperform them, identifying the same level of erroneous transactions with signiﬁcantly less manual inspection.},
	language = {en},
	author = {Loureiro, Antonio and Torgo, Luis and Soares, Carlos},
	file = {Loureiro et al. - Outlier Detection Using Clustering Methods a data.pdf:C\:\\Users\\manue\\Zotero\\storage\\M9W5FZNZ\\Loureiro et al. - Outlier Detection Using Clustering Methods a data.pdf:application/pdf},
}

@misc{liu_are_2024,
	title = {Are {LLMs} {Capable} of {Data}-based {Statistical} and {Causal} {Reasoning}? {Benchmarking} {Advanced} {Quantitative} {Reasoning} with {Data}},
	shorttitle = {Are {LLMs} {Capable} of {Data}-based {Statistical} and {Causal} {Reasoning}?},
	url = {http://arxiv.org/abs/2402.17644},
	abstract = {Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRDATA) benchmark, aiming to evaluate Large Language Models’ capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models’ quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRTEXT. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58\%, which has much room for improvement. Among open-source models, Deepseekcoder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37\%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.},
	language = {en},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Liu, Xiao and Wu, Zirui and Wu, Xueqing and Lu, Pan and Chang, Kai-Wei and Feng, Yansong},
	month = jun,
	year = {2024},
	note = {arXiv:2402.17644 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Findings of ACL 2024. Project website: https://xxxiaol.github.io/QRData/},
	file = {Liu et al. - 2024 - Are LLMs Capable of Data-based Statistical and Cau.pdf:C\:\\Users\\manue\\Zotero\\storage\\INWPKCBG\\Liu et al. - 2024 - Are LLMs Capable of Data-based Statistical and Cau.pdf:application/pdf},
}

@misc{liu_are_2024-1,
	title = {Are {LLMs} {Capable} of {Data}-based {Statistical} and {Causal} {Reasoning}? {Benchmarking} {Advanced} {Quantitative} {Reasoning} with {Data}},
	shorttitle = {Are {LLMs} {Capable} of {Data}-based {Statistical} and {Causal} {Reasoning}?},
	url = {http://arxiv.org/abs/2402.17644},
	abstract = {Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRDATA) benchmark, aiming to evaluate Large Language Models’ capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models’ quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRTEXT. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58\%, which has much room for improvement. Among open-source models, Deepseekcoder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37\%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.},
	language = {en},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Liu, Xiao and Wu, Zirui and Wu, Xueqing and Lu, Pan and Chang, Kai-Wei and Feng, Yansong},
	month = jun,
	year = {2024},
	note = {arXiv:2402.17644 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Findings of ACL 2024. Project website: https://xxxiaol.github.io/QRData/},
	file = {Liu et al. - 2024 - Are LLMs Capable of Data-based Statistical and Cau.pdf:C\:\\Users\\manue\\Zotero\\storage\\2QLQSZLI\\Liu et al. - 2024 - Are LLMs Capable of Data-based Statistical and Cau.pdf:application/pdf},
}

@incollection{kyllonen_reasoning_2020,
	title = {Reasoning {Abilities}},
	isbn = {978-0-19-026409-3},
	url = {https://oxfordre.com/education/display/10.1093/acrefore/9780190264093.001.0001/acrefore-9780190264093-e-878},
	abstract = {"Reasoning Abilities" published on  by Oxford University Press.},
	language = {en},
	urldate = {2024-08-01},
	booktitle = {Oxford {Research} {Encyclopedia} of {Education}},
	author = {Kyllonen, Patrick C.},
	month = jul,
	year = {2020},
	doi = {10.1093/acrefore/9780190264093.013.878},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\658PQ92P\\acrefore-9780190264093-e-878.html:text/html},
}

@article{machado_time_2024,
	title = {Time series clustering to improve one-class classifier performance},
	volume = {243},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423033973},
	doi = {10.1016/j.eswa.2023.122895},
	abstract = {The improvement of one-class classifiers’ performance through clustering of multivariate time series is considered in this paper. Datasets arising from real processes come from the available sensors and are affected by many factors, such as aging of the process, changes in the operation region, and equipment malfunction. Despite that, one expects that the classes represented by such diverse data can be unveiled via trained classifiers. This work hypothesizes that the overall performance can be improved by training sets of one-class classifiers with subsets of data clustered by similarity. The proposed method is applied to one class classifiers since they are trained only with the target class, which is clustered based on time series similarity using Dynamic Time Warping and k-means. The advantages of the techniques are illustrated through their application to a public dataset from the oil industry with instances characterizing eight classes of data represented by five time series. Seven classes are selected to train LSTM classifiers using the variables and instances clustered using time series clustering algorithms. The results show that the increase in the similarity of training data tends to improve the performance of the LSTM classifier, achieving an increase of 10\% in the overall performance. In a specific case, where the clustering model raised the similarity by 84\%, the classification performance improved by 21\%.},
	urldate = {2024-08-01},
	journal = {Expert Systems with Applications},
	author = {Machado, André Paulo Ferreira and Munaro, Celso Jose and Ciarelli, Patrick Marques and Vargas, Ricardo Emanuel Vaz},
	month = jun,
	year = {2024},
	keywords = {Dynamic time warping, Long short term memory, Multivariate time series classification, Oil wells, One-class classifier, Time series clustering},
	pages = {122895},
}

@article{sarwar_incremental_nodate,
	title = {Incremental {Singular} {Value} {Decomposition} {Algorithms} for {Highly} {Scalable} {Recommender} {Systems}},
	abstract = {We investigate the use of dimensionality reduction to improve the performance for a new class of data analysis software called “recommender systems”. Recommender systems apply knowledge discovery techniques to the problem of making personalized product recommendations during a live customer interaction. The tremendous growth of customers and products in recent years poses some key challenges for recommender systems. These are: producing high quality recommendations and performing many recommendations per second for millions of customers and products. Singular Value Decomposition(SVD)-based recommendation algorithms can quickly produce high quality recommendations, but has to undergo very expensive matrix factorization steps. In this paper, we propose and experimentally validate a technique that has the potential to incrementally build SVD-based models and promises to make the recommender systems highly scalable.},
	language = {en},
	author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
	file = {Sarwar et al. - Incremental Singular Value Decomposition Algorithm.pdf:C\:\\Users\\manue\\Zotero\\storage\\RPHZZJ6R\\Sarwar et al. - Incremental Singular Value Decomposition Algorithm.pdf:application/pdf},
}

@inproceedings{feng_language-agnostic_2022,
	address = {Dublin, Ireland},
	title = {Language-agnostic {BERT} {Sentence} {Embedding}},
	url = {https://aclanthology.org/2022.acl-long.62},
	doi = {10.18653/v1/2022.acl-long.62},
	abstract = {While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80\%. Composing the best of these methods produces a model that achieves 83.7\% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5\% achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.},
	urldate = {2024-08-26},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {878--891},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\J4KPZ8N8\\Feng et al. - 2022 - Language-agnostic BERT Sentence Embedding.pdf:application/pdf},
}

@misc{noauthor_savedio_nodate,
	title = {Saved.io},
	url = {https://saved.io/edit/n2VLXZ},
	urldate = {2024-09-08},
	file = {Saved.io:C\:\\Users\\manue\\Zotero\\storage\\9PGYJBLM\\n2VLXZ.html:text/html},
}

@misc{noauthor_quimica_nodate,
	title = {A {Química} do {Amor}},
	url = {https://ensina.rtp.pt/artigo/a-quimica-do-amor/},
	language = {pt-PT},
	urldate = {2024-09-13},
	journal = {RTP Ensina},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\4HXMDDG4\\a-quimica-do-amor.html:text/html},
}

@article{hoyer_what_2001,
	title = {What is quality?},
	volume = {34},
	abstract = {The strategies and tools for assuring quality may have changed, but basic customer expectations have been fairly constant for a long time. Although the demand for quality has been part of human nature for a long time, the quantification of quality and establishment of formal quality standards are decidedly 20th century phenomena. This article looks at the writings of eight quality gurus to show what they mean when they use the word "quality." It is emphasized that even though there may be a great deal of agreement among the gurus, they do not agree on a consensus definition.},
	journal = {Quality Progress},
	author = {Hoyer, R.W. and Hoyer, B.B.Y.},
	month = jul,
	year = {2001},
	pages = {52--62},
}

@mastersthesis{leite_concecao_2022,
	title = {Conceção e implementação de um {Sistema} de {Gestão} da {Qualidade}: o caso de uma microempresa},
	copyright = {openAccess},
	shorttitle = {Conceção e implementação de um {Sistema} de {Gestão} da {Qualidade}},
	url = {https://repositorium.sdum.uminho.pt/handle/1822/78516},
	abstract = {Os fenómenos de globalização e avanço tecnológico, bem como a pandemia, afetam negativamente as 
empresas de todas as indústrias, independentemente da sua dimensão. É então necessário procurar 
soluções que permitam às empresas manterem-se competitivas, mitigando as adversidades existentes.
No caso da microempresa em estudo (ACRIPAL), foram identificados dois problemas críticos – a falta de 
informação documentada e a falta de estrutura organizacional – que estavam a dificultar o 
desenvolvimento da mesma. No entanto, todos os elementos possuíam o mesmo objetivo: evoluir a 
organização. Desta forma, o Sistema de Gestão da Qualidade (SGQ) surge como uma ferramenta capaz 
de potenciar o crescimento sustentável e o aumento de desempenho da ACRIPAL, de forma a 
corresponder continuamente às necessidades e expectativas dos seus clientes e partes interessadas.
Para tal, o projeto focou-se na conceção e implementação de um SGQ ajustado à dimensão de 
microempresa apresentada pela ACRIPAL. Na fase de conceção, foram estipulados 55 documentos 
necessários para satisfazer os requisitos da norma ISO 9001:2015. Dos 55 documentos, apenas foram 
elaborados e implementados 35 documentos, o que confere um total de 64\% do SGQ implementado. 
No entanto, o presente projeto de dissertação permitiu implementar uma abordagem por processos, 
definir objetivos estratégicos e operacionais, definir indicadores de desempenho, aumentar a quantidade 
de informação documentada, aumentar a integração de informação, diminuir as falhas de comunicação 
e de informação, aumentar a motivação dos colaboradores e, consequentemente, criar uma estrutura 
organizacional estável. Todos estes fatores atualmente existentes possibilitarão o crescimento sustentável 
que a organização tem como objetivo. Analisando os registos dos últimos 3 meses do projeto, foi ainda 
possível observar que a organização cumpriu em 81\% com os seus objetivos de gestão. Portanto, o 
projeto auxiliou a organização a fortalecer o seu desempenho, aumentando a qualidade dos produtos e 
serviços. 
Concluindo, é possível conceber e implementar um SGQ em microempresas, independentemente do seu 
setor ou sistema produtivo apresentado, possibilitando o alcance de benefícios quantitativos e qualitativos 
a curto e longo prazo, que permitem aumentar o seu desempenho.},
	language = {por},
	urldate = {2024-09-13},
	author = {Leite, Beatriz Alexandra Pires},
	month = jan,
	year = {2022},
	note = {Accepted: 2022-06-28T14:35:06Z
Journal Abbreviation: Design and implementation of a Quality Management System: the case of a micro company},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\6E78XVKN\\Leite - 2022 - Conceção e implementação de um Sistema de Gestão d.pdf:application/pdf},
}

@mastersthesis{luis_laboratorios_2022,
	title = {Laboratórios de análises clínicas: sistemas de gestão da qualidade interna},
	copyright = {openAccess},
	shorttitle = {Laboratórios de análises clínicas},
	url = {https://repositorio.ipl.pt/handle/10400.21/15203},
	abstract = {Nos laboratórios de análises clínicas, a gestão da qualidade interna é crucial para o bom funcionamento destas entidades sendo a sua implementação, uma tarefa complexa devido à especialização do produto em saúde. É crucial porque permite identificar, prevenir e corrigir as possíveis falhas, garantindo a exatidão dos resultados do laboratório através da aplicação de metodologias de controlo de qualidade interno. Para a aplicação destas metodologias é necessário um Sistema de Gestão da Qualidade interna baseado nas Normas ISO 9001:2015 e ISO 15189:2014. O principal objetivo deste trabalho foi identificar, qual o sistema de gestão da qualidade interna utilizado num laboratório de análises clínicas em Portugal comparando um laboratório do setor publico com um laboratório do privado e avaliando efeito dos processos de certificação/acreditação da qualidade na melhoria contínua dos laboratórios de análises clínicas nacionais. Realizou-se um estudo descritivo de tipo estudo de caso múltiplo com contornos exploratórios que analisa uma questão real no contexto em que está inserida. A amostra incluiu 2 laboratórios de análises clínicas nacionais, um público e um privado com sistema de gestão da qualidade implementado. Os instrumentos para a recolha de dados foram um questionário respondido pelos profissionais de saúde do núcleo operacional e entrevistas individuais aos gestores de topo da qualidade. O tratamento de dados obtidos pelo questionário teve avaliação quantitativa, foram respondidos através da plataforma Google e utilizou-se estatística descritiva e os dados obtidos através da entrevista teve avaliação qualitativa e foram extraídos através de análise de conteúdo. Os participantes foram tratados de forma anónima, através da criação de códigos. Verificou-se que tanto o laboratório público como o privado seguem a Norma ISO 9001:2015, existindo opiniões divergentes referente à Norma ISO 15189:2014. Observou-se com a triangulação dos resultados entre os gestores de topo e o núcleo operacional, estes seguem a Norma ISO9001:2015.},
	language = {por},
	urldate = {2024-09-13},
	school = {Instituto Politécnico de Lisboa, Escola Superior de Tecnologia da Saúde de Lisboa},
	author = {Luís, Ana Filipa Pimpão},
	year = {2022},
	note = {Accepted: 2022-12-28T11:20:25Z},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\WQMCLYD8\\Luís - 2022 - Laboratórios de análises clínicas sistemas de ges.pdf:application/pdf},
}

@misc{noauthor_iso_nodate,
	title = {{ISO} - {About} {ISO}},
	url = {https://www.iso.org/about},
	abstract = {ISO brings together global experts to develop International Standards that help solve problems and drive innovation.},
	language = {en},
	urldate = {2024-09-13},
	journal = {ISO},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\6IHB7W23\\about.html:text/html},
}

@misc{noauthor_ipq_nodate,
	title = {{IPQ}},
	url = {https://www.ipq.pt/loja/normas/norma/2e2c14c3-5bd7-ec11-a7b5-0022489eb1bc/},
	urldate = {2024-09-13},
	file = {IPQ:C\:\\Users\\manue\\Zotero\\storage\\T3DXQD2P\\2e2c14c3-5bd7-ec11-a7b5-0022489eb1bc.html:text/html},
}

@article{noauthor_quality_nodate,
	title = {Quality management principles},
	language = {en},
	file = {Quality management principles.pdf:C\:\\Users\\manue\\Zotero\\storage\\IMPXX8ZM\\Quality management principles.pdf:application/pdf},
}

@misc{noauthor_iso_nodate-1,
	title = {{ISO} - {Certification}},
	url = {https://www.iso.org/certification.html},
	abstract = {The facts about certification, how to choose a certification body and display your certificate.},
	language = {en},
	urldate = {2024-09-13},
	journal = {ISO},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\DBCMDSMH\\certification.html:text/html},
}

@misc{noauthor_ipac_nodate,
	title = {{IPAC}},
	url = {http://www.ipac.pt/ipac/funcao.asp},
	urldate = {2024-09-13},
	file = {IPAC:C\:\\Users\\manue\\Zotero\\storage\\VNYEF9W7\\funcao.html:text/html},
}

@misc{noauthor_ipac_nodate-1,
	title = {{IPAC} - {Reconhecimento} internacional},
	url = {http://www.ipac.pt/ipac/recint.asp},
	urldate = {2024-09-13},
	file = {IPAC - Reconhecimento internacional:C\:\\Users\\manue\\Zotero\\storage\\L54NW5KM\\recint.html:text/html},
}

@article{cortez_drc001_nodate,
	title = {{DRC001} - {Regulamento} {Geral}},
	language = {pt},
	author = {Cortez, Leopoldo},
	file = {Cortez - DRC001 - Regulamento Geral.pdf:C\:\\Users\\manue\\Zotero\\storage\\D6T6HMEY\\Cortez - DRC001 - Regulamento Geral.pdf:application/pdf},
}

@misc{noauthor_ipac_nodate-2,
	title = {{IPAC} - {Reconhecimento} internacional},
	url = {http://www.ipac.pt/ipac/recint.asp},
	urldate = {2024-09-13},
	file = {IPAC - Reconhecimento internacional:C\:\\Users\\manue\\Zotero\\storage\\XLPSBLCS\\recint.html:text/html},
}

@misc{noauthor_ipac_nodate-3,
	title = {{IPAC}},
	url = {http://www.ipac.pt/pesquisa/pesq_lae.asp},
	urldate = {2024-09-13},
	file = {IPAC:C\:\\Users\\manue\\Zotero\\storage\\8X29USJE\\pesq_lae.html:text/html},
}

@article{rodrigues_gas_nodate,
	title = {{GAS} {ANALYZERS} {CALIBRATION} {BY} {DYNAMIC} {DILUTION} {FOR} {MONITORING} {AIR} {POLLUTION} {AND} {AIR} {EMISSIONS}},
	abstract = {The monitoring of air pollution and air emissions has suffered a great evolution during last decade in Portugal. In relation with this evolution, the necessary metrological control for the measurement equipments increased substantially, as required from the European Norm EN ISO/IEC 17025 [1].},
	language = {en},
	author = {Rodrigues, Nuno J F and Gomes, Paulo and Fernandes, Eduardo S and Ferreira, Carlos P and Sampaio, João},
	file = {Rodrigues et al. - GAS ANALYZERS CALIBRATION BY DYNAMIC DILUTION FOR .pdf:C\:\\Users\\manue\\Zotero\\storage\\2IWYNFQC\\Rodrigues et al. - GAS ANALYZERS CALIBRATION BY DYNAMIC DILUTION FOR .pdf:application/pdf},
}

@article{ketkar_dynamic_1994,
	title = {Dynamic dilution calibration system for calibrating analytical instruments used in gas analysis},
	volume = {141},
	url = {https://www.osti.gov/biblio/142215},
	doi = {10.1149/1.2054681},
	abstract = {A dynamic dilution system, utilizing single dilution, is described that generates sub ppb concentration of impurities in bulk nitrogen. An atmosphere pressure ionization mass spectrometer (APIMS) is used to verify the linearity of the dilution system. This system is used to obtain limits of detection of a commercially available single quadrupole APIMS for detecting methane, moisture, oxygen, and carbon dioxide in high purity nitrogen.},
	language = {English},
	number = {1},
	urldate = {2024-09-13},
	journal = {Journal of the Electrochemical Society},
	author = {Ketkar, S. N. and Scott, Jr and Martinez de Pinillos, J. V.},
	month = jan,
	year = {1994},
}

@article{rodrigues_gas_nodate-1,
	title = {{GAS} {ANALYZERS} {CALIBRATION} {BY} {DYNAMIC} {DILUTION} {FOR} {MONITORING} {AIR} {POLLUTION} {AND} {AIR} {EMISSIONS}},
	abstract = {The monitoring of air pollution and air emissions has suffered a great evolution during last decade in Portugal. In relation with this evolution, the necessary metrological control for the measurement equipments increased substantially, as required from the European Norm EN ISO/IEC 17025 [1].},
	language = {en},
	author = {Rodrigues, Nuno J F and Gomes, Paulo and Fernandes, Eduardo S and Ferreira, Carlos P and Sampaio, João},
	file = {Rodrigues et al. - GAS ANALYZERS CALIBRATION BY DYNAMIC DILUTION FOR .pdf:C\:\\Users\\manue\\Zotero\\storage\\KG4UUNIP\\Rodrigues et al. - GAS ANALYZERS CALIBRATION BY DYNAMIC DILUTION FOR .pdf:application/pdf},
}

@misc{1400-1700_iso_nodate,
	title = {{ISO} 6145-1:2019},
	shorttitle = {{ISO} 6145-1},
	url = {https://www.iso.org/standard/53598.html},
	abstract = {Gas analysis — Preparation of calibration gas mixtures using dynamic methods — Part 1: General aspects},
	language = {en},
	urldate = {2024-09-13},
	journal = {ISO},
	author = {{14:00-17:00}},
}

@misc{noauthor_ipac_nodate-4,
	title = {{IPAC}},
	url = {http://www.ipac.pt/pesquisa/ficha_lac.asp?ID=M0067},
	urldate = {2024-09-13},
	file = {IPAC:C\:\\Users\\manue\\Zotero\\storage\\5T4VKJLY\\ficha_lac.html:text/html},
}

@misc{noauthor_ipac_nodate-5,
	title = {{IPAC}},
	url = {http://www.ipac.pt/pesquisa/ficha_lae.asp?ID=L0331},
	urldate = {2024-09-13},
	file = {IPAC:C\:\\Users\\manue\\Zotero\\storage\\4MA9E2ST\\ficha_lae.html:text/html},
}

@mastersthesis{rodrigues_metrologia_2009,
	title = {Metrologia aplicada ao desenvolvimento de sistemas de medição},
	copyright = {openAccess},
	url = {https://ria.ua.pt/handle/10773/4547},
	abstract = {Metrologia, também conhecida como a “ciência das medições” é uma área
transversal a quase todos os campos da ciência e engenharia. Este tema foi
objecto de estudo neste trabalho.
No decorrer deste trabalho, foi desenvolvido, a partir de uma versão anterior,
um sistema de medição que contempla um dispositivo de diluição de gases,
um dispositivo para preparação de misturas gasosas contendo vapores e um
sistema de análise de misturas gasosas, assentes numa tecnologia de
controlo e aquisição de dados.
O sistema de medição desenvolvido encontra-se operacional e foi calibrado
de forma a estabelecer uma cadeia de rastreabilidade para cada sensor de
medida.
Foi aplicada a metodologia de cálculo de incertezas publicada no documento
GUM, designada de “lei de propagação de incertezas”, à calibração de um
sensor de temperatura, um sensor de pressão barométrica, três medidores de
caudal mássico de gás, à calibração dinâmica e calibração simples de um
sensor de CO2 (NDIR) e dois sensores de O2 (Electroquímica e Zircónio) e à
calibração dinâmica de um medidor de caudal mássico de gás de grande
capacidade.
A calibração de medidores de caudal mássico de gás foi efectuada para
alguns gases puros (Ar, CO2, N2, O2) e misturas (ar seco) com o objectivo de
determinar os factores de resposta destes sensores em função de um gás de
referência (N2).
O modelo matemático de calibração dinâmica de sensores de composição e
medidores de caudal mássico de gás, associado ao sistema de medição, foi
validado, com recurso à comparação dos resultados obtidos através das
metodologias de calibração dinâmica e calibração simples, para tal foram
utilizados equipamentos padrão e materiais de referência certificados.},
	language = {por},
	urldate = {2024-09-13},
	school = {Universidade de Aveiro},
	author = {Rodrigues, Nuno José Fernandes},
	month = dec,
	year = {2009},
	note = {Accepted: 2011-12-14T12:15:30Z},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\BVZTAV67\\Rodrigues - 2009 - Metrologia aplicada ao desenvolvimento de sistemas.pdf:application/pdf},
}

@article{wang_confidence_2015,
	title = {Confidence {Analysis} of {Standard} {Deviational} {Ellipse} and {Its} {Extension} into {Higher} {Dimensional} {Euclidean} {Space}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118537},
	doi = {10.1371/journal.pone.0118537},
	abstract = {Standard deviational ellipse (SDE) has long served as a versatile GIS tool for delineating the geographic distribution of concerned features. This paper firstly summarizes two existing models of calculating SDE, and then proposes a novel approach to constructing the same SDE based on spectral decomposition of the sample covariance, by which the SDE concept is naturally generalized into higher dimensional Euclidean space, named standard deviational hyper-ellipsoid (SDHE). Then, rigorous recursion formulas are derived for calculating the confidence levels of scaled SDHE with arbitrary magnification ratios in any dimensional space. Besides, an inexact-newton method based iterative algorithm is also proposed for solving the corresponding magnification ratio of a scaled SDHE when the confidence probability and space dimensionality are pre-specified. These results provide an efficient manner to supersede the traditional table lookup of tabulated chi-square distribution. Finally, synthetic data is employed to generate the 1-3 multiple SDEs and SDHEs. And exploratory analysis by means of SDEs and SDHEs are also conducted for measuring the spread concentrations of Hong Kong’s H1N1 in 2009.},
	language = {en},
	number = {3},
	urldate = {2024-09-13},
	journal = {PLOS ONE},
	author = {Wang, Bin and Shi, Wenzhong and Miao, Zelang},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Covariance, Ellipses, Ellipsoids, Geographic distribution, Geographic information systems, H1N1, Normal distribution},
	pages = {e0118537},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\KYYLRTDG\\Wang et al. - 2015 - Confidence Analysis of Standard Deviational Ellips.pdf:application/pdf},
}

@misc{noauthor_67_nodate,
	type = {Page},
	title = {6.7. {Carbon} moxide ({CO})},
	url = {https://www.eea.europa.eu/publications/2-9167-057-X/page024.html},
	abstract = {Information on the environment for those involved in developing, adopting, implementing and evaluating environmental policy, and also the general public},
	language = {en},
	urldate = {2024-09-13},
	journal = {European Environment Agency},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\2N5WSTGH\\page024.html:text/html},
}

@misc{noauthor_revised_2020,
	title = {Revised {ILAC} {P14} {Published}},
	url = {https://ilac.org/latest_ilac_news/revised-ilac-p14-published/},
	abstract = {The latest version of ILAC P14:09/2020 ILAC Policy for Measurement Uncertainty in Calibration has been published and is available from https://ilac.org/publications-and-resources/ilac-policy-series/ This document sets out the requirements and guidelines for the estimation and statement of uncertainty in calibration. This latest revision reflects the 2017 versions of ISO/IEC 17011 and ISO/IEC 17025. The policy continues to},
	language = {en},
	urldate = {2024-09-13},
	journal = {ILAC – ILAC Live Site},
	month = sep,
	year = {2020},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\8WQNJCHL\\revised-ilac-p14-published.html:text/html},
}

@misc{1400-1700_iso_nodate-1,
	title = {{ISO} 10156:2017},
	shorttitle = {{ISO} 10156},
	url = {https://www.iso.org/standard/66752.html},
	abstract = {Gas cylinders — Gases and gas mixtures — Determination of fire potential and oxidizing ability for the selection of cylinder valve outlets},
	language = {en},
	urldate = {2024-09-13},
	journal = {ISO},
	author = {{14:00-17:00}},
}

@misc{noauthor_portaria_nodate,
	title = {Portaria n.º 221/2012 {\textbar} {DR}},
	url = {https://diariodarepublica.pt/dr/detalhe/portaria/221-2012-179503},
	urldate = {2024-09-13},
	file = {Portaria n.º 221/2012 | DR:C\:\\Users\\manue\\Zotero\\storage\\D72CF96R\\221-2012-179503.html:text/html},
}

@misc{joshi_spanbert_2020,
	title = {{SpanBERT}: {Improving} {Pre}-training by {Representing} and {Predicting} {Spans}},
	shorttitle = {{SpanBERT}},
	url = {http://arxiv.org/abs/1907.10529},
	doi = {10.48550/arXiv.1907.10529},
	abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6\% and 88.7\% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\textbackslash}\% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
	month = jan,
	year = {2020},
	note = {arXiv:1907.10529 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted at TACL},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\REVEDYPK\\Joshi et al. - 2020 - SpanBERT Improving Pre-training by Representing a.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\ASRPCQUG\\1907.html:text/html},
}

@misc{tsang_review_2022,
	title = {Review — {SpanBERT}: {Improving} {Pre}-training by {Representing} and {Predicting} {Spans}},
	shorttitle = {Review — {SpanBERT}},
	url = {https://sh-tsang.medium.com/review-spanbert-improving-pre-training-by-representing-and-predicting-spans-da61f8a3e7b1},
	abstract = {SpanBERT, Mask a Span of Tokens for Pretraining},
	language = {en},
	urldate = {2024-09-19},
	journal = {Medium},
	author = {Tsang, Sik-Ho},
	month = nov,
	year = {2022},
}

@article{bach_doubleml_2024,
	title = {{DoubleML}: {An} {Object}-{Oriented} {Implementation} of {Double} {Machine} {Learning} in {R}},
	volume = {108},
	copyright = {Copyright (c) 2024 Philipp Bach, Malte S. Kurz, Victor Chernozhukov, Martin Spindler, Sven Klaassen},
	issn = {1548-7660},
	shorttitle = {{DoubleML}},
	url = {https://doi.org/10.18637/jss.v108.i03},
	doi = {10.18637/jss.v108.i03},
	abstract = {The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consists of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods.},
	language = {en},
	urldate = {2024-09-23},
	journal = {Journal of Statistical Software},
	author = {Bach, Philipp and Kurz, Malte S. and Chernozhukov, Victor and Spindler, Martin and Klaassen, Sven},
	month = feb,
	year = {2024},
	keywords = {Machine Learning, Causal Inference, Causal Machine Learning, mlr3, Object Orientation, R},
	pages = {1--56},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\C9XH6YNB\\Bach et al. - 2024 - DoubleML An Object-Oriented Implementation of Dou.pdf:application/pdf},
}

@article{chernozhukov_doubledebiased_2018,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	copyright = {© 2017 Royal Economic Society.},
	issn = {1368-423X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter θ0 in the presence of high-dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	language = {en},
	number = {1},
	urldate = {2024-09-23},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097},
	pages = {C1--C68},
	file = {Full Text:C\:\\Users\\manue\\Zotero\\storage\\BFV2APL5\\Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and.pdf:application/pdf},
}

@misc{bach_doubleml_2021,
	title = {{DoubleML} -- {An} {Object}-{Oriented} {Implementation} of {Double} {Machine} {Learning} in {Python}},
	url = {http://arxiv.org/abs/2104.03220},
	doi = {10.48550/arXiv.2104.03220},
	abstract = {DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.},
	urldate = {2024-09-25},
	publisher = {arXiv},
	author = {Bach, Philipp and Chernozhukov, Victor and Kurz, Malte S. and Spindler, Martin},
	month = dec,
	year = {2021},
	note = {arXiv:2104.03220 [cs, econ, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 62-04, Economics - Econometrics},
	annote = {Comment: 6 pages, 2 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\manue\\Zotero\\storage\\G39Z853P\\Bach et al. - 2021 - DoubleML -- An Object-Oriented Implementation of D.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\manue\\Zotero\\storage\\QF9GFP5P\\2104.html:text/html},
}

@misc{noauthor_causalml_nodate,
	title = {{CausalML} {Book}},
	url = {https://www.causalml-book.org/},
	urldate = {2024-09-25},
	file = {CausalML Book:C\:\\Users\\manue\\Zotero\\storage\\XAQUGGBM\\www.causalml-book.org.html:text/html},
}

@article{besancon_sneaked_nodate,
	title = {Sneaked references: {Fabricated} reference metadata distort citation counts},
	volume = {n/a},
	copyright = {© 2024 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals LLC on behalf of Association for Information Science and Technology.},
	issn = {2330-1643},
	shorttitle = {Sneaked references},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24896},
	doi = {10.1002/asi.24896},
	abstract = {We report evidence of an undocumented method to manipulate citation counts involving “sneaked” references. Sneaked references are registered as metadata for published scientific articles in which they do not appear. This manipulation exploits trusted relationships between various actors: publishers, the Crossref metadata registration agency, digital libraries, and bibliometric platforms. By collecting metadata from various sources, we show that extra undue references are actually sneaked in at Digital Object Identifier (DOI) registration time, resulting in artificially inflated citation counts. As a case study, focusing on three journals from a given publisher, we identified at least 9\% sneaked references (5978⁄65,836{\textbackslash} 5978/65,836 {\textbackslash}) mainly benefiting two authors. Despite not being present in the published articles, these sneaked references exist in metadata registries and inappropriately propagate to bibliometric dashboards. Furthermore, we discovered “lost” references: the studied bibliometric platform failed to index at least 56\% (36,939/65,836{\textbackslash} {\textbackslash}mathrm36,939/{\textbackslash}mathrm65,836 {\textbackslash}) of the references present in the HTML version of the publications. This research led to an investigation by Crossref (confirming our findings) and to subsequent corrective actions. The extent of the distortion—due to sneaked and lost references—in the global literature remains unknown and requires further investigations. Bibliometric platforms producing citation counts should identify, quantify, and correct these flaws to provide accurate data to their patrons and prevent further citation gaming.},
	language = {en},
	number = {n/a},
	urldate = {2024-09-26},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Besançon, Lonni and Cabanac, Guillaume and Labbé, Cyril and Magazinov, Alexander},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24896},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\3G2RCI4L\\Besançon et al. - Sneaked references Fabricated reference metadata .pdf:application/pdf},
}

@misc{chang_goldfish_2024,
	title = {Goldfish: {Monolingual} {Language} {Models} for 350 {Languages}},
	shorttitle = {Goldfish},
	url = {http://arxiv.org/abs/2408.10441},
	abstract = {For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24\% of languages in XGLM 4.5B; 43\% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pretrain and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10× smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in lowresource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.},
	language = {en},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Chang, Tyler A. and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K.},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10441 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Chang et al. - 2024 - Goldfish Monolingual Language Models for 350 Lang.pdf:C\:\\Users\\manue\\Zotero\\storage\\UWMNQMWW\\Chang et al. - 2024 - Goldfish Monolingual Language Models for 350 Lang.pdf:application/pdf},
}

@misc{chang_goldfish_2024-1,
	title = {Goldfish: {Monolingual} {Language} {Models} for 350 {Languages}},
	shorttitle = {Goldfish},
	url = {http://arxiv.org/abs/2408.10441},
	abstract = {For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24\% of languages in XGLM 4.5B; 43\% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pretrain and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10× smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in lowresource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.},
	language = {en},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Chang, Tyler A. and Arnett, Catherine and Tu, Zhuowen and Bergen, Benjamin K.},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10441 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Chang et al. - 2024 - Goldfish Monolingual Language Models for 350 Lang.pdf:C\:\\Users\\manue\\Zotero\\storage\\VKVHPMCY\\Chang et al. - 2024 - Goldfish Monolingual Language Models for 350 Lang.pdf:application/pdf},
}

@misc{elfes_mapping_2024,
	title = {Mapping {News} {Narratives} {Using} {LLMs} and {Narrative}-{Structured} {Text} {Embeddings}},
	url = {http://arxiv.org/abs/2409.06540},
	abstract = {Given the profound impact of narratives across various societal levels, from personal identities to international politics, it is crucial to understand their distribution and development over time. This is particularly important in online spaces. On the Web, narratives can spread rapidly and intensify societal divides and conflicts. While many qualitative approaches exist, quantifying narratives remains a significant challenge. Computational narrative analysis lacks frameworks that are both comprehensive and generalizable. To address this gap, we introduce a numerical narrative representation grounded in structuralist linguistic theory. Chiefly, Greimas’ Actantial Model represents a narrative through a constellation of six functional character roles. These so-called actants are genreagnostic, making the model highly generalizable. We extract the actants using an open-source LLM and integrate them into a Narrative-Structured Text Embedding that captures both the semantics and narrative structure of a text. We demonstrate the analytical insights of the method on the example of 5000 full-text news articles from Al Jazeera and The Washington Post on the Israel-Palestine conflict. Our method successfully distinguishes articles that cover the same topics but differ in narrative structure.},
	language = {en},
	urldate = {2024-09-27},
	publisher = {arXiv},
	author = {Elfes, Jan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.06540 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 13 figures, 4 tables},
	file = {Elfes - 2024 - Mapping News Narratives Using LLMs and Narrative-S.pdf:C\:\\Users\\manue\\Zotero\\storage\\2J8E64EC\\Elfes - 2024 - Mapping News Narratives Using LLMs and Narrative-S.pdf:application/pdf},
}

@article{wang_expression_2019,
	title = {Expression of {Concern}: {Facial} feature discovery for ethnicity recognition},
	volume = {9},
	copyright = {© 2018 Wiley Periodicals, Inc.},
	issn = {1942-4795},
	shorttitle = {Expression of {Concern}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1278},
	doi = {10.1002/widm.1278},
	abstract = {The salient facial feature discovery is one of the important research tasks in ethnical group face recognition. In this paper, we first construct an ethnical group face dataset including Chinese Uyghur, Tibetan, and Korean. Then, we show that the effective sparse sensing approach to general face recognition is not working anymore for ethnical group facial recognition if the features based on whole face image are used. This is partially due to a fact that each ethnical group may have its own characteristics manifesting only in specified face regions. Therefore, we will analyze the particularity of three ethnical groups and aim to find the common characterizations in some local regions for the three ethnical groups. For this purpose, we first use the facial landmark detector STASM to find some important landmarks in a face image, then, we use the well-known data mining technique, the mRMR algorithm, to select the salient geometric length features based on all possible lines connected by any two landmarks. Second, based on these selected salient features, we construct three “T” regions in a face image for ethnical feature representation and prove them to be effective areas for ethnicity recognition. Finally, some extensive experiments are conducted and the results reveal that the proposed “T” regions with extracted features are quite effective for ethnical group facial recognition when the L2-norm is adopted using the sparse sensing approach. In comparison to face recognition, the proposed three “T” regions are evaluated on the olivetti research laboratory face dataset, and the results show that the constructed “T” regions for ethnicity recognition are not suitable for general face recognition. This article is categorized under: Algorithmic Development {\textgreater} Structure Discovery Algorithmic Development {\textgreater} Biological Data Mining Fundamental Concepts of Data and Knowledge {\textgreater} Knowledge Representation Technologies {\textgreater} Classification},
	language = {en},
	number = {1},
	urldate = {2024-10-01},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Wang, Cunrui and Zhang, Qingling and Liu, Wanquan and Liu, Yu and Miao, Lixin},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1278},
	keywords = {facial ethnic feature, feature discovery, sparse coding, sparse representation},
	pages = {e1278},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\CM4MEWC4\\Wang et al. - 2019 - Expression of Concern Facial feature discovery fo.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\X5WWIJ3P\\widm.html:text/html},
}

@article{wang_expression_2019-1,
	title = {Expression of {Concern}: {Facial} feature discovery for ethnicity recognition},
	volume = {9},
	issn = {1942-4795},
	shorttitle = {Expression of {Concern}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1278},
	doi = {10.1002/widm.1278},
	abstract = {The salient facial feature discovery is one of the important research tasks in ethnical group face recognition. In this paper, we first construct an ethnical group face dataset including Chinese Uyghur, Tibetan, and Korean. Then, we show that the effective sparse sensing approach to general face recognition is not working anymore for ethnical group facial recognition if the features based on whole face image are used. This is partially due to a fact that each ethnical group may have its own characteristics manifesting only in specified face regions. Therefore, we will analyze the particularity of three ethnical groups and aim to find the common characterizations in some local regions for the three ethnical groups. For this purpose, we first use the facial landmark detector STASM to find some important landmarks in a face image, then, we use the well-known data mining technique, the mRMR algorithm, to select the salient geometric length features based on all possible lines connected by any two landmarks. Second, based on these selected salient features, we construct three “T” regions in a face image for ethnical feature representation and prove them to be effective areas for ethnicity recognition. Finally, some extensive experiments are conducted and the results reveal that the proposed “T” regions with extracted features are quite effective for ethnical group facial recognition when the L2-norm is adopted using the sparse sensing approach. In comparison to face recognition, the proposed three “T” regions are evaluated on the olivetti research laboratory face dataset, and the results show that the constructed “T” regions for ethnicity recognition are not suitable for general face recognition. This article is categorized under: Algorithmic Development {\textgreater} Structure Discovery Algorithmic Development {\textgreater} Biological Data Mining Fundamental Concepts of Data and Knowledge {\textgreater} Knowledge Representation Technologies {\textgreater} Classification},
	language = {en},
	number = {1},
	urldate = {2024-10-01},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Wang, Cunrui and Zhang, Qingling and Liu, Wanquan and Liu, Yu and Miao, Lixin},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1278},
	keywords = {facial ethnic feature, feature discovery, sparse coding, sparse representation},
	pages = {e1278},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\IZ4V9GYE\\widm.html:text/html},
}

@article{wang_expression_2019-2,
	title = {Expression of {Concern}: {Facial} feature discovery for ethnicity recognition},
	volume = {9},
	copyright = {© 2018 Wiley Periodicals, Inc.},
	issn = {1942-4795},
	shorttitle = {Expression of {Concern}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1278},
	doi = {10.1002/widm.1278},
	abstract = {The salient facial feature discovery is one of the important research tasks in ethnical group face recognition. In this paper, we first construct an ethnical group face dataset including Chinese Uyghur, Tibetan, and Korean. Then, we show that the effective sparse sensing approach to general face recognition is not working anymore for ethnical group facial recognition if the features based on whole face image are used. This is partially due to a fact that each ethnical group may have its own characteristics manifesting only in specified face regions. Therefore, we will analyze the particularity of three ethnical groups and aim to find the common characterizations in some local regions for the three ethnical groups. For this purpose, we first use the facial landmark detector STASM to find some important landmarks in a face image, then, we use the well-known data mining technique, the mRMR algorithm, to select the salient geometric length features based on all possible lines connected by any two landmarks. Second, based on these selected salient features, we construct three “T” regions in a face image for ethnical feature representation and prove them to be effective areas for ethnicity recognition. Finally, some extensive experiments are conducted and the results reveal that the proposed “T” regions with extracted features are quite effective for ethnical group facial recognition when the L2-norm is adopted using the sparse sensing approach. In comparison to face recognition, the proposed three “T” regions are evaluated on the olivetti research laboratory face dataset, and the results show that the constructed “T” regions for ethnicity recognition are not suitable for general face recognition. This article is categorized under: Algorithmic Development {\textgreater} Structure Discovery Algorithmic Development {\textgreater} Biological Data Mining Fundamental Concepts of Data and Knowledge {\textgreater} Knowledge Representation Technologies {\textgreater} Classification},
	language = {en},
	number = {1},
	urldate = {2024-10-01},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Wang, Cunrui and Zhang, Qingling and Liu, Wanquan and Liu, Yu and Miao, Lixin},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1278},
	keywords = {facial ethnic feature, feature discovery, sparse coding, sparse representation},
	pages = {e1278},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\X5PQXVIR\\Wang et al. - 2019 - Expression of Concern Facial feature discovery fo.pdf:application/pdf},
}

@inproceedings{batista_automated_2021,
	address = {Cham},
	title = {Automated {Housing} {Price} {Valuation} and {Spatial} {Data}},
	isbn = {978-3-030-86973-1},
	doi = {10.1007/978-3-030-86973-1_26},
	abstract = {The demand for automated, reliable and understandable housing price valuation mechanisms is increasing. Most efforts have been made to improve model accuracy and prediction power through the well-established standard econometric models based on regression techniques. However, the modelling of the spatial attributes of housing through mass appraisal tools has been given less attention. Incorporating spatial modelling approaches through econometrics frameworks opens new opportunities for improving automated valuation tools.},
	language = {en},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2021},
	publisher = {Springer International Publishing},
	author = {Batista, Paulo and Marques, João Lourenço},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blečić, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria},
	year = {2021},
	keywords = {AVM, Spatial econometric models, The housing market},
	pages = {366--381},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\X82ISE99\\Batista and Marques - 2021 - Automated Housing Price Valuation and Spatial Data.pdf:application/pdf},
}

@inproceedings{wang_glue_2018,
	address = {Brussels, Belgium},
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://aclweb.org/anthology/W18-5446},
	doi = {10.18653/v1/W18-5446},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2024-10-09},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	year = {2018},
	pages = {353--355},
	file = {Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:C\:\\Users\\manue\\Zotero\\storage\\LI37NNRU\\Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf},
}

@inproceedings{batista_automated_2021-1,
	address = {Cham},
	title = {Automated {Housing} {Price} {Valuation} and {Spatial} {Data}},
	isbn = {978-3-030-86973-1},
	doi = {10.1007/978-3-030-86973-1_26},
	abstract = {The demand for automated, reliable and understandable housing price valuation mechanisms is increasing. Most efforts have been made to improve model accuracy and prediction power through the well-established standard econometric models based on regression techniques. However, the modelling of the spatial attributes of housing through mass appraisal tools has been given less attention. Incorporating spatial modelling approaches through econometrics frameworks opens new opportunities for improving automated valuation tools.},
	language = {en},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2021},
	publisher = {Springer International Publishing},
	author = {Batista, Paulo and Marques, João Lourenço},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blečić, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria},
	year = {2021},
	keywords = {AVM, Spatial econometric models, The housing market},
	pages = {366--381},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\RU3LK94A\\Batista and Marques - 2021 - Automated Housing Price Valuation and Spatial Data.pdf:application/pdf},
}

@inproceedings{paiola_deep_2022,
	address = {Cham},
	title = {Deep {Learning}-{Based} {Abstractive} {Summarization} for {Brazilian} {Portuguese} {Texts}},
	isbn = {978-3-031-21689-3},
	doi = {10.1007/978-3-031-21689-3_34},
	abstract = {Automatic summarization captures the most relevant information and condenses it into an understandable text in natural language. Such a task can be classified as either extractive or abstractive summarization. Research on Brazilian Portuguese-based abstractive summarization is still scarce. This work explores abstractive summarization in Portuguese-based texts using a deep learning-based approach. The results are relatively satisfactory considering the ROUGE measurements and the quality of the generated summaries. Still, there are some problems regarding coherence, readability, and grammar. We strongly believe they are linked to the inherent complexity of generating an abstract and the degradation of text quality by the translation steps. These results should be seen as preliminary, serving as a basis for future research.},
	language = {en},
	booktitle = {Intelligent {Systems}},
	publisher = {Springer International Publishing},
	author = {Paiola, Pedro H. and de Rosa, Gustavo H. and Papa, João P.},
	editor = {Xavier-Junior, João Carlos and Rios, Ricardo Araújo},
	year = {2022},
	pages = {479--493},
}

@misc{savelieva_abstractive_2020,
	title = {Abstractive {Summarization} of {Spoken} and {Written} {Instructions} with {BERT}},
	url = {http://arxiv.org/abs/2008.09676},
	abstract = {Summarization of speech is a difficult problem due to the spontaneity of the flow, disfluencies, and other issues that are not usually encountered in written texts. Our work presents the first application of the BERTSum model to conversational language. We generate abstractive summaries of narrated instructional videos across a wide variety of topics, from gardening and cooking to software configuration and sports. In order to enrich the vocabulary, we use transfer learning and pretrain the model on a few large cross-domain datasets in both written and spoken English. We also do preprocessing of transcripts to restore sentence segmentation and punctuation in the output of an ASR system. The results are evaluated with ROUGE and Content-F1 scoring for the How2 and WikiHow datasets. We engage human judges to score a set of summaries randomly selected from a dataset curated from HowTo100M and YouTube. Based on blind evaluation, we achieve a level of textual fluency and utility close to that of summaries written by human content creators. The model beats current SOTA when applied to WikiHow articles that vary widely in style and topic, while showing no performance regression on the canonical CNN/DailyMail dataset. Due to the high generalizability of the model across different styles and domains, it has great potential to improve accessibility and discoverability of internet content. We envision this integrated as a feature in intelligent virtual assistants, enabling them to summarize both written and spoken instructional content upon request.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Savelieva, Alexandra and Au-Yeung, Bryan and Ramani, Vasanth},
	month = aug,
	year = {2020},
	note = {arXiv:2008.09676},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\IW6TWSCB\\Savelieva et al. - 2020 - Abstractive Summarization of Spoken and Written In.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\AWMUZ7ET\\2008.html:text/html},
}

@misc{tang_mvp_2023,
	title = {{MVP}: {Multi}-task {Supervised} {Pre}-training for {Natural} {Language} {Generation}},
	shorttitle = {{MVP}},
	url = {http://arxiv.org/abs/2206.12131},
	abstract = {Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. "supervised pre-training") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from \$77\$ datasets over \$11\$ diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on \$13\$ out of \$17\$ datasets, outperforming BART by \$9.3{\textbackslash}\%\$ and Flan-T5 by \$5.8{\textbackslash}\%\$.},
	urldate = {2024-11-15},
	publisher = {arXiv},
	author = {Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
	month = may,
	year = {2023},
	note = {arXiv:2206.12131 
version: 3},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\ILPXR3L3\\Tang et al. - 2023 - MVP Multi-task Supervised Pre-training for Natura.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\ZLK8FMJ2\\2206.html:text/html},
}

@article{liu_learning_2018,
	title = {Learning visual and textual representations for multimodal matching and classification},
	volume = {84},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318302334},
	doi = {10.1016/j.patcog.2018.07.001},
	abstract = {Multimodal learning has been an important and challenging problem for decades, which aims to bridge the modality gap between heterogeneous representations, such as vision and language. Unlike many current approaches which only focus on either multimodal matching or classification, we propose a unified network to jointly learn multimodal matching and classification (MMC-Net) between images and texts. The proposed MMC-Net model can seamlessly integrate the matching and classification components. It first learns visual and textual embedding features in the matching component, and then generates discriminative multimodal representations in the classification component. Combining the two components in a unified model can help in improving their performance. Moreover, we present a multi-stage training algorithm by minimizing both of the matching and classification loss functions. Experimental results on four well-known multimodal benchmarks demonstrate the effectiveness and efficiency of the proposed approach, which achieves competitive performance for multimodal matching and classification compared to state-of-the-art approaches.},
	urldate = {2024-11-18},
	journal = {Pattern Recognition},
	author = {Liu, Yu and Liu, Li and Guo, Yanming and Lew, Michael S.},
	month = dec,
	year = {2018},
	keywords = {Deep learning, Multimodal classification, Multimodal matching, Vision and language},
	pages = {51--67},
	file = {Accepted Version:C\:\\Users\\manue\\Zotero\\storage\\NSVV6SVW\\Liu et al. - 2018 - Learning visual and textual representations for mu.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\manue\\Zotero\\storage\\GGKFHFL4\\S0031320318302334.html:text/html},
}

@misc{noauthor_effective_nodate,
	title = {Effective {Large} {Language} {Model} {Adaptation} for {Improved} {Grounding}},
	url = {https://ar5iv.labs.arxiv.org/html/2311.09533},
	abstract = {Large language models (LLMs) have achieved remarkable advancements in natural language understanding, generation, and manipulation of text-based data.
However, one major issue towards their widespread deployment in the…},
	language = {en},
	urldate = {2024-11-18},
	journal = {ar5iv},
	file = {Snapshot:C\:\\Users\\manue\\Zotero\\storage\\XFU5L3MD\\2311.html:text/html},
}

@misc{wang_gui_2024,
	title = {{GUI} {Agents} with {Foundation} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {{GUI} {Agents} with {Foundation} {Models}},
	url = {http://arxiv.org/abs/2411.04890},
	doi = {10.48550/arXiv.2411.04890},
	abstract = {Recent advances in foundation models, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent agents being capable of performing complex tasks. By leveraging the ability of (M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents can autonomously execute user instructions by simulating human-like interactions such as clicking and typing. This survey consolidates recent research on (M)LLM-based GUI agents, highlighting key innovations in data, frameworks, and applications. We begin by discussing representative datasets and benchmarks. Next, we summarize a unified framework that captures the essential components used in prior research, accompanied by a taxonomy. Additionally, we explore commercial applications of (M)LLM-based GUI agents. Drawing from existing work, we identify several key challenges and propose future research directions. We hope this paper will inspire further developments in the field of (M)LLM-based GUI agents.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Wang, Shuai and Liu, Weiwen and Chen, Jingxuan and Gan, Weinan and Zeng, Xingshan and Yu, Shuai and Hao, Xinlong and Shao, Kun and Wang, Yasheng and Tang, Ruiming},
	month = nov,
	year = {2024},
	note = {arXiv:2411.04890},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\7EH2WWNI\\Wang et al. - 2024 - GUI Agents with Foundation Models A Comprehensive.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\2Z8YMRX9\\2411.html:text/html},
}

@misc{tang_mvp_2023-1,
	title = {{MVP}: {Multi}-task {Supervised} {Pre}-training for {Natural} {Language} {Generation}},
	shorttitle = {{MVP}},
	url = {http://arxiv.org/abs/2206.12131},
	abstract = {Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. "supervised pre-training") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from \$77\$ datasets over \$11\$ diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on \$13\$ out of \$17\$ datasets, outperforming BART by \$9.3{\textbackslash}\%\$ and Flan-T5 by \$5.8{\textbackslash}\%\$.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
	month = may,
	year = {2023},
	note = {arXiv:2206.12131 
version: 3},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\E3B3M3KP\\Tang et al. - 2023 - MVP Multi-task Supervised Pre-training for Natura.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\R26FY9NQ\\2206.html:text/html},
}

@inproceedings{carvalho_sentiment_2022,
	title = {Sentiment {Analysis} in {Portuguese} {Dialogues}},
	url = {https://www.isca-archive.org/iberspeech_2022/carvalho22_iberspeech.html},
	doi = {10.21437/IberSPEECH.2022-36},
	abstract = {Sentiment analysis in dialogue aims at detecting the sentiment expressed in the utterances of a conversation, which may improve human-computer interaction in natural language. In this paper, we explore different approaches for sentiment analysis in written Portuguese dialogues, mainly related to customer support in Telecommunications. If integrated into a conversational agent, this will enable the automatic identification and a quick reaction upon clients manifesting negative sentiments, possibly with human intervention, hopefully minimising the damage. Experiments were performed in two manually annotated real datasets: one with dialogues from the call-center of a Telecommunications company (TeleComSA); another of Twitter conversations primarily involving accounts of Telecommunications companies. We compare the performance of different machine learning approaches, from traditional to more recent, with and without considering previous utterances. The Finetuned BERT achieved the highest F1 Scores in both datasets, 0.87 in the Twitter dataset, without context, and 0.93 in the TeleComSA, considering context. These are interesting results and suggest that automated customer-support may benefit from sentiment detection. Another interesting finding was that most models did not benefit from using previous utterances, suggesting that, in this scenario, context does not contribute much, and classifying the current utterance can be enough.},
	language = {en},
	urldate = {2024-11-26},
	booktitle = {{IberSPEECH} 2022},
	publisher = {ISCA},
	author = {Carvalho, Isabel and Oliveira, Hugo Gonçalo and Silva, Catarina},
	month = nov,
	year = {2022},
	pages = {176--180},
	file = {Carvalho et al. - 2022 - Sentiment Analysis in Portuguese Dialogues.pdf:C\:\\Users\\manue\\Zotero\\storage\\SQNQBCJC\\Carvalho et al. - 2022 - Sentiment Analysis in Portuguese Dialogues.pdf:application/pdf},
}

@misc{mendonca_dialogue_2023,
	title = {Dialogue {Quality} and {Emotion} {Annotations} for {Customer} {Support} {Conversations}},
	url = {http://arxiv.org/abs/2311.13910},
	doi = {10.48550/arXiv.2311.13910},
	abstract = {Task-oriented conversational datasets often lack topic variability and linguistic diversity. However, with the advent of Large Language Models (LLMs) pretrained on extensive, multilingual and diverse text data, these limitations seem overcome. Nevertheless, their generalisability to different languages and domains in dialogue applications remains uncertain without benchmarking datasets. This paper presents a holistic annotation approach for emotion and conversational quality in the context of bilingual customer support conversations. By performing annotations that take into consideration the complete instances that compose a conversation, one can form a broader perspective of the dialogue as a whole. Furthermore, it provides a unique and valuable resource for the development of text classification models. To this end, we present benchmarks for Emotion Recognition and Dialogue Quality Estimation and show that further research is needed to leverage these models in a production setting.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Mendonça, John and Pereira, Patrícia and Menezes, Miguel and Cabarrão, Vera and Farinha, Ana C. and Moniz, Helena and Carvalho, João Paulo and Lavie, Alon and Trancoso, Isabel},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13910},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\5N8PWGDG\\Mendonça et al. - 2023 - Dialogue Quality and Emotion Annotations for Custo.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\495BRQGA\\2311.html:text/html},
}

@article{moniz_empirical_2016,
	title = {Empirical analysis of the {Portuguese} governments social network},
	volume = {6},
	issn = {1869-5450, 1869-5469},
	url = {http://link.springer.com/10.1007/s13278-016-0348-7},
	doi = {10.1007/s13278-016-0348-7},
	abstract = {The Portuguese governmental network comprising all the 776 ministers and junior ministers who were part of the 19 governments between the year 1976 and 2013 is presented and analysed. The data contains information on connections concerning business and other types of organizations and, to our knowledge, there is no such extensive research in previous literature. Upon the presentation of the data, a social network analysis considering the temporal dimension is performed at three levels of granularity: network-level, subnetwork-level (political groups) and node-level. A discussion based on the results is presented. We conclude that although it ﬁts two of the four preconditions of a small-world model, the Portuguese governmental network is not a small-world network, although presenting an evolution pointing towards becoming one. Also, we use a resilience test to study the evolution of the robustness of the Portuguese governmental network, pinpointing the moment when a set of members became structurally important.},
	language = {en},
	number = {1},
	urldate = {2024-12-09},
	journal = {Social Network Analysis and Mining},
	author = {Moniz, Nuno and Louçã, Francisco and Oliveira, Márcia and Soeiro, Renato},
	month = dec,
	year = {2016},
	pages = {43},
	file = {Moniz et al. - 2016 - Empirical analysis of the Portuguese governments s.pdf:C\:\\Users\\manue\\Zotero\\storage\\TG536MCB\\Moniz et al. - 2016 - Empirical analysis of the Portuguese governments s.pdf:application/pdf},
}

@misc{cruz_evaluating_2024,
	title = {Evaluating language models as risk scores},
	url = {http://arxiv.org/abs/2407.14614},
	doi = {10.48550/arXiv.2407.14614},
	abstract = {Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to evaluate LLMs' ability to quantify ground-truth outcome uncertainty. In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks. We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products. A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks. We evaluate 17 recent LLMs across five proposed benchmark tasks. We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are widely miscalibrated. Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty. This reveals a general inability of instruction-tuned LLMs to express data uncertainty using multiple-choice answers. A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models. These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Cruz, André F. and Hardt, Moritz and Mendler-Dünner, Celestine},
	month = sep,
	year = {2024},
	note = {arXiv:2407.14614 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\3HTM8UGV\\Cruz et al. - 2024 - Evaluating language models as risk scores.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\KD2URM3E\\2407.html:text/html},
}

@article{wang_framework_2021,
	title = {Framework and deployment of a cloud-based advanced planning and scheduling system},
	volume = {70},
	issn = {0736-5845},
	url = {https://www.sciencedirect.com/science/article/pii/S0736584520302982},
	doi = {10.1016/j.rcim.2020.102088},
	abstract = {Many small and medium-sized manufacturing enterprises (SMEs) have already implemented enterprise resource planning (ERP) and manufacturing execution system (MES) and began to start the journey of cloud manufacturing; however, the high cost of hardware and software investment, implementation, and maintenance usually hinder SMEs from adopting an advanced planning and scheduling (APS) system. This paper aims to develop a cloud-based APS (C-APS) system framework, the service structure, and approach of deploying the C-APS system in a public cloud infrastructure platform and service provider or hybrid cloud platform. The package diagram is proposed for building the C-APS system's virtual factory model to improve modeling efficiency and data stability. The C-APS system is a cloud-based and object-oriented software; its simulation-based scheduling engine can generate the significant production and operations schedule, and has the characteristics of on-demand self-service, quickly expanding and adjusting to the virtual plant model. The C-APS system's application in a leading automotive part assembly company's printed circuit board production scheduling shows that the input planning data model is easy to maintain. The scheduling quality is high; the computing time is short and acceptable for practical application.},
	urldate = {2024-12-13},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Wang, Li-Chih and Chen, Chun-Chih and Liu, Jen-Li and Chu, Pei-Chun},
	month = aug,
	year = {2021},
	keywords = {Advanced Planning and Scheduling, Cloud Computing, Cloud-Based System, Industry 4.0},
	pages = {102088},
	file = {ScienceDirect Snapshot:C\:\\Users\\manue\\Zotero\\storage\\MM4AYKMS\\S0736584520302982.html:text/html},
}

@inproceedings{khalman_forumsum_2021,
	address = {Punta Cana, Dominican Republic},
	title = {{ForumSum}: {A} {Multi}-{Speaker} {Conversation} {Summarization} {Dataset}},
	shorttitle = {{ForumSum}},
	url = {https://aclanthology.org/2021.findings-emnlp.391},
	doi = {10.18653/v1/2021.findings-emnlp.391},
	abstract = {Abstractive summarization quality had large improvements since recent language pretraining techniques. However, currently there is a lack of datasets for the growing needs of conversation summarization applications. Thus we collected ForumSum, a diverse and high-quality conversation summarization dataset with human written summaries. The conversations in ForumSum dataset are collected from a wide variety of internet forums. To make the dataset easily expandable, we also release the process of dataset creation. Our experiments show that models trained on ForumSum have better zero-shot and few-shot transferability to other datasets than the existing large chat summarization dataset SAMSum. We also show that using a conversational corpus for pre-training improves the quality of the chat summarization model.},
	urldate = {2024-12-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Khalman, Misha and Zhao, Yao and Saleh, Mohammad},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4592--4599},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\5YJGXJFW\\Khalman et al. - 2021 - ForumSum A Multi-Speaker Conversation Summarizati.pdf:application/pdf},
}

@misc{zhang_bertscore_2020,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {http://arxiv.org/abs/1904.09675},
	doi = {10.48550/arXiv.1904.09675},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09675 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Code available at https://github.com/Tiiiger/bert\_score; To appear in ICLR2020},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\LEIAHJIE\\Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\YZMXXTTR\\1904.html:text/html},
}

@misc{zhang_bertscore_2020-1,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {http://arxiv.org/abs/1904.09675},
	doi = {10.48550/arXiv.1904.09675},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09675 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Code available at https://github.com/Tiiiger/bert\_score; To appear in ICLR2020},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\MAMIN5Z6\\Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\CGKWPCHE\\1904.html:text/html},
}

@misc{kryscinski_booksum_2022,
	title = {{BookSum}: {A} {Collection} of {Datasets} for {Long}-form {Narrative} {Summarization}},
	shorttitle = {{BookSum}},
	url = {http://arxiv.org/abs/2105.08209},
	doi = {10.48550/arXiv.2105.08209},
	abstract = {The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future generations of text summarization systems. We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization. Our dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Kryściński, Wojciech and Rajani, Nazneen and Agarwal, Divyansh and Xiong, Caiming and Radev, Dragomir},
	month = dec,
	year = {2022},
	note = {arXiv:2105.08209 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 12 tables, 3 figures},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\4UITFPLQ\\Kryściński et al. - 2022 - BookSum A Collection of Datasets for Long-form Na.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\6NA44PKJ\\2105.html:text/html},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\9E2QZT58\\DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\3NKUFRDY\\2501.html:text/html},
}

@misc{deepseek-ai_deepseek-r1_2025-1,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\ZDATHQCD\\DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\N8VVXEHZ\\2501.html:text/html},
}

@incollection{fensel_knowledge_2023,
	address = {Cham},
	title = {Knowledge {Bases} and {Language} {Models}: {Complementing} {Forces}},
	volume = {14244},
	isbn = {978-3-031-45071-6 978-3-031-45072-3},
	shorttitle = {Knowledge {Bases} and {Language} {Models}},
	url = {https://link.springer.com/10.1007/978-3-031-45072-3_1},
	abstract = {Large language models (LLMs), as a particular instance of generative artificial intelligence, have revolutionized natural language processing. In this invited paper, we argue that LLMs are complementary to structured data repositories such as databases or knowledge bases, which use symbolic knowledge representations. Hence, the two ways of knowledge representation will likely continue to co-exist, at least in the near future. We discuss ways that have been explored to make the two approaches work together, and point out opportunities and challenges for their symbiosis.},
	language = {en},
	urldate = {2025-01-29},
	booktitle = {Rules and {Reasoning}},
	publisher = {Springer Nature Switzerland},
	author = {Suchanek, Fabian and Luu, Anh Tuan},
	editor = {Fensel, Anna and Ozaki, Ana and Roman, Dumitru and Soylu, Ahmet},
	year = {2023},
	doi = {10.1007/978-3-031-45072-3_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--15},
	file = {Suchanek and Luu - 2023 - Knowledge Bases and Language Models Complementing.pdf:C\:\\Users\\manue\\Zotero\\storage\\JZ9VHAWI\\Suchanek and Luu - 2023 - Knowledge Bases and Language Models Complementing.pdf:application/pdf},
}

@article{suchanek_knowledge-based_nodate,
	title = {Knowledge-{Based} {Language} {Models}},
	abstract = {We present a scientific project that aims to remedy the weaknesses of Large Language Models (such as GPT etc.) by resorting to structured data (such as databases or knowledge bases). The main insight is that language models and structured data are complementary: while the models excel at conversational skills, structured data can bring factual accuracy. The project thus aims to combine the two: First, to guide language models by providing them factual input from the structured data. Second, by verifying the output of language models to make sure it is correct, safe, and conforming.},
	language = {en},
	author = {Suchanek, Fabian M and Holzenberger, Nils},
	file = {Suchanek and Holzenberger - Knowledge-Based Language Models.pdf:C\:\\Users\\manue\\Zotero\\storage\\JQ2ABK58\\Suchanek and Holzenberger - Knowledge-Based Language Models.pdf:application/pdf},
}

@misc{kryscinski_booksum_2022-1,
	title = {{BookSum}: {A} {Collection} of {Datasets} for {Long}-form {Narrative} {Summarization}},
	shorttitle = {{BookSum}},
	url = {http://arxiv.org/abs/2105.08209},
	doi = {10.48550/arXiv.2105.08209},
	abstract = {The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future generations of text summarization systems. We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization. Our dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset.},
	urldate = {2025-02-06},
	publisher = {arXiv},
	author = {Kryściński, Wojciech and Rajani, Nazneen and Agarwal, Divyansh and Xiong, Caiming and Radev, Dragomir},
	month = dec,
	year = {2022},
	note = {arXiv:2105.08209 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 12 tables, 3 figures},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\959DI335\\Kryściński et al. - 2022 - BookSum A Collection of Datasets for Long-form Na.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\RJRG2Y6L\\2105.html:text/html},
}

@inproceedings{hasan_xl-sum_2021,
	address = {Online},
	title = {{XL}-{Sum}: {Large}-{Scale} {Multilingual} {Abstractive} {Summarization} for 44 {Languages}},
	shorttitle = {{XL}-{Sum}},
	url = {https://aclanthology.org/2021.findings-acl.413/},
	doi = {10.18653/v1/2021.findings-acl.413},
	urldate = {2025-02-07},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Hasan, Tahmid and Bhattacharjee, Abhik and Islam, Md. Saiful and Mubasshir, Kazi and Li, Yuan-Fang and Kang, Yong-Bin and Rahman, M. Sohel and Shahriyar, Rifat},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {4693--4703},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\EQGKACCF\\Hasan et al. - 2021 - XL-Sum Large-Scale Multilingual Abstractive Summa.pdf:application/pdf},
}

@misc{keskar_ctrl_2019,
	title = {{CTRL}: {A} {Conditional} {Transformer} {Language} {Model} for {Controllable} {Generation}},
	shorttitle = {{CTRL}},
	url = {http://arxiv.org/abs/1909.05858},
	doi = {10.48550/arXiv.1909.05858},
	abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
	month = sep,
	year = {2019},
	note = {arXiv:1909.05858 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\9VSASM43\\Keskar et al. - 2019 - CTRL A Conditional Transformer Language Model for.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\U7HA5YZE\\1909.html:text/html},
}

@misc{carmo_ptt5_2020,
	title = {{PTT5}: {Pretraining} and validating the {T5} model on {Brazilian} {Portuguese} data},
	shorttitle = {{PTT5}},
	url = {http://arxiv.org/abs/2008.09144},
	doi = {10.48550/arXiv.2008.09144},
	abstract = {In natural language processing (NLP), there is a need for more resources in Portuguese, since much of the data used in the state-of-the-art research is in other languages. In this paper, we pretrain a T5 model on the BrWac corpus, an extensive collection of web pages in Portuguese, and evaluate its performance against other Portuguese pretrained models and multilingual models on three different tasks. We show that our Portuguese pretrained models have significantly better performance over the original T5 models. Moreover, we demonstrate the positive impact of using a Portuguese vocabulary. Our code and models are available at https://github.com/unicamp-dl/PTT5.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Carmo, Diedre and Piau, Marcos and Campiotti, Israel and Nogueira, Rodrigo and Lotufo, Roberto},
	month = oct,
	year = {2020},
	note = {arXiv:2008.09144 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\SNPZKQNK\\Carmo et al. - 2020 - PTT5 Pretraining and validating the T5 model on B.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\8E3IPPIZ\\2008.html:text/html},
}

@article{migliaccio_sports_2023,
	title = {Sports {Performance} and {Breathing} {Rate}: {What} {Is} the {Connection}? {A} {Narrative} {Review} on {Breathing} {Strategies}},
	volume = {11},
	issn = {2075-4663},
	shorttitle = {Sports {Performance} and {Breathing} {Rate}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10224217/},
	doi = {10.3390/sports11050103},
	abstract = {Breathing is a natural and necessary process for humans. At the same time, the respiratory pace and frequency can vary so much, depending on the status of the subject. Specifically, in sports, breathing can have the effect of limiting performance from a physiological point of view, or, on the other hand, breathing can regulate the psychological status of the athletes. Therefore, the aim of this narrative review is to focus on the literature about the physiological and psychological aspects of breathing pace in sports performance, merging these two aspects because they are usually considered split, in order to create a new integrated vision of breathing and sports performance. Voluntary breathing can be divided into a slow or fast pace (VSB and VFB, respectively), and their effects on both the physiological and psychological parameters are very different. VSB can benefit athletes in a variety of ways, not just physically but mentally as well. It can help improve cardiovascular fitness, reduce stress and anxiety, and improve overall health and well-being, allowing athletes to maintain focus and concentration during training and competition. VFB is normal during physical training and competition, but away from training, if it is not voluntary, it can cause feelings of anxiety, panic, dizziness, and lightheadedness and trigger a stress response in the body, affecting the athlete’s quality of life. In summary, the role of breathing in the performance of athletes should be considered, although no definitive data are available. The connection between breathing and sports performance is still unclear, but athletes can obtain benefits in focus and concentration using slow breathing strategies.},
	number = {5},
	urldate = {2025-04-15},
	journal = {Sports},
	author = {Migliaccio, Gian Mario and Russo, Luca and Maric, Mike and Padulo, Johnny},
	month = may,
	year = {2023},
	pmid = {37234059},
	pmcid = {PMC10224217},
	pages = {103},
	file = {PubMed Central Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\PKIIARKA\\Migliaccio et al. - 2023 - Sports Performance and Breathing Rate What Is the.pdf:application/pdf},
}

@misc{clark_electra_2020-1,
	title = {{ELECTRA}: {Pre}-training {Text} {Encoders} as {Discriminators} {Rather} {Than} {Generators}},
	shorttitle = {{ELECTRA}},
	url = {http://arxiv.org/abs/2003.10555},
	doi = {10.48550/arXiv.2003.10555},
	abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
	urldate = {2025-04-27},
	publisher = {arXiv},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
	month = mar,
	year = {2020},
	note = {arXiv:2003.10555 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2020},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\N34Y3C6U\\Clark et al. - 2020 - ELECTRA Pre-training Text Encoders as Discriminat.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\PCYEFCBE\\2003.html:text/html},
}

@misc{wang_stocktime_2024,
	title = {{StockTime}: {A} {Time} {Series} {Specialized} {Large} {Language} {Model} {Architecture} for {Stock} {Price} {Prediction}},
	shorttitle = {{StockTime}},
	url = {http://arxiv.org/abs/2409.08281},
	doi = {10.48550/arXiv.2409.08281},
	abstract = {The stock price prediction task holds a significant role in the financial domain and has been studied for a long time. Recently, large language models (LLMs) have brought new ways to improve these predictions. While recent financial large language models (FinLLMs) have shown considerable progress in financial NLP tasks compared to smaller pre-trained language models (PLMs), challenges persist in stock price forecasting. Firstly, effectively integrating the modalities of time series data and natural language to fully leverage these capabilities remains complex. Secondly, FinLLMs focus more on analysis and interpretability, which can overlook the essential features of time series data. Moreover, due to the abundance of false and redundant information in financial markets, models often produce less accurate predictions when faced with such input data. In this paper, we introduce StockTime, a novel LLM-based architecture designed specifically for stock price data. Unlike recent FinLLMs, StockTime is specifically designed for stock price time series data. It leverages the natural ability of LLMs to predict the next token by treating stock prices as consecutive tokens, extracting textual information such as stock correlations, statistical trends and timestamps directly from these stock prices. StockTime then integrates both textual and time series data into the embedding space. By fusing this multimodal data, StockTime effectively predicts stock prices across arbitrary look-back periods. Our experiments demonstrate that StockTime outperforms recent LLMs, as it gives more accurate predictions while reducing memory usage and runtime costs.},
	urldate = {2025-05-06},
	publisher = {arXiv},
	author = {Wang, Shengkun and Ji, Taoran and Wang, Linhan and Sun, Yanshen and Liu, Shang-Ching and Kumar, Amit and Lu, Chang-Tien},
	month = aug,
	year = {2024},
	note = {arXiv:2409.08281 [q-fin]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Quantitative Finance - Statistical Finance},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\HG8D8HFA\\Wang et al. - 2024 - StockTime A Time Series Specialized Large Languag.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\272SL55P\\2409.html:text/html},
}

@article{tan_sustainable_2024,
	title = {A {Sustainable} {Rental} {Price} {Prediction} {Model} {Based} on {Multimodal} {Input} and {Deep} {Learning}—{Evidence} from {Airbnb}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/16/15/6384},
	doi = {10.3390/su16156384},
	abstract = {In the accommodation field, reasonable pricing is crucial for hosts to maximize their profits and is also an essential factor influencing tourists’ tendency to choose. The link between price prediction and findings about the causal relationships between key indicators and prices is not well discussed in the literature. This research aims to identify comprehensive pricing determinants for sharing economy-based lodging services and utilize them for lodging price prediction. Utilizing data retrieved from InsideAirbnb, we recognized 50 variables classified into five categories: property functions, host attributes, reputation, location, and indispensable miscellaneous factors. Property descriptions and a featured image posted by hosts were also added as input to indicate price-influencing antecedents. We proposed a price prediction model by incorporating a fully connected neural network, the bidirectional encoder representations from transformers (BERT), and MobileNet with these data sources. The model was validated using 8380 Airbnb listings from Amsterdam, North Holland, Netherlands. Results reveal that our model outperforms other models with simple or fewer inputs, reaching a minimum MAPE (mean absolute percentage error) of 5.5682\%. The novelty of this study is the application of multimodal input and multiple neural networks in forecasting sharing economy accommodation prices to boost predictive performance. The findings provide useful guidance on price setting for hosts in the sharing economy that is compliant with rental market regulations, which is particularly important for sustainable hospitality growth.},
	language = {en},
	number = {15},
	urldate = {2025-05-06},
	journal = {Sustainability},
	author = {Tan, Hongbo and Su, Tian and Wu, Xusheng and Cheng, Pengzhan and Zheng, Tianxiang},
	month = jan,
	year = {2024},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {sustainable price, deep learning, multimodal input, multiple neural networks, price prediction model, sharing economy accommodation},
	pages = {6384},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\LAA9VPQJ\\Tan et al. - 2024 - A Sustainable Rental Price Prediction Model Based .pdf:application/pdf},
}

@incollection{puderbaugh_neuroplasticity_2025,
	address = {Treasure Island (FL)},
	title = {Neuroplasticity},
	copyright = {Copyright © 2025, StatPearls Publishing LLC.},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK557811/},
	abstract = {Neuroplasticity, also known as neural plasticity or brain plasticity, is a process that involves adaptive structural and functional changes to the brain. A good definition is “the ability of the nervous system to change its activity in response to intrinsic or extrinsic stimuli by reorganizing its structure, functions, or connections.” Clinically, it is the process of brain changes after injury, such as a stroke or traumatic brain injury (TBI). These changes can either be beneficial (restoration of function after injury), neutral (no change), or negative (can have pathological consequences). Neuroplasticity can be broken down into two major mechanisms: Neuronal regeneration/collateral sprouting: This includes concepts such as synaptic plasticity and neurogenesis. Functional reorganization: This includes concepts such as equipotentiality, vicariation, and diaschisis. The first mention of the term plasticity in regards to the nervous system was by William James in 1890. However, the term neural plasticity is credited to Jerzy Konorski in 1948 and was popularized by Donald Hebb in 1949.},
	language = {eng},
	urldate = {2025-05-12},
	booktitle = {{StatPearls}},
	publisher = {StatPearls Publishing},
	author = {Puderbaugh, Matt and Emmady, Prabhu D.},
	year = {2025},
	pmid = {32491743},
	file = {Printable HTML:C\:\\Users\\manue\\Zotero\\storage\\DEIZ24P6\\NBK557811.html:text/html},
}

@misc{mendes-neves_forecasting_2024,
	title = {Forecasting {Events} in {Soccer} {Matches} {Through} {Language}},
	url = {http://arxiv.org/abs/2402.06820},
	doi = {10.48550/arXiv.2402.06820},
	abstract = {This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including match prediction and analytics. Moreover, we show that LEMs provide a simulation backbone for users to build many analytics pipelines, an approach opposite to the current specialized single-purpose models. LEMs represent a pivotal advancement in soccer analytics, establishing a foundational framework for multifaceted analytics pipelines through a singular machine-learning model.},
	urldate = {2025-05-13},
	publisher = {arXiv},
	author = {Mendes-Neves, Tiago and Meireles, Luís and Mendes-Moreira, João},
	month = apr,
	year = {2024},
	note = {arXiv:2402.06820 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\7CNZZR2C\\Mendes-Neves et al. - 2024 - Forecasting Events in Soccer Matches Through Langu.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\878YJ7LP\\2402.html:text/html},
}

@misc{mendes-neves_forecasting_2024-1,
	title = {Forecasting {Events} in {Soccer} {Matches} {Through} {Language}},
	url = {http://arxiv.org/abs/2402.06820},
	doi = {10.48550/arXiv.2402.06820},
	abstract = {This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including match prediction and analytics. Moreover, we show that LEMs provide a simulation backbone for users to build many analytics pipelines, an approach opposite to the current specialized single-purpose models. LEMs represent a pivotal advancement in soccer analytics, establishing a foundational framework for multifaceted analytics pipelines through a singular machine-learning model.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Mendes-Neves, Tiago and Meireles, Luís and Mendes-Moreira, João},
	month = apr,
	year = {2024},
	note = {arXiv:2402.06820 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\P4CIZH2Z\\Mendes-Neves et al. - 2024 - Forecasting Events in Soccer Matches Through Langu.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\TYT6I56M\\2402.html:text/html},
}

@misc{minaee_large_2025,
	title = {Large {Language} {Models}: {A} {Survey}},
	shorttitle = {Large {Language} {Models}},
	url = {http://arxiv.org/abs/2402.06196},
	doi = {10.48550/arXiv.2402.06196},
	abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws {\textbackslash}cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
	month = mar,
	year = {2025},
	note = {arXiv:2402.06196 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\DSUB5UAC\\Minaee et al. - 2025 - Large Language Models A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\AQLLKIMU\\2402.html:text/html},
}

@misc{zhao_babel_2025,
	title = {Babel: {Open} {Multilingual} {Large} {Language} {Models} {Serving} {Over} 90\% of {Global} {Speakers}},
	shorttitle = {Babel},
	url = {http://arxiv.org/abs/2503.00865},
	doi = {10.48550/arXiv.2503.00865},
	abstract = {Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce \${\textbackslash}texttt\{Babel\}\$, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90\% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: \${\textbackslash}texttt\{Babel-9B\}\$, designed for efficient inference and fine-tuning, and \${\textbackslash}texttt\{Babel-83B\}\$, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Zhao, Yiran and Liu, Chaoqun and Deng, Yue and Ying, Jiahao and Aljunied, Mahani and Li, Zhaodonghui and Bing, Lidong and Chan, Hou Pong and Rong, Yu and Zhao, Deli and Zhang, Wenxuan},
	month = mar,
	year = {2025},
	note = {arXiv:2503.00865 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\KMQE2L4R\\Zhao et al. - 2025 - Babel Open Multilingual Large Language Models Ser.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\8GKES4S9\\2503.html:text/html},
}

@misc{boizard_eurobert_2025,
	title = {{EuroBERT}: {Scaling} {Multilingual} {Encoders} for {European} {Languages}},
	shorttitle = {{EuroBERT}},
	url = {http://arxiv.org/abs/2503.05500},
	doi = {10.48550/arXiv.2503.05500},
	abstract = {General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Boizard, Nicolas and Gisserot-Boukhlef, Hippolyte and Alves, Duarte M. and Martins, André and Hammal, Ayoub and Corro, Caio and Hudelot, Céline and Malherbe, Emmanuel and Malaboeuf, Etienne and Jourdan, Fanny and Hautreux, Gabriel and Alves, João and El-Haddad, Kevin and Faysse, Manuel and Peyrard, Maxime and Guerreiro, Nuno M. and Fernandes, Patrick and Rei, Ricardo and Colombo, Pierre},
	month = mar,
	year = {2025},
	note = {arXiv:2503.05500 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 28 pages, 8 figures, 13 tables},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\RYNFWA9C\\Boizard et al. - 2025 - EuroBERT Scaling Multilingual Encoders for Europe.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\IMN34RRI\\2503.html:text/html},
}

@inproceedings{antunes_european_2025,
	address = {Albuquerque, New Mexico, U.S.A.},
	title = {A {European} {Portuguese} corpus annotated for verbal idioms},
	isbn = {9798891762435},
	url = {https://aclanthology.org/2025.mwe-1.7/},
	abstract = {This paper presents the construction of VIDiom-PT, a corpus in European Portuguese annotated for verbal idioms (e.g. O Rui bateu a bota, lit.: Rui hit the boot ‘Rui died'). This linguistic resource aims to support the development of systems capable of processing such constructions in this language variety. To assist in the annotation effort, two tools were built. The first allows for the detection of possible instances of verbal idioms in texts, while the second provides a graphical interface for annotating them. This effort culminated in the annotation of a total of 5,178 instances of 747 different verbal idioms in more than 200,000 sentences in European Portuguese. A highly reliable inter-annotator agreement was achieved, using Krippendorff`s alpha for nominal data (0.869) with 5\% of the data independently annotated by 3 experts. Part of the annotated corpus is also made publicly available.},
	urldate = {2025-05-19},
	booktitle = {Proceedings of the 21st {Workshop} on {Multiword} {Expressions} ({MWE} 2025)},
	publisher = {Association for Computational Linguistics},
	author = {Antunes, David and Baptista, Jorge and Mamede, Nuno J.},
	editor = {Ojha, Atul Kr. and Giouli, Voula and Mititelu, Verginica Barbu and Constant, Mathieu and Korvel, Gražina and Doğruöz, A. Seza and Rademaker, Alexandre},
	month = may,
	year = {2025},
	pages = {58--66},
	annote = {This paper presents VIDiom-PT, a European Portuguese corpus that’s tagged for verbal idioms — those expressions where the meaning isn’t obvious from the words themselves (like “Rui hit the boot” meaning “Rui died”). They built two tools: one to spot possible idioms in text, and another to help annotate them. They ended up tagging over 5,000 instances of nearly 750 idioms in more than 200,000 sentences and made part of the corpus publicly available. The annotation was very reliable, validated by experts.
What do you want to dig into?
Summarize this work
How does the annotation tool work to identify verbal idioms?What are the main challenges when processing verbal idioms in European Portuguese?
resume-me as ideias principais
As principais ideias do documento são as seguintes:


Criação do corpus VIDiom-PT: Foi desenvolvido um corpus anotado que cobre mais de 5.000 instâncias de expressões idiomáticas verbais em Português Europeu, totalizando cerca de 750 expressões diferentes, abrangendo mais de 200.000 frases.


Foco em expressões não composicionais: Essas expressões possuem significado não dedutível das palavras que as compõem, o que torna seu processamento mais desafiador.


Ferramentas de anotação: Foram construídos dois instrumentos — um para detectar possíveis expressões idiomáticas nos textos e outro para auxiliar na anotação manual, aumentando a precisão e fiabilidade do processo de rotulagem.


Confiabilidade dos dados: A anotação contou com a validação por especialistas, atingindo uma alta concordância entre anotadores (Krippendorff’s alpha de 0.869).


Importância do recurso: O corpus pretende apoiar futuras tarefas de NLP, como identificação de idiomas, extração de significado e tradução automática do português europeu.


Desafios na identificação de expressões idiomáticas: Dificuldades incluem distinguir entre uso literal e idiomático, além de lidar com a variação linguística e a disponibilidade de recursos para essa variedade do português.


},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\6UFWCMYP\\Antunes et al. - 2025 - A European Portuguese corpus annotated for verbal .pdf:application/pdf},
}

@article{jagan_chowhaan_machine_2023,
	title = {Machine {Learning} {Approach} for {House} {Price} {Prediction}},
	volume = {16},
	doi = {10.9734/ajrcos/2023/v16i2339},
	abstract = {In our ecosystem, real estate is clearly a distinct industry. Predicting house prices, significant housing characteristics, and many other things is made a lot easier by the capacity to extract data from raw data and extract essential information. Daily fluctuations in housing costs are still present, and they occasionally rise without regard to calculations. According to research, changes in property prices frequently have an impact on both homeowners and the real estate market. To analyze the key elements and the best predictive models for home prices, literature research is conducted. The analyses' findings supported the usage of artificial neural networks, support vector regression, and linear regression as the most effective modeling techniques. Our results also imply that real estate agents and geography play important roles in determining property prices. Finding the most crucial factors affecting housing prices and identifying the best machine learning model to utilize for this research would both be greatly aided by this study, especially for housing developers and researchers.},
	journal = {Asian Journal of Research in Computer Science},
	author = {Jagan Chowhaan, Mudavath and Nitish, D. and Akash, G. and Nelli, Sreevidya and Shaik, Subhani},
	month = jun,
	year = {2023},
	pages = {54--61},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\MC3LUNV5\\Jagan Chowhaan et al. - 2023 - Machine Learning Approach for House Price Predicti.pdf:application/pdf},
}

@misc{sudalairaj_lab_2024,
	title = {{LAB}: {Large}-{Scale} {Alignment} for {ChatBots}},
	shorttitle = {{LAB}},
	url = {http://arxiv.org/abs/2403.01081},
	doi = {10.48550/arXiv.2403.01081},
	abstract = {This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instructionfollowing behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.},
	language = {en},
	urldate = {2025-05-21},
	publisher = {arXiv},
	author = {Sudalairaj, Shivchander and Bhandwaldar, Abhishek and Pareja, Aldo and Xu, Kai and Cox, David D. and Srivastava, Akash},
	month = apr,
	year = {2024},
	note = {arXiv:2403.01081 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Corresponding Author: Akash Srivastava. Equal Contribution: Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Akash Srivastava, Code: https://github.com/instructlab},
	file = {Sudalairaj et al. - 2024 - LAB Large-Scale Alignment for ChatBots.pdf:C\:\\Users\\manue\\Zotero\\storage\\X2BAXUWH\\Sudalairaj et al. - 2024 - LAB Large-Scale Alignment for ChatBots.pdf:application/pdf},
}

@misc{feurer_auto-sklearn_2022,
	title = {Auto-{Sklearn} 2.0: {Hands}-free {AutoML} via {Meta}-{Learning}},
	shorttitle = {Auto-{Sklearn} 2.0},
	url = {http://arxiv.org/abs/2007.04074},
	doi = {10.48550/arXiv.2007.04074},
	abstract = {Automated Machine Learning (AutoML) supports practitioners and researchers with the tedious task of designing machine learning pipelines and has recently achieved substantial success. In this paper, we introduce new AutoML approaches motivated by our winning submission to the second ChaLearn AutoML challenge. We develop PoSH Auto-sklearn, which enables AutoML systems to work well on large datasets under rigid time limits by using a new, simple and meta-feature-free meta-learning technique and by employing a successful bandit strategy for budget allocation. However, PoSH Auto-sklearn introduces even more ways of running AutoML and might make it harder for users to set it up correctly. Therefore, we also go one step further and study the design space of AutoML itself, proposing a solution towards truly hands-free AutoML. Together, these changes give rise to the next generation of our AutoML system, Auto-sklearn 2.0. We verify the improvements by these additions in an extensive experimental study on 39 AutoML benchmark datasets. We conclude the paper by comparing to other popular AutoML frameworks and Auto-sklearn 1.0, reducing the relative error by up to a factor of 4.5, and yielding a performance in 10 minutes that is substantially better than what Auto-sklearn 1.0 achieves within an hour.},
	urldate = {2025-06-11},
	publisher = {arXiv},
	author = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
	month = oct,
	year = {2022},
	note = {arXiv:2007.04074 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Final version as published at JMLR 23(261)},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\R4HEDPA7\\Feurer et al. - 2022 - Auto-Sklearn 2.0 Hands-free AutoML via Meta-Learn.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\ZJYPN6NT\\2007.html:text/html},
}

@article{luo_critical_2022,
	title = {A critical review of state-of-the-art chatbot designs and applications},
	volume = {12},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1434},
	doi = {10.1002/widm.1434},
	abstract = {Chatbots are intelligent conversational agents that can interact with users through natural languages. As chatbots can perform a variety of tasks, many companies have committed numerous resources to develop and deploy chatbots to enhance various business processes. However, we lack an up-to-date critical review that thoroughly examines both state-of-the-art technologies and innovative applications of chatbots. In this review, we not only critically analyze the various computational approaches used to develop state-of-the-art chatbots, but also thoroughly review the usability and applications of chatbots for various business sectors. We also identify gaps in chatbot-related studies and propose new research directions to address the shortcomings of existing studies and applications. Our review advances both academic research and practical business applications of state-of-the-art chatbots. We provide guidance for practitioners to fully realize the business value of chatbots and assist in making sensible decisions related to the development and deployment of chatbots in various business contexts. Researchers interested in the design and development of chatbots can also gain useful insights from our critical review and identify fruitful research topics and future research directions based on the research gaps discussed herein. This article is categorized under: Technologies {\textgreater} Machine Learning Application Areas {\textgreater} Business and Industry},
	language = {en},
	number = {1},
	urldate = {2025-06-13},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Luo, Bei and Lau, Raymond Y. K. and Li, Chunping and Si, Yain-Whar},
	year = {2022},
	note = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1434},
	keywords = {machine learning, deep learning, Chatbot applications, Chatbots, conversational agents},
	pages = {e1434},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\RB2GPPA8\\Luo et al. - 2022 - A critical review of state-of-the-art chatbot desi.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\DV5CAK2D\\widm.html:text/html},
}

@article{sculley_machine_nodate,
	title = {Machine {Learning}: {The} {High}-{Interest} {Credit} {Card} of {Technical} {Debt}},
	abstract = {Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning speciﬁc risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
	language = {en},
	author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	file = {Sculley et al. - Machine Learning The High-Interest Credit Card of.pdf:C\:\\Users\\manue\\Zotero\\storage\\HWCKC23N\\Sculley et al. - Machine Learning The High-Interest Credit Card of.pdf:application/pdf},
}

@inproceedings{banerjee_meteor_2005,
	address = {Ann Arbor, Michigan},
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	shorttitle = {{METEOR}},
	url = {https://aclanthology.org/W05-0909/},
	urldate = {2025-06-15},
	booktitle = {Proceedings of the {ACL} {Workshop} on {Intrinsic} and {Extrinsic} {Evaluation} {Measures} for {Machine} {Translation} and/or {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare},
	month = jun,
	year = {2005},
	pages = {65--72},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\AKF9F7AT\\Banerjee and Lavie - 2005 - METEOR An Automatic Metric for MT Evaluation with.pdf:application/pdf},
}

@article{lombe_drought_2024,
	title = {Drought {Dynamics} in {Sub}-{Saharan} {Africa}: {Impacts} and {Adaptation} {Strategies}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	shorttitle = {Drought {Dynamics} in {Sub}-{Saharan} {Africa}},
	url = {https://www.mdpi.com/2071-1050/16/22/9902},
	doi = {10.3390/su16229902},
	abstract = {The escalation in both frequency and severity of drought events has significantly amplified the vulnerability of numerous countries, particularly in developing ones, imposing substantial economic, environmental, and social pressures. This article presents a systematic review of drought occurrences in Sub-Saharan Africa (SSA), examining historical trends, current impacts, and projected future implications. Through this comprehensive assessment, a clear trend of intensifying drought phenomena emerges across SSA, leading to crop failures, drying of water sources, loss of pasture, food shortages, and an increase in food prices. This review also highlights the concerning potential for worsening conditions in certain regions, resulting in consequences such as migration, food insecurity, malnutrition, family disintegration, crop losses, and increased disease prevalence, notably HIV/AIDS. This study further reveals that current adaptation measures by governments and NGOs should be improved to effectively adapt to the diverse impacts of drought, and it contributes to a deeper understanding of drought dynamics in Sub-Saharan Africa and assesses its critical impacts on food security and social well-being. It also evaluates adaptation measures across different countries, highlighting their strengths and weaknesses and enabling quick identification of areas for improvement. Additionally, it informs resilience-building efforts in vulnerable communities.},
	language = {en},
	number = {22},
	urldate = {2025-07-11},
	journal = {Sustainability},
	author = {Lombe, Pedro and Carvalho, Elsa and Rosa-Santos, Paulo},
	month = jan,
	year = {2024},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {drought mitigation, drought severity, food security, socio-economic vulnerability, Sub-Saharan Africa, water scarcity},
	pages = {9902},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\RTEPMWJB\\Lombe et al. - 2024 - Drought Dynamics in Sub-Saharan Africa Impacts an.pdf:application/pdf},
}

@book{fares_predicting_2021,
	title = {Predicting {Predawn} {Leaf} {Water} {Potential} up to {Seven} {Days} {Using} {Machine} {Learning}},
	isbn = {978-3-030-86229-9},
	abstract = {Sustainable agricultural production requires a controlled usage of water, nutrients, and minerals from the environment. Different strategies of plant irrigation are being studied to control the quantity and quality balance of the fruits. Regarding efficient irrigation, particularly in deficit irrigation strategies, it is essential to act according to water stress status in the plant. For example, in the vine, to improve the quality of the grapes, the plants are deprived of water until they reach particular water stress before re-watered in specified phenological stages. The water status inside the plant is estimated by measuring either the Leaf Potential during the Predawn or soil water potential, along with the root zones. Measuring soil water potential has the advantage of being independent of diurnal atmospheric variations. However, this method has many logistic problems, making it very hard to apply along all the yard, especially the big ones. In this study, the Predawn Leaf Water Potential (PLWP) is daily predicted by Machine Learning models using data such as grapes variety, soil characteristics, irrigation schedules, and meteorological data. The benefits of these techniques are the reduction of the manual work of measuring PLWP and the capacity to implement those models on a larger scale by predicting PLWP up to 7 days which should enhance the ability to optimize the irrigation plan while the quantity and quality of the crop are under control.},
	author = {Fares, Ahmed and Vasconcelos, Fabio and Moreira, João and Ferreira, Carlos},
	month = sep,
	year = {2021},
	doi = {10.1007/978-3-030-86230-5_4},
	note = {Pages: 50},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\KS59HBF3\\Fares et al. - 2021 - Predicting Predawn Leaf Water Potential up to Seve.pdf:application/pdf},
}

@misc{cornacchia_path_2025,
	title = {The {Path} is the {Goal}: a {Study} on the {Nature} and {Effects} of {Shortest}-{Path} {Stability} {Under} {Perturbation} of {Destination}},
	shorttitle = {The {Path} is the {Goal}},
	url = {http://arxiv.org/abs/2506.09731},
	doi = {10.48550/arXiv.2506.09731},
	abstract = {This work examines the phenomenon of path variability in urban navigation, where small changes in destination might lead to significantly different suggested routes. Starting from an observation of this variability over the city of Barcelona, we explore whether this is a localized or widespread occurrence and identify factors influencing path variability. We introduce the concept of "path stability", a measure of how robust a suggested route is to minor destination adjustments, define a detailed experimentation process and apply it across multiple cities worldwide. Our analysis shows that path stability is shaped by city-specific factors and trip characteristics, also identifying some common patterns. Results reveal significant heterogeneity in path stability across cities, allowing for categorization into "stable" and "unstable" cities. These findings offer new insights for urban planning and traffic management, highlighting opportunities for optimizing navigation systems to enhance route consistency and urban mobility.},
	urldate = {2025-07-14},
	publisher = {arXiv},
	author = {Cornacchia, Giuliano and Nanni, Mirco},
	month = jun,
	year = {2025},
	note = {arXiv:2506.09731 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\6G4I58LB\\Cornacchia and Nanni - 2025 - The Path is the Goal a Study on the Nature and Ef.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\ETQYJIFK\\2506.html:text/html},
}

@article{cornacchia_path_2025-1,
	title = {The path is the goal: {A} study on the nature and effects of shortest-path stability under perturbation of destination},
	issn = {1573-7624},
	shorttitle = {The path is the goal},
	url = {https://doi.org/10.1007/s10707-025-00553-z},
	doi = {10.1007/s10707-025-00553-z},
	abstract = {This work examines the phenomenon of path variability in urban navigation, where small changes in destination might lead to significantly different suggested routes. Starting from an observation of this variability over the city of Barcelona, we explore whether this is a localized or widespread occurrence and identify factors influencing path variability. We introduce the concept of “path stability”, a measure of how robust a suggested route is to minor destination adjustments, define a detailed experimentation process and apply it across multiple cities worldwide. Our analysis shows that path stability is shaped by city-specific factors and trip characteristics, also identifying some common patterns. Results reveal significant heterogeneity in path stability across cities, allowing for categorization into “path-stable” and “path-unstable” cities. These findings offer new insights for urban planning and traffic management, highlighting opportunities for optimizing navigation systems to enhance route consistency and urban mobility.},
	language = {en},
	urldate = {2025-07-14},
	journal = {GeoInformatica},
	author = {Cornacchia, Giuliano and Nanni, Mirco},
	month = jun,
	year = {2025},
	keywords = {Cohesion, Navigation, Path stability, Road network, Shortest path, Transport Geography, Transportation Technology and Traffic Engineering, Urban Policy, Urban Politics, Urban structure},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\XS3Z3TBE\\Cornacchia and Nanni - 2025 - The path is the goal A study on the nature and ef.pdf:application/pdf},
}

@article{mirzaei_robust_2021,
	title = {Robust network-constrained energy management of a multiple energy distribution company in the presence of multi-energy conversion and storage technologies},
	volume = {74},
	issn = {2210-6707},
	url = {https://www.sciencedirect.com/science/article/pii/S2210670721004297},
	doi = {10.1016/j.scs.2021.103147},
	abstract = {Multi-energy systems have been developed to supply the multi-energy users economically by considering the physical limitations of different energy networks. This paper proposes a new entity called multiple energy distribution company (MEDC) to meet the electricity, gas, and heat demands of consumers in the presence of renewable energy resources (RESs) and multi-energy conversion technologies with the lowest operating cost. To achieve a more accurate scheduling model, a multi-energy flow model is used that involves practical constraints of the power distribution network, Heating distribution network (HDN) and natural gas distribution network simultaneously. A variable mass flow and temperature control strategy is applied in the HDN to make a high-performance energy supply scheme. Multi-energy storage systems (MESSs) and integrated demand response (IDR) are also considered to increase the flexibility of the MEDC for serving multi-type energy demands. Moreover, a hybrid robust-stochastic optimization technique is adopted to handle the system uncertainties, where the uncertainties related to RESs and energy prices are addressed under a scenario-based stochastic programming and a robust optimization technique, respectively. The simulation results demonstrate that the efficient use of MESSs and IDR improves the performance of multi-energy generation units in the presence of multi-energy distribution network constraints and reduces the total operation cost by 15\%.},
	urldate = {2025-07-14},
	journal = {Sustainable Cities and Society},
	author = {Mirzaei, Mohammad Amin and Zare, Kazem and Mohammadi-Ivatloo, Behnam and Marzband, Mousa and Anvari-Moghaddam, Amjad},
	month = nov,
	year = {2021},
	keywords = {Gas distribution network, Heating distribution network, Hybrid optimization approach, Integrated demand response, Multi-energy storage systems, Multi-energy systems, Robust optimization},
	pages = {103147},
	file = {ScienceDirect Snapshot:C\:\\Users\\manue\\Zotero\\storage\\KC8IK7RL\\S2210670721004297.html:text/html;Submitted Version:C\:\\Users\\manue\\Zotero\\storage\\WZDGFD4L\\Mirzaei et al. - 2021 - Robust network-constrained energy management of a .pdf:application/pdf},
}

@article{hong_house_2020,
	title = {A house price valuation based on the random forest approach: the mass appraisal of residential property in {South} {Korea}},
	volume = {24},
	copyright = {Copyright (c) 2020 The Author(s). Published by Vilnius Gediminas Technical University.},
	issn = {1648-9179},
	shorttitle = {A house price valuation based on the random forest approach},
	url = {https://journals.vilniustech.lt/index.php/IJSPM/article/view/11544},
	doi = {10.3846/ijspm.2020.11544},
	abstract = {Mass appraisal is the standardized procedure of valuing a large number of properties at the same time and is commonly used to compute real estate tax. While a hedonic pricing model based on the ordinary least squares (OLS) linear regression has been employed as the traditional method in this process, the stability and accuracy of the model remain questionable. This paper investigates the features of a house price predictor based on the Random Forest (RF) method by comparing it with that of a conventional hedonic pricing model. We used apartment transaction data from the period of 2006 to 2017 in the district of Gangnam, one of the most developed areas in South Korea. Using a data set covering 40\% of all transactions in the sample area, we demonstrate that the accuracy of a machine learning-based predictor can be surprisingly high. The average of percentage deviations between the predicted and the actual market price was found to be only around 5.5\% in the RF predictor, whereas it was almost 20\% in the OLS-based predictor. With the RF predictor, the probability of the predicted price being within 5\% of its actual market price was 72\%, while only about 17.5\% of the regression-based predictions fell within the same range. These results show that, in the practice of mass appraisal, the RF method may be a useful complement to the hedonic models, as it more adequately captures the complexity or non-linearity of actual housing markets.
First published online 03 February 2020},
	language = {en},
	number = {3},
	urldate = {2025-07-16},
	journal = {International Journal of Strategic Property Management},
	author = {Hong, Jengei and Choi, Heeyoul and Kim, Woo-sung},
	month = mar,
	year = {2020},
	note = {Number: 3},
	keywords = {apartment, hedonic pricing model, housing price forecasting, machine learning technique, mass appraisal, random forest approach},
	pages = {140--152},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\VIA24R93\\Hong et al. - 2020 - A house price valuation based on the random forest.pdf:application/pdf},
}

@article{matthews_mapping_2012,
	title = {Mapping the results of local statistics: {Using} geographically weighted regression},
	volume = {26},
	issn = {1435-9871},
	shorttitle = {Mapping the results of local statistics},
	url = {https://www.demographic-research.org/articles/volume/26/6},
	doi = {10.4054/demres.2012.26.6},
	abstract = {BACKGROUND The application of geographically weighted regression (GWR) – a local spatial statistical technique used to test for spatial nonstationarity – has grown rapidly in the social, health, and demographic sciences. GWR is a useful exploratory analytical tool that generates a set of location-specific parameter estimates which can be mapped and analysed to provide information on spatial nonstationarity in the relationships between predictors and the outcome variable.
OBJECTIVE A major challenge to users of GWR methods is how best to present and synthesize the large number of mappable results, specifically the local parameter parameter estimates and local t-values, generated from local GWR models. We offer an elegant solution.
METHODS This paper introduces a mapping technique to simultaneously display local parameter estimates and local t-values on one map based on the use of data selection and transparency techniques. We integrate GWR software and GIS software package (ArcGIS) and adapt earlier work in cartography on bivariate mapping. We compare traditional mapping strategies (i.e., side-by-side comparison and isoline overlay maps) with our method using an illustration focusing on US county infant mortality data.
CONCLUSIONS The resultant map design is more elegant than methods used to date. This type of map presentation can facilitate the exploration and interpretation of nonstationarity, focusing map reader attention on the areas of primary interest.},
	language = {en},
	urldate = {2025-07-19},
	journal = {Demographic Research},
	author = {Matthews, Stephen and Yang, Tse-Chuan},
	month = mar,
	year = {2012},
	note = {Publisher: Max Planck Institute for Demographic Research},
	pages = {151--166},
	file = {Matthews and Yang - 2012 - Mapping the results of local statistics Using geo.pdf:C\:\\Users\\manue\\Zotero\\storage\\Z4JPEVWK\\Matthews and Yang - 2012 - Mapping the results of local statistics Using geo.pdf:application/pdf},
}

@inproceedings{liu_improving_2025,
	address = {Albuquerque, New Mexico},
	title = {Improving {Data} {Annotation} for {Low}-{Resource} {Relation} {Extraction} with {Logical} {Rule}-{Augmented} {Collaborative} {Language} {Models}},
	isbn = {9798891761896},
	url = {https://aclanthology.org/2025.naacl-long.70/},
	doi = {10.18653/v1/2025.naacl-long.70},
	abstract = {Low-resource relation extraction aims to identify semantic relationships between entities using scarce labeled data. Recent studies exploit large language models to recognize relations based on retrieved examplars, yielding promising results. However, the reliability of predictions from these methods is constrained by the presence of irrelevant context within demonstrations and the inherent flaws of large language models in producing undesired outputs. Inspired by the precision and generalization of abstract logic, in this paper, we propose distilling logical rules to uniformly represent task knowledge sourced from distinct origins and facilitate deductive reasoning. We develop a collaborative annotating framework that iteratively integrates high-confidence predictions of rule-enhanced relation extractors with varying scales, efficiently obtaining reliable pseudo annotations from massive unlabeled samples without human supervision. Experiments under two inference settings show that our approach achieves new state-of-the-art performance on benchmark datasets in few-shot scenarios.},
	urldate = {2025-07-23},
	booktitle = {Proceedings of the 2025 {Conference} of the {Nations} of the {Americas} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Xiyang and Hu, Chunming and Zhang, Richong and Chen, Junfan and Xu, Baowen},
	editor = {Chiruzzo, Luis and Ritter, Alan and Wang, Lu},
	month = apr,
	year = {2025},
	pages = {1497--1510},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\U6MCHSIB\\Liu et al. - 2025 - Improving Data Annotation for Low-Resource Relatio.pdf:application/pdf},
}

@book{jankowski_analyzing_2017,
	title = {Analyzing {Local} {Party} {Manifestos} in {Multi}-{Level} {Democracies}},
	author = {Jankowski, Michael and Gross, Martin},
	month = aug,
	year = {2017},
	keywords = {\#me \#exploring \#byMyself},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\UAFYMRAC\\Jankowski and Gross - 2017 - Analyzing Local Party Manifestos in Multi-Level De.pdf:application/pdf},
}

@article{kirono_drought_2020,
	title = {Drought projections for {Australia}: {Updated} results and analysis of model simulations},
	volume = {30},
	shorttitle = {Drought projections for {Australia}},
	doi = {10.1016/j.wace.2020.100280},
	abstract = {To meet increasing demand for information on future drought hazard to help Australia build resilience and preparedness under a changing climate, we developed new information on drought projections for Australia and four sub-regions based on the natural resources management (NRM) zones. The information reported here includes: two drought indices (the Standardised Precipitation Index, SPI, and the Standardised Soil Moisture Index, SSMI); four drought metrics (percent time spent in droughts, mean drought duration, mean drought frequency, and mean drought intensity); and two drought categories (drought and extreme drought). The projections are developed from CMIP5 global climate model simulations of rainfall and soil moisture for the historical (1900–2005) and future (2006–2 100) climates.
The multi-model results project significant increases in all the drought hazard metrics, except frequency, with larger changes in the SSMI compared to SPI. The more severe drought hazard under climate change is apparent over a larger area than previously indicated, particularly in southern and eastern Australia. Although the majority of modelling results indicate more severe drought conditions, the range in the results is large, mainly because of the uncertainty in the global climate model rainfall projections. A projected decrease in rainfall results in a projected increase in drought severity (which is further enhanced by the increase in potential evapotranspiration), and a projected increase in rainfall results in a projected decrease in drought severity (moderated by the increase in potential evapotranspiration). The assessment of the ability of models to reproduce historical observations does not show clusters of models that best simulate all the different drought metrics. Unlike previously assumed, the results show that the models that best reproduce the observed rainfall are not necessarily best in simulating the drought metrics. For this reason, all the models are used here to estimate the multi-model median and range of results. The large uncertainty in the projections can be confusing to end users and present challenges in adapting to climate change. The presentation and communication of projections here will also go some way towards overcoming this challenge.},
	journal = {Weather and Climate Extremes},
	author = {Kirono, Dewi and Round, Vanessa and Heady, Craig and Chiew, F. and Osbrough, Stacey},
	month = aug,
	year = {2020},
	pages = {100280},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\JXIEXKQW\\Kirono et al. - 2020 - Drought projections for Australia Updated results.pdf:application/pdf},
}

@article{kirono_drought_2020-1,
	title = {Drought projections for {Australia}: {Updated} results and analysis of model simulations},
	volume = {30},
	shorttitle = {Drought projections for {Australia}},
	doi = {10.1016/j.wace.2020.100280},
	abstract = {To meet increasing demand for information on future drought hazard to help Australia build resilience and preparedness under a changing climate, we developed new information on drought projections for Australia and four sub-regions based on the natural resources management (NRM) zones. The information reported here includes: two drought indices (the Standardised Precipitation Index, SPI, and the Standardised Soil Moisture Index, SSMI); four drought metrics (percent time spent in droughts, mean drought duration, mean drought frequency, and mean drought intensity); and two drought categories (drought and extreme drought). The projections are developed from CMIP5 global climate model simulations of rainfall and soil moisture for the historical (1900–2005) and future (2006–2 100) climates.
The multi-model results project significant increases in all the drought hazard metrics, except frequency, with larger changes in the SSMI compared to SPI. The more severe drought hazard under climate change is apparent over a larger area than previously indicated, particularly in southern and eastern Australia. Although the majority of modelling results indicate more severe drought conditions, the range in the results is large, mainly because of the uncertainty in the global climate model rainfall projections. A projected decrease in rainfall results in a projected increase in drought severity (which is further enhanced by the increase in potential evapotranspiration), and a projected increase in rainfall results in a projected decrease in drought severity (moderated by the increase in potential evapotranspiration). The assessment of the ability of models to reproduce historical observations does not show clusters of models that best simulate all the different drought metrics. Unlike previously assumed, the results show that the models that best reproduce the observed rainfall are not necessarily best in simulating the drought metrics. For this reason, all the models are used here to estimate the multi-model median and range of results. The large uncertainty in the projections can be confusing to end users and present challenges in adapting to climate change. The presentation and communication of projections here will also go some way towards overcoming this challenge.},
	journal = {Weather and Climate Extremes},
	author = {Kirono, Dewi and Round, Vanessa and Heady, Craig and Chiew, F. and Osbrough, Stacey},
	month = aug,
	year = {2020},
	pages = {100280},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\H8VVKZIW\\Kirono et al. - 2020 - Drought projections for Australia Updated results.pdf:application/pdf},
}

@article{schneider_climate_2013,
	title = {Climate {Data} {Guide} {Spurs} {Discovery} and {Understanding}},
	volume = {94},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0096-3941, 2324-9250},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2013EO130001},
	doi = {10.1002/2013EO130001},
	abstract = {Highly accurate and stable observations—beyond those provided by routine weather monitoring—are essential for understanding the behavior of the climate system, developing and validating Earth system models, and attributing extreme weather events and long‐term trends to causes [
              National Research Council
              , 2012;
              Trenberth et al
              ., 2013]. In parallel with an exploding volume of climate data, ready access to data in user‐friendly formats is important to an expanding number and increasing diversity of individuals worldwide across public, private, and academic sectors [
              Overpeck et al
              ., 2011].},
	language = {en},
	number = {13},
	urldate = {2025-08-05},
	journal = {Eos, Transactions American Geophysical Union},
	author = {Schneider, David P. and Deser, Clara and Fasullo, John and Trenberth, Kevin E.},
	month = mar,
	year = {2013},
	pages = {121--122},
}

@incollection{pandey_indices_2021,
	address = {Singapore},
	title = {Indices for {Meteorological} and {Hydrological} {Drought}},
	isbn = {9789811603938 9789811603945},
	url = {https://link.springer.com/10.1007/978-981-16-0394-5_11},
	language = {en},
	urldate = {2025-08-05},
	booktitle = {Hydrological {Aspects} of {Climate} {Change}},
	publisher = {Springer Singapore},
	author = {Keyantash, John},
	editor = {Pandey, Ashish and Kumar, Sanjay and Kumar, Arun},
	year = {2021},
	doi = {10.1007/978-981-16-0394-5_11},
	note = {Series Title: Springer Transactions in Civil and Environmental Engineering},
	pages = {215--235},
}

@misc{duan_breaking_2025,
	title = {Breaking the {Sorting} {Barrier} for {Directed} {Single}-{Source} {Shortest} {Paths}},
	url = {http://arxiv.org/abs/2504.17033},
	doi = {10.48550/arXiv.2504.17033},
	abstract = {We give a deterministic \$O(m{\textbackslash}log{\textasciicircum}\{2/3\}n)\$-time algorithm for single-source shortest paths (SSSP) on directed graphs with real non-negative edge weights in the comparison-addition model. This is the first result to break the \$O(m+n{\textbackslash}log n)\$ time bound of Dijkstra's algorithm on sparse graphs, showing that Dijkstra's algorithm is not optimal for SSSP.},
	urldate = {2025-08-24},
	publisher = {arXiv},
	author = {Duan, Ran and Mao, Jiayi and Mao, Xiao and Shu, Xinkai and Yin, Longhui},
	month = jul,
	year = {2025},
	note = {arXiv:2504.17033 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	annote = {Comment: 17 pages},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\XAV63S4F\\Duan et al. - 2025 - Breaking the Sorting Barrier for Directed Single-S.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\U967D6EG\\2504.html:text/html},
}

@misc{wang_housets_2025,
	title = {{HouseTS}: {A} {Large}-{Scale}, {Multimodal} {Spatiotemporal} {U}.{S}. {Housing} {Dataset}},
	shorttitle = {{HouseTS}},
	url = {http://arxiv.org/abs/2506.00765},
	doi = {10.48550/arXiv.2506.00765},
	abstract = {Accurate house-price forecasting is essential for investors, planners, and researchers. However, reproducible benchmarks with sufficient spatiotemporal depth and contextual richness for long horizon prediction remain scarce. To address this, we introduce HouseTS a large scale, multimodal dataset covering monthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in 30 major U.S. metropolitan areas. The dataset includes over 890K records, enriched with points of Interest (POI), socioeconomic indicators, and detailed real estate metrics. To establish standardized performance baselines, we evaluate 14 models, spanning classical statistical approaches, deep neural networks (DNNs), and pretrained time-series foundation models. We further demonstrate the value of HouseTS in a multimodal case study, where a vision language model extracts structured textual descriptions of geographic change from time stamped satellite imagery. This enables interpretable, grounded insights into urban evolution. HouseTS is hosted on Kaggle, while all preprocessing pipelines, benchmark code, and documentation are openly maintained on GitHub to ensure full reproducibility and easy adoption.},
	urldate = {2025-09-09},
	publisher = {arXiv},
	author = {Wang, Shengkun and Sun, Yanshen and Chen, Fanglan and Wang, Linhan and Ramakrishnan, Naren and Lu, Chang-Tien and Chen, Yinlin},
	month = jun,
	year = {2025},
	note = {arXiv:2506.00765 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\manue\\Zotero\\storage\\FHLZ8X2R\\Wang et al. - 2025 - HouseTS A Large-Scale, Multimodal Spatiotemporal .pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\UMERVIQ8\\2506.html:text/html},
}

@article{hang_few-shot_2025,
	title = {Few-{Shot} {Relation} {Extraction} {Based} on {Prompt} {Learning}: {A} {Taxonomy}, {Survey}, {Challenges} and {Future} {Directions}},
	volume = {58},
	issn = {0360-0300},
	shorttitle = {Few-{Shot} {Relation} {Extraction} {Based} on {Prompt} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3746281},
	doi = {10.1145/3746281},
	abstract = {Relation extraction (RE) is critical in information extraction (IE) and knowledge graph construction. RE aims to identify the semantic relations between entities from natural language texts. Traditional RE models often rely on many manually annotated training samples, which are limited when data is scarce. Therefore, exploring how to perform RE under few-shot conditions has become a research focus. Recently, prompt learning has attracted attention from researchers due to its ability to fully activate the potential of Pre-trained Language Models (PLMs), especially making significant progress in Few-Shot RE (FSRE). This article comprehensively reviews FSRE based on prompt learning. We first introduce the fundamental concepts of FSRE and prompt learning. Then, we systematically review recent research advances in FSRE with prompt learning, focusing on two perspectives: template construction and model fine-tuning strategies. Next, we summarize the benchmark datasets, evaluation metrics, and experimental results of representative works in FSRE. Afterward, we present practical applications of prompt-based FSRE in specialized domains. Finally, we discuss the critical challenges and future research directions of FSRE tasks based on prompt learning.},
	number = {2},
	urldate = {2025-10-30},
	journal = {ACM Comput. Surv.},
	author = {Hang, Tingting and Liu, Shuting and Feng, Jun and Djigal, Hamza and Huang, Jun},
	year = {2025},
	pages = {40:1--40:38},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\K9CD63GA\\Hang et al. - 2025 - Few-Shot Relation Extraction Based on Prompt Learn.pdf:application/pdf},
}

@article{wang_deep_2021,
	title = {Deep {Learning} {Model} for {House} {Price} {Prediction} {Using} {Heterogeneous} {Data} {Analysis} {Along} {With} {Joint} {Self}-{Attention} {Mechanism}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9395585},
	doi = {10.1109/ACCESS.2021.3071306},
	abstract = {House price prediction is a popular topic, and research teams are increasingly performing related studies by using deep learning or machine learning models. However, because some studies have not considered comprehensive information that affects house prices, prediction results are not always sufficiently precise. Therefore, we propose an end to end joint self-attention model for house prediction. In this model, we import data on public facilities such as parks, schools, and mass rapid transit stations to represent the availability of amenities, and we use satellite maps to analyze the environment surrounding houses. We adopt attention mechanisms, which are widely used in image, speech, and translation tasks, to identify crucial features that are considered by prospective house buyers. The model can automatically assign weights when given transaction data. Our proposed model differs from self-attention models because it considers the interaction between two different features to learn the complicated relationship between features in order to increase prediction precision. We conduct experiments to demonstrate the performance of the model. Experimental data include actual selling prices in real estate transaction data for the period from 2017 to 2018, public facility data acquired from the Taipei and New Taipei governments, and satellite maps crawled using the Google Maps application programming interface. We utilize these datasets to train our proposed and compare its performance with that of other machine learning-based models such as Extreme Gradient Boosting and Light Gradient Boosted Machine, deep learning, and several attention models. The experimental results indicate that the proposed model achieves a low prediction error and outperforms the other models. To the best of our knowledge, we are the first research to incorporate attention mechanism and STN network to conduct house price prediction.},
	urldate = {2025-11-11},
	journal = {IEEE Access},
	author = {Wang, Pei-Ying and Chen, Chiao-Ting and Su, Jain-Wun and Wang, Ting-Yun and Huang, Szu-Hao},
	year = {2021},
	keywords = {Feature extraction, Autoregressive processes, Data models, Biological system modeling, Google satellite map, heterogeneous data, Hidden Markov models, House price prediction, joint self-attention mechanism, Predictive models, Satellites, spatial transformer network},
	pages = {55244--55259},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\N46BBGM5\\Wang et al. - 2021 - Deep Learning Model for House Price Prediction Usi.pdf:application/pdf},
}

@misc{zhu_unpaired_2020,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	doi = {10.48550/arXiv.1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	note = {arXiv:1703.10593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: An extended version of our ICCV 2017 paper, v7 fixed the typos and updated the implementation details. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\YN5CC4ZB\\Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\F3ICZCER\\1703.html:text/html},
}

@misc{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	doi = {10.48550/arXiv.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2016},
	note = {arXiv:1611.07004 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:C\:\\Users\\manue\\Zotero\\storage\\ZGT349UZ\\Isola et al. - 2016 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;Snapshot:C\:\\Users\\manue\\Zotero\\storage\\GMPWBFCT\\1611.html:text/html},
}
